{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em2-6WR3KCfi",
        "outputId": "69b3c3f0-bc46-44b2-933b-cc1f73340174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers numpy scikit-learn pandas\n",
        "!pip install pandas torch transformers tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1ijs6B5bBPM"
      },
      "outputs": [],
      "source": [
        "# Bloom taxomony"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqw3xyRsuFIv",
        "outputId": "02e4a3d0-b30b-4c17-fbc7-9888a82a8792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading MiniLM model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CSV files...\n",
            "Processing 2186 courses...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing courses: 100%|██████████| 2186/2186 [18:48<00:00,  1.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analysis complete!\n",
            "Total processing time: 18.8 minutes (0.52 seconds per course)\n",
            "Summary results saved to 'course_module_alignment_summary_minilm.csv'\n",
            "Detailed results saved to 'course_module_alignment_detailed_minilm.csv'\n",
            "\n",
            "Summary statistics:\n",
            "Total courses analyzed: 2186\n",
            "Average coverage percentage: 62.12%\n",
            "Average mean max similarity: 0.5601\n",
            "Courses with good alignment: 1433/2185 (65.6%)\n"
          ]
        }
      ],
      "source": [
        "# similarity score (miniLM) - without blooms\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load the MiniLM model and tokenizer\n",
        "print(\"Loading MiniLM model...\")\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Mean Pooling function for creating sentence embeddings\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Function to encode texts using MiniLM\n",
        "def encode_texts(texts):\n",
        "    # Tokenize texts\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "    # Compute token embeddings\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "    # Perform mean pooling\n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "    # Normalize embeddings\n",
        "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "\n",
        "    return sentence_embeddings\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading CSV files...\")\n",
        "courses_df = pd.read_csv('course_scraped_with_lo.csv')\n",
        "modules_df = pd.read_csv('module_scraped_with_content.csv')\n",
        "\n",
        "# Define similarity threshold\n",
        "similarity_threshold = 0.5\n",
        "\n",
        "# Function to calculate alignment metrics\n",
        "def calculate_alignment_metrics(course_row):\n",
        "    # Extract learning objectives for this course\n",
        "    course_url = course_row['url']\n",
        "\n",
        "    if pd.isna(course_row['lo']) or course_row['lo'].strip() == '':\n",
        "        return {\n",
        "            'coverage_percentage': None,\n",
        "            'mean_max_similarity': None,\n",
        "            'num_learning_objectives': 0,\n",
        "            'num_modules': 0,\n",
        "            'num_module_contents': 0\n",
        "        }\n",
        "\n",
        "    learning_objectives = [lo.strip() for lo in course_row['lo'].split(';') if lo.strip()]\n",
        "\n",
        "    # Get all modules for this course\n",
        "    course_modules = modules_df[modules_df['url'] == course_url]\n",
        "\n",
        "    if len(course_modules) == 0 or len(learning_objectives) == 0:\n",
        "        return {\n",
        "            'coverage_percentage': None,\n",
        "            'mean_max_similarity': None,\n",
        "            'num_learning_objectives': len(learning_objectives),\n",
        "            'num_modules': 0,\n",
        "            'num_module_contents': 0\n",
        "        }\n",
        "\n",
        "    # Extract all module contents as individual items\n",
        "    all_module_contents = []\n",
        "    for _, module_row in course_modules.iterrows():\n",
        "        if pd.notna(module_row['module_content']) and module_row['module_content'].strip() != '':\n",
        "            contents = [content.strip() for content in module_row['module_content'].split(';') if content.strip()]\n",
        "            all_module_contents.extend(contents)\n",
        "\n",
        "    if len(all_module_contents) == 0:\n",
        "        return {\n",
        "            'coverage_percentage': 0.0,\n",
        "            'mean_max_similarity': 0.0,\n",
        "            'num_learning_objectives': len(learning_objectives),\n",
        "            'num_modules': len(course_modules),\n",
        "            'num_module_contents': 0\n",
        "        }\n",
        "\n",
        "    # Encode all learning objectives and module contents\n",
        "    lo_embeddings = encode_texts(learning_objectives)\n",
        "    module_embeddings = encode_texts(all_module_contents)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_scores = torch.mm(lo_embeddings, module_embeddings.T)\n",
        "\n",
        "    # Calculate metrics\n",
        "    max_similarities = []\n",
        "    covered_los = 0\n",
        "\n",
        "    # For each learning objective, find its highest similarity with any module content\n",
        "    detailed_results = []\n",
        "    for i in range(len(learning_objectives)):\n",
        "        # Find the highest similarity for this learning objective\n",
        "        max_sim_for_lo = torch.max(cosine_scores[i]).item()\n",
        "        max_similarities.append(max_sim_for_lo)\n",
        "\n",
        "        # Find the index of the best matching module content\n",
        "        max_sim_idx = torch.argmax(cosine_scores[i]).item()\n",
        "        best_match = all_module_contents[max_sim_idx]\n",
        "\n",
        "        # Check if this learning objective is covered by any module content\n",
        "        is_covered = max_sim_for_lo >= similarity_threshold\n",
        "        if is_covered:\n",
        "            covered_los += 1\n",
        "\n",
        "        # Store detailed result for this learning objective\n",
        "        detailed_results.append({\n",
        "            'learning_objective': learning_objectives[i],\n",
        "            'max_similarity': max_sim_for_lo,\n",
        "            'best_match_content': best_match,\n",
        "            'is_covered': is_covered\n",
        "        })\n",
        "\n",
        "    coverage_percentage = (covered_los / len(learning_objectives)) * 100 if learning_objectives else 0\n",
        "    mean_max_similarity = np.mean(max_similarities) if max_similarities else 0\n",
        "\n",
        "    return {\n",
        "        'coverage_percentage': coverage_percentage,\n",
        "        'mean_max_similarity': mean_max_similarity,\n",
        "        'num_learning_objectives': len(learning_objectives),\n",
        "        'num_modules': len(course_modules),\n",
        "        'num_module_contents': len(all_module_contents),\n",
        "        'detailed_results': detailed_results\n",
        "    }\n",
        "\n",
        "# Process each course\n",
        "print(f\"Processing {len(courses_df)} courses...\")\n",
        "start_time = time.time()\n",
        "results = []\n",
        "detailed_all = []\n",
        "\n",
        "# Process in batches to avoid memory issues\n",
        "batch_size = 10\n",
        "num_batches = (len(courses_df) + batch_size - 1)//batch_size\n",
        "\n",
        "# Set up progress tracking with just a simple progress bar\n",
        "with tqdm(total=len(courses_df), desc=\"Analyzing courses\") as pbar:\n",
        "    for i in range(0, len(courses_df), batch_size):\n",
        "        batch = courses_df.iloc[i:i+batch_size]\n",
        "\n",
        "        for idx, course_row in batch.iterrows():\n",
        "            metrics = calculate_alignment_metrics(course_row)\n",
        "\n",
        "            result = {\n",
        "                'program_id': course_row['program_id'],\n",
        "                'course_title': course_row['course_title'],\n",
        "                'url': course_row['url'],\n",
        "                'num_learning_objectives': metrics['num_learning_objectives'],\n",
        "                'num_modules': metrics['num_modules'],\n",
        "                'num_module_contents': metrics['num_module_contents'],\n",
        "                'coverage_percentage': metrics['coverage_percentage'],\n",
        "                'mean_max_similarity': metrics['mean_max_similarity']\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Create detailed results for this course\n",
        "            if 'detailed_results' in metrics:\n",
        "                for detail in metrics['detailed_results']:\n",
        "                    detailed = {\n",
        "                        'program_id': course_row['program_id'],\n",
        "                        'course_title': course_row['course_title'],\n",
        "                        'url': course_row['url'],\n",
        "                        'learning_objective': detail['learning_objective'],\n",
        "                        'max_similarity': detail['max_similarity'],\n",
        "                        'best_match_content': detail['best_match_content'],\n",
        "                        'is_covered': detail['is_covered']\n",
        "                    }\n",
        "                    detailed_all.append(detailed)\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "# Create results DataFrames\n",
        "results_df = pd.DataFrame(results)\n",
        "detailed_df = pd.DataFrame(detailed_all)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('course_module_alignment_summary_minilm.csv', index=False)\n",
        "detailed_df.to_csv('course_module_alignment_detailed_minilm.csv', index=False)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\nAnalysis complete!\")\n",
        "print(f\"Total processing time: {total_time/60:.1f} minutes ({total_time/len(courses_df):.2f} seconds per course)\")\n",
        "print(f\"Summary results saved to 'course_module_alignment_summary_minilm.csv'\")\n",
        "print(f\"Detailed results saved to 'course_module_alignment_detailed_minilm.csv'\")\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(f\"Total courses analyzed: {len(results_df)}\")\n",
        "valid_results = results_df[results_df['coverage_percentage'].notna()]\n",
        "if len(valid_results) > 0:\n",
        "    print(f\"Average coverage percentage: {valid_results['coverage_percentage'].mean():.2f}%\")\n",
        "    print(f\"Average mean max similarity: {valid_results['mean_max_similarity'].mean():.4f}\")\n",
        "\n",
        "    # Count courses with good alignment (>50% coverage and >0.5 mean similarity)\n",
        "    good_alignment = valid_results[\n",
        "        (valid_results['coverage_percentage'] >= 50) &\n",
        "        (valid_results['mean_max_similarity'] >= 0.5)\n",
        "    ]\n",
        "    print(f\"Courses with good alignment: {len(good_alignment)}/{len(valid_results)} ({len(good_alignment)/len(valid_results)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQI4Arl_uFIv",
        "outputId": "1fc2ac64-6c98-490c-d6ba-423dccf5c585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 100.00% (2186/2186 courses)\n",
            "Processing complete. Processed 14289 learning objectives from 2186 courses.\n",
            "Output saved to courses_with_blooms_per_lo.csv\n",
            "   program_id                      course_title  \\\n",
            "0           0  Systems and Application Security   \n",
            "1           0  Systems and Application Security   \n",
            "2           0  Systems and Application Security   \n",
            "3           0  Systems and Application Security   \n",
            "4           0  Systems and Application Security   \n",
            "\n",
            "                                                 url  lo_num  \\\n",
            "0  https://www.coursera.org/learn/systems-and-app...       1   \n",
            "1  https://www.coursera.org/learn/systems-and-app...       2   \n",
            "2  https://www.coursera.org/learn/systems-and-app...       3   \n",
            "3  https://www.coursera.org/learn/systems-and-app...       4   \n",
            "4  https://www.coursera.org/learn/systems-and-app...       5   \n",
            "\n",
            "                                                  lo   bloom  \n",
            "0                   Systems and Application Security   apply  \n",
            "1        Course 7 - Systems and Application Security   apply  \n",
            "2  This is the seventh course under the specializ...   apply  \n",
            "3  This course discusses two major changes in rec...  create  \n",
            "4  First, we use our data on the go by means of d...  create  \n"
          ]
        }
      ],
      "source": [
        "# finding bloom tags for each learning objective\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Suppress TensorFlow and transformers warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')  # Suppress TensorFlow warnings\n",
        "\n",
        "# Suppress sentence transformers progress bars\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas(disable=True)\n",
        "\n",
        "# Initialize the MiniLM model for semantic embedding\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Bloom's Taxonomy categories and their example keywords\n",
        "blooms_keywords = {\n",
        "    'remember': ['define', 'identify', 'recall', 'recognize', 'state', 'memorize'],\n",
        "    'understand': ['comprehend', 'explain', 'summarize', 'infer', 'translate', 'discuss'],\n",
        "    'apply': ['apply', 'demonstrate', 'use', 'execute', 'implement', 'solve'],\n",
        "    'analyze': ['analyze', 'compare', 'contrast', 'distinguish', 'differentiate', 'examine'],\n",
        "    'evaluate': ['evaluate', 'justify', 'assess', 'support', 'judge', 'rate'],\n",
        "    'create': ['create', 'design', 'develop', 'generate', 'plan', 'formulate']\n",
        "}\n",
        "\n",
        "# Generate Bloom's category centroid embeddings\n",
        "def generate_bloom_centroids():\n",
        "    centroids = {}\n",
        "    for level, keywords in blooms_keywords.items():\n",
        "        embeddings = model.encode(keywords, show_progress_bar=False)\n",
        "        centroids[level] = np.mean(embeddings, axis=0)\n",
        "    return centroids\n",
        "\n",
        "bloom_centroids = generate_bloom_centroids()\n",
        "\n",
        "# Function to classify Bloom's level based on cosine similarity\n",
        "def classify_blooms_level(text):\n",
        "    \"\"\"Classify Bloom's Taxonomy level for a given learning objective.\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return None  # Skip empty LOs\n",
        "\n",
        "    text_embedding = model.encode([text], show_progress_bar=False)[0]\n",
        "    similarities = {level: cosine_similarity([text_embedding], [centroid])[0][0] for level, centroid in bloom_centroids.items()}\n",
        "\n",
        "    return max(similarities, key=similarities.get)\n",
        "\n",
        "# Function to transform input CSV and classify Bloom's taxonomy\n",
        "def transform_and_classify(input_file, output_file):\n",
        "    \"\"\"Transform CSV: Each LO gets its own row, with Bloom's classification, skipping empty LOs.\"\"\"\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    transformed_rows = []\n",
        "    total_courses = len(df)\n",
        "    total_lo_count = 0\n",
        "\n",
        "    for course_idx, row in df.iterrows():\n",
        "        course_title = row['course_title']\n",
        "        program_id = row['program_id']\n",
        "        url = row['url']  # Include URL\n",
        "        lo_list = row['lo'].split(';') if pd.notna(row['lo']) else []  # Handle NaN LO field\n",
        "\n",
        "        lo_num = 0  # Track valid LOs per course\n",
        "\n",
        "        for lo in lo_list:\n",
        "            lo = lo.strip()\n",
        "            if not lo:  # Skip empty LOs\n",
        "                continue\n",
        "\n",
        "            lo_num += 1  # Increment only for valid LOs\n",
        "            bloom_level = classify_blooms_level(lo)\n",
        "\n",
        "            transformed_rows.append({\n",
        "                'program_id': program_id,\n",
        "                'course_title': course_title,\n",
        "                'url': url,\n",
        "                'lo_num': lo_num,\n",
        "                'lo': lo,\n",
        "                'bloom': bloom_level\n",
        "            })\n",
        "\n",
        "            total_lo_count += 1\n",
        "\n",
        "        progress_percentage = (course_idx + 1) / total_courses * 100\n",
        "        sys.stdout.write(f'\\rProgress: {progress_percentage:.2f}% ({course_idx + 1}/{total_courses} courses)')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    transformed_df = pd.DataFrame(transformed_rows)\n",
        "\n",
        "\n",
        "    transformed_df.rename(columns={\n",
        "        \"bloom_category\": \"bloom_from_lo\",\n",
        "        \"module_bloom_category\": \"bloom_from_module\"\n",
        "    }, inplace=True)\n",
        "\n",
        "\n",
        "    # Save the final cleaned output\n",
        "    transformed_df.to_csv(output_file, index=False)\n",
        "\n",
        "    sys.stdout.write(f'\\nProcessing complete. Processed {total_lo_count} learning objectives from {total_courses} courses.\\n')\n",
        "    sys.stdout.write(f'Output saved to {output_file}\\n')\n",
        "\n",
        "    return transformed_df\n",
        "\n",
        "# Paths to input and output files\n",
        "input_file = 'course_scraped_with_lo.csv'\n",
        "output_file = 'courses_with_blooms_per_lo.csv'\n",
        "\n",
        "# Transform and classify learning objectives\n",
        "result_df = transform_and_classify(input_file, output_file)\n",
        "\n",
        "# Print the first few rows of the final output\n",
        "print(result_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9jQcaUc3uFIv",
        "outputId": "da052f1f-7229-4ae2-dec3-3a6b182e282c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 100.00% (14285/14285 modules)\n",
            "Processing complete. Processed 14285 content from 14285 modules.\n",
            "Output saved to course_module_alignment_detailed_minilm.csv\n",
            "   program_id                      course_title  \\\n",
            "0           0  Systems and Application Security   \n",
            "1           0  Systems and Application Security   \n",
            "2           0  Systems and Application Security   \n",
            "3           0  Systems and Application Security   \n",
            "4           0  Systems and Application Security   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.coursera.org/learn/systems-and-app...   \n",
            "1  https://www.coursera.org/learn/systems-and-app...   \n",
            "2  https://www.coursera.org/learn/systems-and-app...   \n",
            "3  https://www.coursera.org/learn/systems-and-app...   \n",
            "4  https://www.coursera.org/learn/systems-and-app...   \n",
            "\n",
            "                                  learning_objective  max_similarity  \\\n",
            "0                   Systems and Application Security        0.527121   \n",
            "1        Course 7 - Systems and Application Security        0.491941   \n",
            "2  This is the seventh course under the specializ...        0.240702   \n",
            "3  This course discusses two major changes in rec...        0.271610   \n",
            "4  First, we use our data on the go by means of d...        0.294011   \n",
            "\n",
            "                    best_match_content  is_covered   bloom  \n",
            "0    Security Strategies for Endpoints        True   apply  \n",
            "1    Security Strategies for Endpoints       False   apply  \n",
            "2  Essential Requirements in P&DP Laws       False  create  \n",
            "3                     Virtual Machines       False  create  \n",
            "4    Security Strategies for Endpoints       False   apply  \n"
          ]
        }
      ],
      "source": [
        "# find bloom tag for best-match module content of each learning objectives\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Suppress TensorFlow and transformers warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')  # Suppress TensorFlow warnings\n",
        "\n",
        "# Suppress sentence transformers progress bars\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas(disable=True)\n",
        "\n",
        "# Initialize the MiniLM model for semantic embedding\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Bloom's Taxonomy categories and their example keywords\n",
        "blooms_keywords = {\n",
        "    'remember': ['define', 'identify', 'recall', 'recognize', 'state', 'memorize'],\n",
        "    'understand': ['comprehend', 'explain', 'summarize', 'infer', 'translate', 'discuss'],\n",
        "    'apply': ['apply', 'demonstrate', 'use', 'execute', 'implement', 'solve'],\n",
        "    'analyze': ['analyze', 'compare', 'contrast', 'distinguish', 'differentiate', 'examine'],\n",
        "    'evaluate': ['evaluate', 'justify', 'assess', 'support', 'judge', 'rate'],\n",
        "    'create': ['create', 'design', 'develop', 'generate', 'plan', 'formulate']\n",
        "}\n",
        "\n",
        "# Generate Bloom's category centroid embeddings\n",
        "def generate_bloom_centroids():\n",
        "    centroids = {}\n",
        "    for level, keywords in blooms_keywords.items():\n",
        "        embeddings = model.encode(keywords, show_progress_bar=False)\n",
        "        centroids[level] = np.mean(embeddings, axis=0)\n",
        "    return centroids\n",
        "\n",
        "bloom_centroids = generate_bloom_centroids()\n",
        "\n",
        "# Function to classify Bloom's level based on cosine similarity\n",
        "def classify_blooms_level(text):\n",
        "    \"\"\"Classify Bloom's Taxonomy level for a given text.\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return None  # Skip empty texts\n",
        "\n",
        "    text_embedding = model.encode([text], show_progress_bar=False)[0]\n",
        "    similarities = {level: cosine_similarity([text_embedding], [centroid])[0][0] for level, centroid in bloom_centroids.items()}\n",
        "\n",
        "    return max(similarities, key=similarities.get)\n",
        "\n",
        "# Function to transform input CSV and classify Bloom's taxonomy for `best_match_content`\n",
        "def transform_and_classify(input_file, output_file):\n",
        "    \"\"\"Transform CSV: Each row gets Bloom's classification added.\"\"\"\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    transformed_rows = []\n",
        "    total_modules = len(df)\n",
        "    total_content_count = 0\n",
        "\n",
        "    for module_idx, row in df.iterrows():\n",
        "        # Keep all original columns\n",
        "        program_id = row['program_id']\n",
        "        course_title = row['course_title']\n",
        "        url = row['url']\n",
        "        learning_objective = row['learning_objective']\n",
        "        max_similarity = row['max_similarity']\n",
        "        best_match_content = row['best_match_content']\n",
        "        is_covered = row['is_covered']\n",
        "\n",
        "        if pd.notna(best_match_content):  # Only process if `best_match_content` is not NaN\n",
        "            best_match_content = best_match_content.strip()\n",
        "\n",
        "            # Classify Bloom's taxonomy level\n",
        "            bloom_level = classify_blooms_level(best_match_content)\n",
        "\n",
        "            transformed_rows.append({\n",
        "                'program_id': program_id,\n",
        "                'course_title': course_title,\n",
        "                'url': url,\n",
        "                'learning_objective': learning_objective,\n",
        "                'max_similarity': max_similarity,\n",
        "                'best_match_content': best_match_content,\n",
        "                'is_covered': is_covered,\n",
        "                'bloom': bloom_level\n",
        "            })\n",
        "\n",
        "            total_content_count += 1\n",
        "\n",
        "        progress_percentage = (module_idx + 1) / total_modules * 100\n",
        "        sys.stdout.write(f'\\rProgress: {progress_percentage:.2f}% ({module_idx + 1}/{total_modules} modules)')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    transformed_df = pd.DataFrame(transformed_rows)\n",
        "\n",
        "    # Save the final cleaned output\n",
        "    transformed_df.to_csv(output_file, index=False)\n",
        "\n",
        "    sys.stdout.write(f'\\nProcessing complete. Processed {total_content_count} content from {total_modules} modules.\\n')\n",
        "    sys.stdout.write(f'Output saved to {output_file}\\n')\n",
        "\n",
        "    return transformed_df\n",
        "\n",
        "# Paths to input and output files (same file for input and output)\n",
        "input_file = 'course_module_alignment_detailed_minilm.csv'\n",
        "output_file = input_file  # Same as input file to overwrite it\n",
        "\n",
        "# Transform and classify best_match_content\n",
        "result_df = transform_and_classify(input_file, output_file)\n",
        "\n",
        "# Print the first few rows of the final output\n",
        "print(result_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KD50ByN6dxX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2257232f-8a2a-46b1-a9ff-05ee05fded95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis complete!\n",
            "Summary results saved to 'course_module_alignment_summary_minilm_bloom.csv'\n",
            "Detailed results saved to 'course_module_alignment_detailed_minilm_bloom.csv'\n",
            "\n",
            "Summary statistics:\n",
            "Total courses analyzed: 2186\n",
            "Courses penalized by Bloom tags: 1791/2186 (81.9%)\n",
            "Average coverage percentage: 59.27%\n",
            "Average mean max similarity: 0.5441\n",
            "Courses with good alignment: 1169/2186 (53.5%)\n"
          ]
        }
      ],
      "source": [
        "# Adjust similarity score based on bloom\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Bloom's Taxonomy complexity mapping\n",
        "blooms_complexity = {\n",
        "    'remember': 1,\n",
        "    'understand': 2,\n",
        "    'apply': 3,\n",
        "    'analyze': 4,\n",
        "    'evaluate': 5,\n",
        "    'create': 6\n",
        "}\n",
        "\n",
        "# Function to safely extract Bloom's complexity\n",
        "def get_bloom_complexity(bloom_tag):\n",
        "    if pd.isna(bloom_tag) or not isinstance(bloom_tag, str):\n",
        "        return 1  # Default to lowest complexity\n",
        "\n",
        "    # Convert to lowercase and strip whitespace\n",
        "    bloom_tag = bloom_tag.lower().strip()\n",
        "\n",
        "    # Direct match first\n",
        "    if bloom_tag in blooms_complexity:\n",
        "        match_type = \"direct\"\n",
        "        complexity = blooms_complexity[bloom_tag]\n",
        "    else:\n",
        "        # Fallback to most specific match\n",
        "        match_type = \"partial\"\n",
        "        complexity = 1  # Default\n",
        "        matched_level = None\n",
        "\n",
        "        for level, level_complexity in blooms_complexity.items():\n",
        "            if level in bloom_tag:\n",
        "                complexity = level_complexity\n",
        "                matched_level = level\n",
        "                break\n",
        "\n",
        "    return complexity\n",
        "\n",
        "\n",
        "def adjust_similarity_with_bloom(max_similarity, lo_complexity, module_complexity):\n",
        "    complexity_diff = abs(lo_complexity - module_complexity)\n",
        "\n",
        "    # Only apply penalty if there's a complexity difference\n",
        "    if complexity_diff == 0:\n",
        "        return max_similarity\n",
        "\n",
        "    # Adjust logarithm-based penalty with max 5% reduction\n",
        "    penalty_factor = max(0.95, 1.0 - (math.log1p(complexity_diff) / (math.log1p(6) * 1.5)))\n",
        "\n",
        "    adjusted_similarity = max_similarity * penalty_factor\n",
        "\n",
        "    return adjusted_similarity\n",
        "\n",
        "\n",
        "def refine_module_alignment(courses_with_blooms_file,\n",
        "                             minilm_detailed_file,\n",
        "                             minilm_summary_file,\n",
        "                             output_detailed_file,\n",
        "                             output_summary_file):\n",
        "\n",
        "    # Load the necessary data files\n",
        "    courses_df = pd.read_csv(courses_with_blooms_file)\n",
        "    detailed_df = pd.read_csv(minilm_detailed_file)\n",
        "    summary_df = pd.read_csv(minilm_summary_file)\n",
        "\n",
        "    # Create a copy for the updated summary to avoid modifying the original\n",
        "    updated_summary_df = summary_df.copy()\n",
        "\n",
        "    # First, verify that the 'bloom' column exists in detailed_df (not courses_df)\n",
        "    if 'bloom' not in detailed_df.columns:\n",
        "        # Try to identify the correct column name that might contain bloom taxonomy data\n",
        "        possible_bloom_columns = [col for col in detailed_df.columns if 'bloom' in col.lower()]\n",
        "\n",
        "        if possible_bloom_columns:\n",
        "            # Use the first found bloom-related column\n",
        "            bloom_column = possible_bloom_columns[0]\n",
        "            print(f\"Column 'bloom' not found in detailed file. Using '{bloom_column}' instead.\")\n",
        "        else:\n",
        "            # If no bloom-related column is found, print available columns and raise error\n",
        "            print(\"Column 'bloom' not found in detailed_df. Available columns:\", detailed_df.columns.tolist())\n",
        "            raise KeyError(\"Required column 'bloom' not found in the detailed file.\")\n",
        "    else:\n",
        "        bloom_column = 'bloom'\n",
        "\n",
        "    # Prepare to store updated detailed results\n",
        "    updated_detailed_results = []\n",
        "\n",
        "    # Similarity threshold\n",
        "    similarity_threshold = 0.5\n",
        "\n",
        "    # Track courses penalized due to Bloom tags\n",
        "    courses_penalized_by_bloom = set()\n",
        "\n",
        "    # Process each row in detailed_df\n",
        "    for _, lo_row in detailed_df.iterrows():\n",
        "        url = lo_row['url']\n",
        "        learning_objective = lo_row['learning_objective']\n",
        "\n",
        "        # Find corresponding bloom tags from courses_df for this specific LO\n",
        "        lo_match = courses_df[(courses_df['url'] == url) & (courses_df['lo'] == learning_objective)]\n",
        "\n",
        "        # Extract LO bloom tag\n",
        "        lo_bloom = lo_match['bloom'].values[0] if len(lo_match) > 0 and 'bloom' in courses_df.columns else None\n",
        "        lo_complexity = get_bloom_complexity(lo_bloom)\n",
        "\n",
        "        # Get module bloom tag directly from detailed_df for this URL\n",
        "        module_entries = detailed_df[detailed_df['url'] == url]\n",
        "        module_bloom = module_entries[bloom_column].iloc[0] if len(module_entries) > 0 and bloom_column in detailed_df.columns else None\n",
        "        module_complexity = get_bloom_complexity(module_bloom)\n",
        "\n",
        "        # Calculate original and adjusted similarity\n",
        "        original_similarity = lo_row['max_similarity']\n",
        "        adjusted_similarity = adjust_similarity_with_bloom(\n",
        "            original_similarity,\n",
        "            lo_complexity,\n",
        "            module_complexity\n",
        "        )\n",
        "\n",
        "        # Check if penalized (still track internally for summary stats)\n",
        "        if adjusted_similarity < original_similarity:\n",
        "            courses_penalized_by_bloom.add(url)\n",
        "\n",
        "        # Determine if covered based on adjusted similarity threshold\n",
        "        is_covered = adjusted_similarity >= similarity_threshold\n",
        "\n",
        "        # Prepare updated detailed result - start with original row data but exclude 'bloom' column if it exists\n",
        "        updated_result = {k: v for k, v in lo_row.to_dict().items() if k != bloom_column}\n",
        "\n",
        "        # Add our new columns\n",
        "        updated_result['lo_bloom'] = lo_bloom\n",
        "        updated_result['lo_complexity'] = lo_complexity\n",
        "        updated_result['module_bloom'] = module_bloom\n",
        "        updated_result['module_complexity'] = module_complexity\n",
        "        updated_result['max_similarity'] = adjusted_similarity\n",
        "        updated_result['is_covered'] = is_covered\n",
        "\n",
        "        updated_detailed_results.append(updated_result)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    updated_detailed_df = pd.DataFrame(updated_detailed_results)\n",
        "\n",
        "    # Create a dictionary mapping URLs to their updated metrics\n",
        "    url_to_metrics = {}\n",
        "\n",
        "    # For each URL in the detailed file, calculate aggregated metrics\n",
        "    for url in updated_detailed_df['url'].unique():\n",
        "        url_detailed = updated_detailed_df[updated_detailed_df['url'] == url]\n",
        "\n",
        "        # Calculate metrics\n",
        "        num_los = len(url_detailed)\n",
        "        num_covered_los = sum(url_detailed['is_covered'])\n",
        "        coverage_percentage = (num_covered_los / num_los) * 100 if num_los > 0 else 0\n",
        "        mean_max_similarity = url_detailed['max_similarity'].mean()\n",
        "\n",
        "        # Store metrics\n",
        "        url_to_metrics[url] = {\n",
        "            'num_los': num_los,\n",
        "            'num_covered_los': num_covered_los,\n",
        "            'coverage_percentage': coverage_percentage,\n",
        "            'mean_max_similarity': mean_max_similarity\n",
        "        }\n",
        "\n",
        "    # Track rows that get updated\n",
        "    updated_rows = 0\n",
        "\n",
        "    # Go through each row in summary_df\n",
        "    for idx, row in updated_summary_df.iterrows():\n",
        "        url = row['url']\n",
        "\n",
        "        # If this URL has updated metrics, apply them\n",
        "        if url in url_to_metrics:\n",
        "            metrics = url_to_metrics[url]\n",
        "            updated_summary_df.at[idx, 'num_los'] = metrics['num_los']\n",
        "            updated_summary_df.at[idx, 'num_covered_los'] = metrics['num_covered_los']\n",
        "            updated_summary_df.at[idx, 'coverage_percentage'] = metrics['coverage_percentage']\n",
        "            updated_summary_df.at[idx, 'mean_max_similarity'] = metrics['mean_max_similarity']\n",
        "            updated_rows += 1\n",
        "\n",
        "    # Save updated files\n",
        "    updated_detailed_df.to_csv(output_detailed_file, index=False)\n",
        "    updated_summary_df.to_csv(output_summary_file, index=False)\n",
        "\n",
        "    # Print summary statistics in the requested format\n",
        "    total_courses = len(updated_summary_df)\n",
        "    avg_coverage_percentage = updated_summary_df['coverage_percentage'].mean()\n",
        "    avg_mean_max_similarity = updated_summary_df['mean_max_similarity'].mean()\n",
        "\n",
        "    # Calculate number of courses with good alignment\n",
        "    courses_with_good_alignment = updated_summary_df[updated_summary_df['coverage_percentage'] >= 60]\n",
        "    num_courses_with_good_alignment = len(courses_with_good_alignment)\n",
        "\n",
        "    # Number of courses penalized by Bloom tags\n",
        "    num_courses_penalized = len(courses_penalized_by_bloom)\n",
        "\n",
        "    print(f\"\\nAnalysis complete!\")\n",
        "    print(f\"Summary results saved to '{output_summary_file}'\")\n",
        "    print(f\"Detailed results saved to '{output_detailed_file}'\\n\")\n",
        "\n",
        "    print(f\"Summary statistics:\")\n",
        "    print(f\"Total courses analyzed: {total_courses}\")\n",
        "    print(f\"Courses penalized by Bloom tags: {num_courses_penalized}/{total_courses} ({(num_courses_penalized / total_courses) * 100:.1f}%)\")\n",
        "    print(f\"Average coverage percentage: {avg_coverage_percentage:.2f}%\")\n",
        "    print(f\"Average mean max similarity: {avg_mean_max_similarity:.4f}\")\n",
        "    print(f\"Courses with good alignment: {num_courses_with_good_alignment}/{total_courses} ({(num_courses_with_good_alignment / total_courses) * 100:.1f}%)\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    courses_with_blooms_file = 'courses_with_blooms_per_lo.csv'\n",
        "    minilm_detailed_file = 'course_module_alignment_detailed_minilm.csv'\n",
        "    minilm_summary_file = 'course_module_alignment_summary_minilm.csv'\n",
        "\n",
        "    output_detailed_file = 'course_module_alignment_detailed_minilm_bloom.csv'\n",
        "    output_summary_file = 'course_module_alignment_summary_minilm_bloom.csv'\n",
        "\n",
        "    refine_module_alignment(\n",
        "        courses_with_blooms_file,\n",
        "        minilm_detailed_file,\n",
        "        minilm_summary_file,\n",
        "        output_detailed_file,\n",
        "        output_summary_file\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6954376,
          "sourceId": 11147491,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6954378,
          "sourceId": 11147496,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}