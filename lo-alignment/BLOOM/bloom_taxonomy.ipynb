{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-25T12:05:57.669409Z",
          "iopub.status.busy": "2025-03-25T12:05:57.669064Z",
          "iopub.status.idle": "2025-03-25T12:06:11.300999Z",
          "shell.execute_reply": "2025-03-25T12:06:11.299820Z",
          "shell.execute_reply.started": "2025-03-25T12:05:57.669386Z"
        },
        "id": "em2-6WR3KCfi",
        "outputId": "79a75e9d-1ede-47cc-fe70-06bb166cd814",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.50.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.29.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers numpy scikit-learn pandas\n",
        "!pip install pandas torch transformers tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-25T12:06:12.406309Z",
          "iopub.status.busy": "2025-03-25T12:06:12.406071Z",
          "iopub.status.idle": "2025-03-25T12:06:12.409714Z",
          "shell.execute_reply": "2025-03-25T12:06:12.408752Z",
          "shell.execute_reply.started": "2025-03-25T12:06:12.406287Z"
        },
        "id": "G1ijs6B5bBPM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Bloom taxomony"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-25T12:06:12.411833Z",
          "iopub.status.busy": "2025-03-25T12:06:12.411552Z",
          "iopub.status.idle": "2025-03-25T12:08:09.400881Z",
          "shell.execute_reply": "2025-03-25T12:08:09.399949Z",
          "shell.execute_reply.started": "2025-03-25T12:06:12.411811Z"
        },
        "id": "pQI4Arl_uFIv",
        "outputId": "fdaebd00-454d-492e-85de-a7f47b58b17a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 100.00% (2186/2186 courses)\n",
            "Processing complete. Processed 14289 learning objectives from 2186 courses.\n",
            "Output saved to courses_with_blooms_per_lo.csv\n",
            "   program_id                      course_title  \\\n",
            "0           0  Systems and Application Security   \n",
            "1           0  Systems and Application Security   \n",
            "2           0  Systems and Application Security   \n",
            "3           0  Systems and Application Security   \n",
            "4           0  Systems and Application Security   \n",
            "\n",
            "                                                 url  lo_num  \\\n",
            "0  https://www.coursera.org/learn/systems-and-app...       1   \n",
            "1  https://www.coursera.org/learn/systems-and-app...       2   \n",
            "2  https://www.coursera.org/learn/systems-and-app...       3   \n",
            "3  https://www.coursera.org/learn/systems-and-app...       4   \n",
            "4  https://www.coursera.org/learn/systems-and-app...       5   \n",
            "\n",
            "                                                  lo   bloom  \n",
            "0                   Systems and Application Security   apply  \n",
            "1        Course 7 - Systems and Application Security   apply  \n",
            "2  This is the seventh course under the specializ...   apply  \n",
            "3  This course discusses two major changes in rec...  create  \n",
            "4  First, we use our data on the go by means of d...  create  \n"
          ]
        }
      ],
      "source": [
        "# finding bloom tags for each learning objective\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Suppress TensorFlow and transformers warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')  # Suppress TensorFlow warnings\n",
        "\n",
        "# Suppress sentence transformers progress bars\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas(disable=True)\n",
        "\n",
        "# Initialize the MiniLM model for semantic embedding\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Bloom's Taxonomy categories and their example keywords\n",
        "blooms_keywords = {\n",
        "    'remember': ['define', 'identify', 'recall', 'recognize', 'state', 'memorize'],\n",
        "    'understand': ['comprehend', 'explain', 'summarize', 'infer', 'translate', 'discuss'],\n",
        "    'apply': ['apply', 'demonstrate', 'use', 'execute', 'implement', 'solve'],\n",
        "    'analyze': ['analyze', 'compare', 'contrast', 'distinguish', 'differentiate', 'examine'],\n",
        "    'evaluate': ['evaluate', 'justify', 'assess', 'support', 'judge', 'rate'],\n",
        "    'create': ['create', 'design', 'develop', 'generate', 'plan', 'formulate']\n",
        "}\n",
        "\n",
        "# Generate Bloom's category centroid embeddings\n",
        "def generate_bloom_centroids():\n",
        "    centroids = {}\n",
        "    for level, keywords in blooms_keywords.items():\n",
        "        embeddings = model.encode(keywords, show_progress_bar=False)\n",
        "        centroids[level] = np.mean(embeddings, axis=0)\n",
        "    return centroids\n",
        "\n",
        "bloom_centroids = generate_bloom_centroids()\n",
        "\n",
        "# Function to classify Bloom's level based on cosine similarity\n",
        "def classify_blooms_level(text):\n",
        "    \"\"\"Classify Bloom's Taxonomy level for a given learning objective.\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return None  # Skip empty LOs\n",
        "\n",
        "    text_embedding = model.encode([text], show_progress_bar=False)[0]\n",
        "    similarities = {level: cosine_similarity([text_embedding], [centroid])[0][0] for level, centroid in bloom_centroids.items()}\n",
        "\n",
        "    return max(similarities, key=similarities.get)\n",
        "\n",
        "# Function to transform input CSV and classify Bloom's taxonomy\n",
        "def transform_and_classify(input_file, output_file):\n",
        "    \"\"\"Transform CSV: Each LO gets its own row, with Bloom's classification, skipping empty LOs.\"\"\"\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    transformed_rows = []\n",
        "    total_courses = len(df)\n",
        "    total_lo_count = 0\n",
        "\n",
        "    for course_idx, row in df.iterrows():\n",
        "        course_title = row['course_title']\n",
        "        program_id = row['program_id']\n",
        "        url = row['url']  # Include URL\n",
        "        lo_list = row['lo'].split(';') if pd.notna(row['lo']) else []  # Handle NaN LO field\n",
        "\n",
        "        lo_num = 0  # Track valid LOs per course\n",
        "\n",
        "        for lo in lo_list:\n",
        "            lo = lo.strip()\n",
        "            if not lo:  # Skip empty LOs\n",
        "                continue\n",
        "\n",
        "            lo_num += 1  # Increment only for valid LOs\n",
        "            bloom_level = classify_blooms_level(lo)\n",
        "\n",
        "            transformed_rows.append({\n",
        "                'program_id': program_id,\n",
        "                'course_title': course_title,\n",
        "                'url': url,\n",
        "                'lo_num': lo_num,\n",
        "                'lo': lo,\n",
        "                'bloom': bloom_level\n",
        "            })\n",
        "\n",
        "            total_lo_count += 1\n",
        "\n",
        "        progress_percentage = (course_idx + 1) / total_courses * 100\n",
        "        sys.stdout.write(f'\\rProgress: {progress_percentage:.2f}% ({course_idx + 1}/{total_courses} courses)')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    transformed_df = pd.DataFrame(transformed_rows)\n",
        "\n",
        "\n",
        "    transformed_df.rename(columns={\n",
        "        \"bloom_category\": \"bloom_from_lo\",\n",
        "        \"module_bloom_category\": \"bloom_from_module\"\n",
        "    }, inplace=True)\n",
        "\n",
        "\n",
        "    # Save the final cleaned output\n",
        "    transformed_df.to_csv(output_file, index=False)\n",
        "\n",
        "    sys.stdout.write(f'\\nProcessing complete. Processed {total_lo_count} learning objectives from {total_courses} courses.\\n')\n",
        "    sys.stdout.write(f'Output saved to {output_file}\\n')\n",
        "\n",
        "    return transformed_df\n",
        "\n",
        "# Paths to input and output files\n",
        "input_file = 'course_scraped_with_lo.csv'\n",
        "output_file = 'courses_with_blooms_per_lo.csv'\n",
        "\n",
        "# Transform and classify learning objectives\n",
        "result_df = transform_and_classify(input_file, output_file)\n",
        "\n",
        "# Print the first few rows of the final output\n",
        "print(result_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-25T12:08:09.403471Z",
          "iopub.status.busy": "2025-03-25T12:08:09.403231Z",
          "iopub.status.idle": "2025-03-25T12:13:45.855535Z",
          "shell.execute_reply": "2025-03-25T12:13:45.854526Z",
          "shell.execute_reply.started": "2025-03-25T12:08:09.403450Z"
        },
        "id": "bqw3xyRsuFIv",
        "outputId": "2519a17e-0fbe-454c-adb1-eba67d7d863c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading MiniLM model...\n",
            "Loading CSV files...\n",
            "Processing 2186 courses...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing courses: 100%|██████████| 2186/2186 [17:05<00:00,  2.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analysis complete!\n",
            "Total processing time: 17.1 minutes (0.47 seconds per course)\n",
            "Summary results saved to 'course_module_alignment_summary_minilm.csv'\n",
            "Detailed results saved to 'course_module_alignment_detailed_minilm.csv'\n",
            "\n",
            "Summary statistics:\n",
            "Total courses analyzed: 2186\n",
            "Average coverage percentage: 62.12%\n",
            "Average mean max similarity: 0.5601\n",
            "Courses with good alignment: 1433/2185 (65.6%)\n"
          ]
        }
      ],
      "source": [
        "# similarity score (miniLM) - without blooms\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load the MiniLM model and tokenizer\n",
        "print(\"Loading MiniLM model...\")\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Mean Pooling function for creating sentence embeddings\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Function to encode texts using MiniLM\n",
        "def encode_texts(texts):\n",
        "    # Tokenize texts\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "    # Compute token embeddings\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "    # Perform mean pooling\n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "    # Normalize embeddings\n",
        "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "\n",
        "    return sentence_embeddings\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading CSV files...\")\n",
        "courses_df = pd.read_csv('course_scraped_with_lo.csv')\n",
        "modules_df = pd.read_csv('module_scraped_with_content.csv')\n",
        "\n",
        "# Define similarity threshold\n",
        "similarity_threshold = 0.5\n",
        "\n",
        "# Function to calculate alignment metrics\n",
        "def calculate_alignment_metrics(course_row):\n",
        "    # Extract learning objectives for this course\n",
        "    course_url = course_row['url']\n",
        "\n",
        "    if pd.isna(course_row['lo']) or course_row['lo'].strip() == '':\n",
        "        return {\n",
        "            'coverage_percentage': None,\n",
        "            'mean_max_similarity': None,\n",
        "            'num_learning_objectives': 0,\n",
        "            'num_modules': 0,\n",
        "            'num_module_contents': 0\n",
        "        }\n",
        "\n",
        "    learning_objectives = [lo.strip() for lo in course_row['lo'].split(';') if lo.strip()]\n",
        "\n",
        "    # Get all modules for this course\n",
        "    course_modules = modules_df[modules_df['url'] == course_url]\n",
        "\n",
        "    if len(course_modules) == 0 or len(learning_objectives) == 0:\n",
        "        return {\n",
        "            'coverage_percentage': None,\n",
        "            'mean_max_similarity': None,\n",
        "            'num_learning_objectives': len(learning_objectives),\n",
        "            'num_modules': 0,\n",
        "            'num_module_contents': 0\n",
        "        }\n",
        "\n",
        "    # Extract all module contents as individual items\n",
        "    all_module_contents = []\n",
        "    for _, module_row in course_modules.iterrows():\n",
        "        if pd.notna(module_row['module_content']) and module_row['module_content'].strip() != '':\n",
        "            contents = [content.strip() for content in module_row['module_content'].split(';') if content.strip()]\n",
        "            all_module_contents.extend(contents)\n",
        "\n",
        "    if len(all_module_contents) == 0:\n",
        "        return {\n",
        "            'coverage_percentage': 0.0,\n",
        "            'mean_max_similarity': 0.0,\n",
        "            'num_learning_objectives': len(learning_objectives),\n",
        "            'num_modules': len(course_modules),\n",
        "            'num_module_contents': 0\n",
        "        }\n",
        "\n",
        "    # Encode all learning objectives and module contents\n",
        "    lo_embeddings = encode_texts(learning_objectives)\n",
        "    module_embeddings = encode_texts(all_module_contents)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_scores = torch.mm(lo_embeddings, module_embeddings.T)\n",
        "\n",
        "    # Calculate metrics\n",
        "    max_similarities = []\n",
        "    covered_los = 0\n",
        "\n",
        "    # For each learning objective, find its highest similarity with any module content\n",
        "    detailed_results = []\n",
        "    for i in range(len(learning_objectives)):\n",
        "        # Find the highest similarity for this learning objective\n",
        "        max_sim_for_lo = torch.max(cosine_scores[i]).item()\n",
        "        max_similarities.append(max_sim_for_lo)\n",
        "\n",
        "        # Find the index of the best matching module content\n",
        "        max_sim_idx = torch.argmax(cosine_scores[i]).item()\n",
        "        best_match = all_module_contents[max_sim_idx]\n",
        "\n",
        "        # Check if this learning objective is covered by any module content\n",
        "        is_covered = max_sim_for_lo >= similarity_threshold\n",
        "        if is_covered:\n",
        "            covered_los += 1\n",
        "\n",
        "        # Store detailed result for this learning objective\n",
        "        detailed_results.append({\n",
        "            'learning_objective': learning_objectives[i],\n",
        "            'max_similarity': max_sim_for_lo,\n",
        "            'best_match_content': best_match,\n",
        "            'is_covered': is_covered\n",
        "        })\n",
        "\n",
        "    coverage_percentage = (covered_los / len(learning_objectives)) * 100 if learning_objectives else 0\n",
        "    mean_max_similarity = np.mean(max_similarities) if max_similarities else 0\n",
        "\n",
        "    return {\n",
        "        'coverage_percentage': coverage_percentage,\n",
        "        'mean_max_similarity': mean_max_similarity,\n",
        "        'num_learning_objectives': len(learning_objectives),\n",
        "        'num_modules': len(course_modules),\n",
        "        'num_module_contents': len(all_module_contents),\n",
        "        'detailed_results': detailed_results\n",
        "    }\n",
        "\n",
        "# Process each course\n",
        "print(f\"Processing {len(courses_df)} courses...\")\n",
        "start_time = time.time()\n",
        "results = []\n",
        "detailed_all = []\n",
        "\n",
        "# Process in batches to avoid memory issues\n",
        "batch_size = 10\n",
        "num_batches = (len(courses_df) + batch_size - 1)//batch_size\n",
        "\n",
        "# Set up progress tracking with just a simple progress bar\n",
        "with tqdm(total=len(courses_df), desc=\"Analyzing courses\") as pbar:\n",
        "    for i in range(0, len(courses_df), batch_size):\n",
        "        batch = courses_df.iloc[i:i+batch_size]\n",
        "\n",
        "        for idx, course_row in batch.iterrows():\n",
        "            metrics = calculate_alignment_metrics(course_row)\n",
        "\n",
        "            result = {\n",
        "                'program_id': course_row['program_id'],\n",
        "                'course_title': course_row['course_title'],\n",
        "                'url': course_row['url'],\n",
        "                'num_learning_objectives': metrics['num_learning_objectives'],\n",
        "                'num_modules': metrics['num_modules'],\n",
        "                'num_module_contents': metrics['num_module_contents'],\n",
        "                'coverage_percentage': metrics['coverage_percentage'],\n",
        "                'mean_max_similarity': metrics['mean_max_similarity']\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Create detailed results for this course\n",
        "            if 'detailed_results' in metrics:\n",
        "                for detail in metrics['detailed_results']:\n",
        "                    detailed = {\n",
        "                        'program_id': course_row['program_id'],\n",
        "                        'course_title': course_row['course_title'],\n",
        "                        'url': course_row['url'],\n",
        "                        'learning_objective': detail['learning_objective'],\n",
        "                        'max_similarity': detail['max_similarity'],\n",
        "                        'best_match_content': detail['best_match_content'],\n",
        "                        'is_covered': detail['is_covered']\n",
        "                    }\n",
        "                    detailed_all.append(detailed)\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "# Create results DataFrames\n",
        "results_df = pd.DataFrame(results)\n",
        "detailed_df = pd.DataFrame(detailed_all)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('course_module_alignment_summary_minilm.csv', index=False)\n",
        "detailed_df.to_csv('course_module_alignment_detailed_minilm.csv', index=False)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\nAnalysis complete!\")\n",
        "print(f\"Total processing time: {total_time/60:.1f} minutes ({total_time/len(courses_df):.2f} seconds per course)\")\n",
        "print(f\"Summary results saved to 'course_module_alignment_summary_minilm.csv'\")\n",
        "print(f\"Detailed results saved to 'course_module_alignment_detailed_minilm.csv'\")\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(f\"Total courses analyzed: {len(results_df)}\")\n",
        "valid_results = results_df[results_df['coverage_percentage'].notna()]\n",
        "if len(valid_results) > 0:\n",
        "    print(f\"Average coverage percentage: {valid_results['coverage_percentage'].mean():.2f}%\")\n",
        "    print(f\"Average mean max similarity: {valid_results['mean_max_similarity'].mean():.4f}\")\n",
        "\n",
        "    # Count courses with good alignment (>50% coverage and >0.5 mean similarity)\n",
        "    good_alignment = valid_results[\n",
        "        (valid_results['coverage_percentage'] >= 50) &\n",
        "        (valid_results['mean_max_similarity'] >= 0.5)\n",
        "    ]\n",
        "    print(f\"Courses with good alignment: {len(good_alignment)}/{len(valid_results)} ({len(good_alignment)/len(valid_results)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jQcaUc3uFIv",
        "outputId": "09d3fd41-a2a0-425d-87e3-9de98094b949",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 100.00% (14285/14285 courses)\n",
            "Processing complete. Processed 14285 content from 14285 courses.\n",
            "Output saved to course_module_alignment_detailed_minilm.csv\n",
            "   program_id                      course_title  \\\n",
            "0           0  Systems and Application Security   \n",
            "1           0  Systems and Application Security   \n",
            "2           0  Systems and Application Security   \n",
            "3           0  Systems and Application Security   \n",
            "4           0  Systems and Application Security   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.coursera.org/learn/systems-and-app...   \n",
            "1  https://www.coursera.org/learn/systems-and-app...   \n",
            "2  https://www.coursera.org/learn/systems-and-app...   \n",
            "3  https://www.coursera.org/learn/systems-and-app...   \n",
            "4  https://www.coursera.org/learn/systems-and-app...   \n",
            "\n",
            "                                  learning_objective  max_similarity  \\\n",
            "0                   Systems and Application Security        0.527121   \n",
            "1        Course 7 - Systems and Application Security        0.491941   \n",
            "2  This is the seventh course under the specializ...        0.240702   \n",
            "3  This course discusses two major changes in rec...        0.271610   \n",
            "4  First, we use our data on the go by means of d...        0.294011   \n",
            "\n",
            "                    best_match_content  is_covered   bloom  \n",
            "0    Security Strategies for Endpoints        True   apply  \n",
            "1    Security Strategies for Endpoints       False   apply  \n",
            "2  Essential Requirements in P&DP Laws       False  create  \n",
            "3                     Virtual Machines       False  create  \n",
            "4    Security Strategies for Endpoints       False   apply  \n"
          ]
        }
      ],
      "source": [
        "# find bloom tag for best-match module content of each learning objectives\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Suppress TensorFlow and transformers warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')  # Suppress TensorFlow warnings\n",
        "\n",
        "# Suppress sentence transformers progress bars\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas(disable=True)\n",
        "\n",
        "# Initialize the MiniLM model for semantic embedding\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Bloom's Taxonomy categories and their example keywords\n",
        "blooms_keywords = {\n",
        "    'remember': ['define', 'identify', 'recall', 'recognize', 'state', 'memorize'],\n",
        "    'understand': ['comprehend', 'explain', 'summarize', 'infer', 'translate', 'discuss'],\n",
        "    'apply': ['apply', 'demonstrate', 'use', 'execute', 'implement', 'solve'],\n",
        "    'analyze': ['analyze', 'compare', 'contrast', 'distinguish', 'differentiate', 'examine'],\n",
        "    'evaluate': ['evaluate', 'justify', 'assess', 'support', 'judge', 'rate'],\n",
        "    'create': ['create', 'design', 'develop', 'generate', 'plan', 'formulate']\n",
        "}\n",
        "\n",
        "# Generate Bloom's category centroid embeddings\n",
        "def generate_bloom_centroids():\n",
        "    centroids = {}\n",
        "    for level, keywords in blooms_keywords.items():\n",
        "        embeddings = model.encode(keywords, show_progress_bar=False)\n",
        "        centroids[level] = np.mean(embeddings, axis=0)\n",
        "    return centroids\n",
        "\n",
        "bloom_centroids = generate_bloom_centroids()\n",
        "\n",
        "# Function to classify Bloom's level based on cosine similarity\n",
        "def classify_blooms_level(text):\n",
        "    \"\"\"Classify Bloom's Taxonomy level for a given text.\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return None  # Skip empty texts\n",
        "\n",
        "    text_embedding = model.encode([text], show_progress_bar=False)[0]\n",
        "    similarities = {level: cosine_similarity([text_embedding], [centroid])[0][0] for level, centroid in bloom_centroids.items()}\n",
        "\n",
        "    return max(similarities, key=similarities.get)\n",
        "\n",
        "# Function to transform input CSV and classify Bloom's taxonomy for `best_match_content`\n",
        "def transform_and_classify(input_file, output_file):\n",
        "    \"\"\"Transform CSV: Each row gets Bloom's classification added.\"\"\"\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    transformed_rows = []\n",
        "    total_courses = len(df)\n",
        "    total_content_count = 0\n",
        "\n",
        "    for course_idx, row in df.iterrows():\n",
        "        # Keep all original columns\n",
        "        program_id = row['program_id']\n",
        "        course_title = row['course_title']\n",
        "        url = row['url']\n",
        "        learning_objective = row['learning_objective']\n",
        "        max_similarity = row['max_similarity']\n",
        "        best_match_content = row['best_match_content']\n",
        "        is_covered = row['is_covered']\n",
        "\n",
        "        if pd.notna(best_match_content):  # Only process if `best_match_content` is not NaN\n",
        "            best_match_content = best_match_content.strip()\n",
        "\n",
        "            # Classify Bloom's taxonomy level\n",
        "            bloom_level = classify_blooms_level(best_match_content)\n",
        "\n",
        "            transformed_rows.append({\n",
        "                'program_id': program_id,\n",
        "                'course_title': course_title,\n",
        "                'url': url,\n",
        "                'learning_objective': learning_objective,\n",
        "                'max_similarity': max_similarity,\n",
        "                'best_match_content': best_match_content,\n",
        "                'is_covered': is_covered,\n",
        "                'bloom': bloom_level\n",
        "            })\n",
        "\n",
        "            total_content_count += 1\n",
        "\n",
        "        progress_percentage = (course_idx + 1) / total_courses * 100\n",
        "        sys.stdout.write(f'\\rProgress: {progress_percentage:.2f}% ({course_idx + 1}/{total_courses} courses)')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    transformed_df = pd.DataFrame(transformed_rows)\n",
        "\n",
        "    # Save the final cleaned output\n",
        "    transformed_df.to_csv(output_file, index=False)\n",
        "\n",
        "    sys.stdout.write(f'\\nProcessing complete. Processed {total_content_count} content from {total_courses} courses.\\n')\n",
        "    sys.stdout.write(f'Output saved to {output_file}\\n')\n",
        "\n",
        "    return transformed_df\n",
        "\n",
        "# Paths to input and output files (same file for input and output)\n",
        "input_file = 'course_module_alignment_detailed_minilm.csv'\n",
        "output_file = input_file  # Same as input file to overwrite it\n",
        "\n",
        "# Transform and classify best_match_content\n",
        "result_df = transform_and_classify(input_file, output_file)\n",
        "\n",
        "# Print the first few rows of the final output\n",
        "print(result_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-25T12:50:15.079453Z",
          "iopub.status.busy": "2025-03-25T12:50:15.079147Z",
          "iopub.status.idle": "2025-03-25T12:50:30.359944Z",
          "shell.execute_reply": "2025-03-25T12:50:30.359161Z",
          "shell.execute_reply.started": "2025-03-25T12:50:15.079430Z"
        },
        "id": "79FNNBoeuFIw",
        "outputId": "2c2581c7-40b9-45f5-d572-5d372fde20de",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analysis complete!\n",
            "Summary results saved to 'course_module_alignment_summary_minilm_bloom.csv'\n",
            "Detailed results saved to 'course_module_alignment_detailed_minilm_bloom.csv'\n",
            "\n",
            "Summary statistics:\n",
            "Total courses analyzed: 1992\n",
            "Courses penalized by Bloom tags: 1683/1992 (84.5%)\n",
            "Average coverage percentage: 58.54%\n",
            "Average mean max similarity: 0.5422\n",
            "Courses with good alignment: 1045/1992 (52.5%)\n"
          ]
        }
      ],
      "source": [
        "# adjust similarity score based on bloom taxonomy tag alignment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Bloom's Taxonomy complexity mapping\n",
        "blooms_complexity = {\n",
        "    'remember': 1,\n",
        "    'understand': 2,\n",
        "    'apply': 3,\n",
        "    'analyze': 4,\n",
        "    'evaluate': 5,\n",
        "    'create': 6\n",
        "}\n",
        "\n",
        "# Function to safely extract Bloom's complexity\n",
        "def get_bloom_complexity(bloom_tag):\n",
        "    if pd.isna(bloom_tag) or not isinstance(bloom_tag, str):\n",
        "        return 1  # Default to lowest complexity\n",
        "\n",
        "    # Convert to lowercase\n",
        "    bloom_tag = bloom_tag.lower()\n",
        "\n",
        "    # Direct match first\n",
        "    if bloom_tag in blooms_complexity:\n",
        "        return blooms_complexity[bloom_tag]\n",
        "\n",
        "    # Fallback to most specific match\n",
        "    for level, complexity in blooms_complexity.items():\n",
        "        if level in bloom_tag:\n",
        "            return complexity\n",
        "\n",
        "    return 1  # Default to lowest complexity if no match\n",
        "\n",
        "\n",
        "def adjust_similarity_with_bloom(max_similarity, lo_complexity, module_complexity):\n",
        "    complexity_diff = abs(lo_complexity - module_complexity)\n",
        "\n",
        "    # Only apply penalty if there's a complexity difference\n",
        "    if complexity_diff == 0:\n",
        "        return max_similarity\n",
        "\n",
        "    # Adjust logarithm-based penalty with max 5% reduction\n",
        "    penalty_factor = max(0.95, 1.0 - (math.log1p(complexity_diff) / (math.log1p(6) * 1.5)))\n",
        "\n",
        "    # Log the penalty factor adjustment\n",
        "    # print(f\"Complexity Difference: {complexity_diff}, Penalty Factor: {penalty_factor:.4f}\")\n",
        "\n",
        "    adjusted_similarity = max_similarity * penalty_factor\n",
        "\n",
        "    # Log the adjustment amount\n",
        "    adjustment = max_similarity - adjusted_similarity\n",
        "    # if adjustment > 0:\n",
        "    #     print(f\"Adjustment applied: {adjustment:.4f} (Original: {max_similarity:.4f}, Adjusted: {adjusted_similarity:.4f})\")\n",
        "\n",
        "    return adjusted_similarity\n",
        "\n",
        "# Adjust the code to include complexity for the modules as well\n",
        "\n",
        "def refine_module_alignment(courses_with_blooms_file,\n",
        "                             minilm_detailed_file,\n",
        "                             minilm_summary_file,\n",
        "                             output_detailed_file,\n",
        "                             output_summary_file):\n",
        "    # Load input files\n",
        "    courses_df = pd.read_csv(courses_with_blooms_file)\n",
        "    detailed_df = pd.read_csv(minilm_detailed_file)\n",
        "    summary_df = pd.read_csv(minilm_summary_file)\n",
        "\n",
        "    # Prepare to store updated detailed results\n",
        "    updated_detailed_results = []\n",
        "\n",
        "    # Similarity threshold\n",
        "    similarity_threshold = 0.5\n",
        "\n",
        "    # Track courses penalized due to Bloom tags\n",
        "    courses_penalized_by_bloom = set()\n",
        "    courses_with_bloom_penalty = []\n",
        "\n",
        "    # Process each unique course\n",
        "    for url in detailed_df['url'].unique():\n",
        "        # Filter detailed and course data for this URL\n",
        "        url_detailed = detailed_df[detailed_df['url'] == url]\n",
        "        url_courses = courses_df[courses_df['url'] == url]\n",
        "\n",
        "        # Find corresponding module Bloom tag and calculate module complexity\n",
        "        module_bloom_tag = url_courses['bloom'].iloc[0] if len(url_courses) > 0 else None\n",
        "        module_complexity = get_bloom_complexity(module_bloom_tag)\n",
        "\n",
        "        # Flag to check if this course is penalized\n",
        "        course_penalized = False\n",
        "        course_penalties = []\n",
        "\n",
        "        # Process each learning objective\n",
        "        for _, lo_row in url_detailed.iterrows():\n",
        "            # Find corresponding Bloom's tag\n",
        "            bloom_match = url_courses[url_courses['lo'] == lo_row['learning_objective']]\n",
        "\n",
        "            # Prepare bloom tag and complexity for LO\n",
        "            bloom_tag = bloom_match['bloom'].values[0] if len(bloom_match) > 0 else None\n",
        "            lo_complexity = get_bloom_complexity(bloom_tag)\n",
        "\n",
        "            # Calculate original and adjusted similarity\n",
        "            original_similarity = lo_row['max_similarity']\n",
        "            adjusted_similarity = adjust_similarity_with_bloom(\n",
        "                lo_row['max_similarity'],\n",
        "                lo_complexity,\n",
        "                module_complexity  # Use module complexity in the adjustment\n",
        "            )\n",
        "\n",
        "            # Check if penalized\n",
        "            if adjusted_similarity < original_similarity:\n",
        "                course_penalized = True\n",
        "                penalty_percentage = ((original_similarity - adjusted_similarity) / original_similarity) * 100\n",
        "                course_penalties.append({\n",
        "                    'learning_objective': lo_row['learning_objective'],\n",
        "                    'bloom_tag': bloom_tag,\n",
        "                    'original_similarity': original_similarity,\n",
        "                    'adjusted_similarity': adjusted_similarity,\n",
        "                    'penalty_percentage': penalty_percentage\n",
        "                })\n",
        "\n",
        "            # Determine if covered based on adjusted similarity threshold\n",
        "            is_covered = adjusted_similarity >= similarity_threshold\n",
        "\n",
        "            # Prepare updated detailed result\n",
        "            updated_result = lo_row.to_dict()\n",
        "            updated_result['bloom_tag'] = bloom_tag\n",
        "            updated_result['lo_complexity'] = lo_complexity\n",
        "            updated_result['module_complexity'] = module_complexity  # Add module complexity to the result\n",
        "            updated_result['max_similarity'] = adjusted_similarity\n",
        "            updated_result['is_covered'] = is_covered\n",
        "\n",
        "            updated_detailed_results.append(updated_result)\n",
        "\n",
        "        # Track courses with penalties\n",
        "        if course_penalized:\n",
        "            courses_penalized_by_bloom.add(url)\n",
        "            courses_with_bloom_penalty.append({\n",
        "                'url': url,\n",
        "                'penalties': course_penalties\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame maintaining original column order\n",
        "    updated_detailed_df = pd.DataFrame(updated_detailed_results)\n",
        "\n",
        "    # Recalculate summary metrics\n",
        "    summary_results = []\n",
        "\n",
        "    for url in updated_detailed_df['url'].unique():\n",
        "        url_detailed = updated_detailed_df[updated_detailed_df['url'] == url]\n",
        "\n",
        "        # Recalculate metrics\n",
        "        num_los = len(url_detailed)\n",
        "        num_covered_los = sum(url_detailed['is_covered'])\n",
        "        coverage_percentage = (num_covered_los / num_los) * 100 if num_los > 0 else 0\n",
        "        mean_max_similarity = url_detailed['max_similarity'].mean()\n",
        "\n",
        "        # Find corresponding summary row\n",
        "        summary_row = summary_df[summary_df['url'] == url].iloc[0].to_dict()\n",
        "\n",
        "        # Update key metrics\n",
        "        summary_row['coverage_percentage'] = coverage_percentage\n",
        "        summary_row['mean_max_similarity'] = mean_max_similarity\n",
        "\n",
        "        summary_results.append(summary_row)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    updated_summary_df = pd.DataFrame(summary_results)\n",
        "\n",
        "    # Save updated files\n",
        "    updated_detailed_df.to_csv(output_detailed_file, index=False)\n",
        "    updated_summary_df.to_csv(output_summary_file, index=False)\n",
        "\n",
        "    # Print summary statistics in the requested format\n",
        "    total_courses = len(updated_summary_df)\n",
        "    avg_coverage_percentage = updated_summary_df['coverage_percentage'].mean()\n",
        "    avg_mean_max_similarity = updated_summary_df['mean_max_similarity'].mean()\n",
        "\n",
        "    # Calculate number of courses with good alignment\n",
        "    courses_with_good_alignment = updated_summary_df[updated_summary_df['coverage_percentage'] >= 60]\n",
        "    num_courses_with_good_alignment = len(courses_with_good_alignment)\n",
        "\n",
        "    # Number of courses penalized by Bloom tags\n",
        "    num_courses_penalized = len(courses_penalized_by_bloom)\n",
        "\n",
        "    print(f\"\\nAnalysis complete!\")\n",
        "    print(f\"Summary results saved to '{output_summary_file}'\")\n",
        "    print(f\"Detailed results saved to '{output_detailed_file}'\\n\")\n",
        "\n",
        "    print(f\"Summary statistics:\")\n",
        "    print(f\"Total courses analyzed: {total_courses}\")\n",
        "    print(f\"Courses penalized by Bloom tags: {num_courses_penalized}/{total_courses} ({(num_courses_penalized / total_courses) * 100:.1f}%)\")\n",
        "    print(f\"Average coverage percentage: {avg_coverage_percentage:.2f}%\")\n",
        "    print(f\"Average mean max similarity: {avg_mean_max_similarity:.4f}\")\n",
        "    print(f\"Courses with good alignment: {num_courses_with_good_alignment}/{total_courses} ({(num_courses_with_good_alignment / total_courses) * 100:.1f}%)\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    courses_with_blooms_file = 'courses_with_blooms_per_lo.csv'\n",
        "    minilm_detailed_file = 'course_module_alignment_detailed_minilm.csv'\n",
        "    minilm_summary_file = 'course_module_alignment_summary_minilm.csv'\n",
        "\n",
        "    output_detailed_file = 'course_module_alignment_detailed_minilm_bloom.csv'\n",
        "    output_summary_file = 'course_module_alignment_summary_minilm_bloom.csv'\n",
        "\n",
        "    refine_module_alignment(\n",
        "        courses_with_blooms_file,\n",
        "        minilm_detailed_file,\n",
        "        minilm_summary_file,\n",
        "        output_detailed_file,\n",
        "        output_summary_file\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUof7aPQ-f0E",
        "outputId": "402a180c-d627-4ad1-a765-c2b19164cb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "course_module_alignment_detailed_minilm_bloom.csv organised\n"
          ]
        }
      ],
      "source": [
        "# Reorganise the csv output for clarity\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('course_module_alignment_detailed_minilm_bloom.csv')\n",
        "\n",
        "# 1. Rename the columns\n",
        "df = df.rename(columns={\n",
        "    'bloom': 'module_bloom',\n",
        "    'bloom_tag': 'lo_bloom'\n",
        "})\n",
        "\n",
        "# 2. Swap columns H and I (assuming H is 'module_bloom' and I is 'lo_bloom' after renaming)\n",
        "# Get the list of column names\n",
        "cols = df.columns.tolist()\n",
        "\n",
        "# Find the positions of the columns to swap\n",
        "h_pos = cols.index('module_bloom')\n",
        "i_pos = cols.index('lo_bloom')\n",
        "\n",
        "# Swap the positions in the columns list\n",
        "cols[h_pos], cols[i_pos] = cols[i_pos], cols[h_pos]\n",
        "\n",
        "# Reorder the dataframe columns\n",
        "df = df[cols]\n",
        "\n",
        "# Save the modified dataframe to a new CSV file\n",
        "df.to_csv('course_module_alignment_detailed_minilm_bloom.csv', index=False)\n",
        "\n",
        "print('course_module_alignment_detailed_minilm_bloom.csv organised')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6954376,
          "sourceId": 11147491,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6954378,
          "sourceId": 11147496,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
