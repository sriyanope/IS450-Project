{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "O4n_nFbHDXOr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import glob\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################################\n",
        "# PART 1: Find and load all CSV files\n",
        "#################################################\n",
        "print(\"STEP 1: Finding and loading CSV files\")\n",
        "csv_files = glob.glob('*.csv')\n",
        "print(f\"Found {len(csv_files)} CSV files: {csv_files}\")\n",
        "\n",
        "if not csv_files:\n",
        "    print(\"No CSV files found in the current directory.\")\n",
        "    exit()\n",
        "\n",
        "csv_dataframes = {}\n",
        "for filename in csv_files:\n",
        "    try:\n",
        "        csv_dataframes[filename] = pd.read_csv(filename)\n",
        "        print(f\"Loaded {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filename}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5XCfdvzDzw6",
        "outputId": "d01092c1-3ed1-4858-c907-f11e69ba4c49"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 1: Finding and loading CSV files\n",
            "Found 8 CSV files: ['use_summary.csv', 'module_scraped_with_content.csv', 'sbert_detailed.csv', 'bloom_detailed.csv', 'sbert_summary.csv', 'course_scraped_with_lo.csv', 'use_detailed.csv', 'bloom_summary.csv']\n",
            "Loaded use_summary.csv\n",
            "Loaded module_scraped_with_content.csv\n",
            "Loaded sbert_detailed.csv\n",
            "Loaded bloom_detailed.csv\n",
            "Loaded sbert_summary.csv\n",
            "Loaded course_scraped_with_lo.csv\n",
            "Loaded use_detailed.csv\n",
            "Loaded bloom_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################################\n",
        "# PART 2: Extract and randomly select URLs from specified CSV\n",
        "#################################################\n",
        "print(\"\\nSTEP 2: Extracting URLs from course_scraped_with_lo.csv\")\n",
        "# Define the specific source file\n",
        "source_file = \"course_scraped_with_lo.csv\"\n",
        "\n",
        "# Check if specified source file exists in the loaded dataframes\n",
        "if source_file not in csv_dataframes:\n",
        "    print(f\"Error: {source_file} not found in directory.\")\n",
        "    exit()\n",
        "\n",
        "# Check if 'url' column exists in the source file\n",
        "if 'url' not in csv_dataframes[source_file].columns:\n",
        "    print(f\"Error: No 'url' column found in {source_file}.\")\n",
        "    exit()\n",
        "\n",
        "# Extract URLs from the specified source\n",
        "source_urls = csv_dataframes[source_file]['url'].tolist()\n",
        "# Remove duplicates from the URL list\n",
        "source_urls = list(dict.fromkeys(source_urls))\n",
        "print(f\"Found {len(source_urls)} unique URLs in {source_file}\")\n",
        "\n",
        "if not source_urls:\n",
        "    print(\"No URLs found in the source CSV file.\")\n",
        "    exit()\n",
        "\n",
        "# Select random URLs\n",
        "print(\"\\nSTEP 3: Randomly selecting URLs\")\n",
        "num_urls_to_select = min(10, len(source_urls))  # Select urls\n",
        "selected_urls = random.sample(source_urls, num_urls_to_select)\n",
        "print(f\"Randomly selected {num_urls_to_select} URLs from {source_file}:\")\n",
        "for i, url in enumerate(selected_urls, 1):\n",
        "    print(f\"{i}. {url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6hCJoG2D10C",
        "outputId": "2f6f454c-6d42-4d51-d07f-d28298079c55"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 2: Extracting URLs from course_scraped_with_lo.csv\n",
            "Found 1993 unique URLs in course_scraped_with_lo.csv\n",
            "\n",
            "STEP 3: Randomly selecting URLs\n",
            "Randomly selected 10 URLs from course_scraped_with_lo.csv:\n",
            "1. https://www.coursera.org/learn/uva-darden-agile-analytics\n",
            "2. https://www.coursera.org/learn/craft-of-plot\n",
            "3. https://www.coursera.org/learn/dsp1\n",
            "4. https://www.coursera.org/learn/investment-banking-mergers-acquisitions-ipos\n",
            "5. https://www.coursera.org/learn/google-docs\n",
            "6. https://www.coursera.org/learn/healthcare-data-literacy\n",
            "7. https://www.coursera.org/learn/video-game-story\n",
            "8. https://www.coursera.org/learn/user-interface-in-game-design\n",
            "9. https://www.coursera.org/learn/java-object-oriented-programming\n",
            "10. https://www.coursera.org/learn/web-design-strategy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################################\n",
        "# PART 3: Search for matches in all CSV files\n",
        "#################################################\n",
        "print(\"\\nSTEP 4: Searching for matches in all CSVs\")\n",
        "# Dictionary to store results by filename\n",
        "results_by_file = {}\n",
        "match_found = False\n",
        "\n",
        "# Track processed rows to avoid duplicates\n",
        "processed_rows = {}  # Dictionary to track processed rows by URL and file\n",
        "\n",
        "for url in selected_urls:\n",
        "    print(f\"\\nSearching for URL: {url}\")\n",
        "    url_found = False\n",
        "\n",
        "    for filename, df in csv_dataframes.items():\n",
        "        if 'url' in df.columns:\n",
        "            # Perform the search\n",
        "            matches = df[df['url'] == url]\n",
        "\n",
        "            if not matches.empty:\n",
        "                url_found = True\n",
        "                match_found = True\n",
        "\n",
        "                # Initialize tracking for this file if needed\n",
        "                if filename not in processed_rows:\n",
        "                    processed_rows[filename] = set()\n",
        "\n",
        "                # Initialize this file's results list if needed\n",
        "                if filename not in results_by_file:\n",
        "                    results_by_file[filename] = []\n",
        "\n",
        "                # Filter and process unique matches\n",
        "                unique_matches = 0\n",
        "                for idx, row in matches.iterrows():\n",
        "                    # Create a hashable representation of the row for duplicate checking\n",
        "                    row_hash = hash(tuple(row.items()))\n",
        "\n",
        "                    # Only process if we haven't seen this exact row before\n",
        "                    if row_hash not in processed_rows[filename]:\n",
        "                        processed_rows[filename].add(row_hash)\n",
        "                        unique_matches += 1\n",
        "                        results_by_file[filename].append(row.to_dict())\n",
        "\n",
        "\n",
        "    if not url_found:\n",
        "        print(f\"  Warning: URL {url} was not found in any CSV\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KC4IXsAD3nS",
        "outputId": "9e15216b-548e-43f7-9f64-21d2d0fd1f43"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 4: Searching for matches in all CSVs\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/uva-darden-agile-analytics\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/craft-of-plot\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/dsp1\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/investment-banking-mergers-acquisitions-ipos\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/google-docs\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/healthcare-data-literacy\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/video-game-story\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/user-interface-in-game-design\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/java-object-oriented-programming\n",
            "\n",
            "Searching for URL: https://www.coursera.org/learn/web-design-strategy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################################\n",
        "# PART 4: Create Excel output with sheets in specific order\n",
        "#################################################\n",
        "print(\"\\nSTEP 5: Creating Excel output with sheets in specified order\")\n",
        "if match_found:\n",
        "    output_filename = \"url_selection_results.xlsx\"\n",
        "\n",
        "    # Define the desired sheet order\n",
        "    sheet_order = [\n",
        "        \"Summary\",  # Summary sheet is always first\n",
        "        \"course_scraped_with_lo\",\n",
        "        \"module_scraped_with_content\",\n",
        "        \"sbert_summary\",\n",
        "        \"sbert_detailed\",\n",
        "        \"use_summary\",\n",
        "        \"use_detailed\",\n",
        "        \"bloom_summary\",\n",
        "        \"bloom_detailed\"\n",
        "    ]\n",
        "\n",
        "    # Create a new Excel writer object\n",
        "    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
        "        # Create the summary sheet first\n",
        "        summary_df = pd.DataFrame({'Selected URLs': selected_urls})\n",
        "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
        "        print(f\"Created sheet 'Summary' with {len(selected_urls)} selected URLs\")\n",
        "\n",
        "        # Create sheets in the specified order\n",
        "        for sheet_name in sheet_order[1:]:  # Skip 'Summary' as we already created it\n",
        "            # Find the corresponding CSV file\n",
        "            matching_files = [f for f in results_by_file.keys() if sheet_name in f]\n",
        "\n",
        "            if matching_files:\n",
        "                filename = matching_files[0]  # Use the first matching file\n",
        "                matches = results_by_file[filename]\n",
        "\n",
        "                if matches:  # Only create sheets if there are matches\n",
        "                    # Convert list of dictionaries to DataFrame\n",
        "                    result_df = pd.DataFrame(matches)\n",
        "\n",
        "                    # Double-check for any remaining duplicates and remove them\n",
        "                    result_df = result_df.drop_duplicates()\n",
        "\n",
        "                    # Write DataFrame to Excel sheet\n",
        "                    sheet_name_excel = sheet_name[:31]  # Excel sheet names max 31 chars\n",
        "                    result_df.to_excel(writer, sheet_name=sheet_name_excel, index=False)\n",
        "                    print(f\"Created sheet '{sheet_name_excel}' with {len(result_df)} unique matches\")\n",
        "\n",
        "        # Add any remaining files that weren't in the specified order\n",
        "        remaining_files = [f for f in results_by_file.keys() if not any(sheet in f for sheet in sheet_order[1:])]\n",
        "        for filename in remaining_files:\n",
        "            matches = results_by_file[filename]\n",
        "\n",
        "            if matches:  # Only create sheets if there are matches\n",
        "                # Convert list of dictionaries to DataFrame\n",
        "                result_df = pd.DataFrame(matches)\n",
        "\n",
        "                # Double-check for any remaining duplicates and remove them\n",
        "                result_df = result_df.drop_duplicates()\n",
        "\n",
        "                # Write DataFrame to Excel sheet\n",
        "                sheet_name = os.path.splitext(filename)[0][:31]  # Excel sheet names max 31 chars\n",
        "                result_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "                print(f\"Created sheet '{sheet_name}' with {len(result_df)} unique matches\")\n",
        "\n",
        "    print(f\"\\nResults saved to {output_filename}\")\n",
        "else:\n",
        "    print(\"No matching results found for any of the selected URLs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eizzR6SHD4r7",
        "outputId": "86eb9b35-3689-4bbe-e3d5-bb252f2a2265"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 5: Creating Excel output with sheets in specified order\n",
            "Created sheet 'Summary' with 10 selected URLs\n",
            "Created sheet 'course_scraped_with_lo' with 11 unique matches\n",
            "Created sheet 'module_scraped_with_content' with 45 unique matches\n",
            "Created sheet 'sbert_summary' with 11 unique matches\n",
            "Created sheet 'sbert_detailed' with 102 unique matches\n",
            "Created sheet 'use_summary' with 11 unique matches\n",
            "Created sheet 'use_detailed' with 102 unique matches\n",
            "Created sheet 'bloom_summary' with 10 unique matches\n",
            "Created sheet 'bloom_detailed' with 102 unique matches\n",
            "\n",
            "Results saved to url_selection_results.xlsx\n"
          ]
        }
      ]
    }
  ]
}