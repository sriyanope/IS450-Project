{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import course_scraped and process it to extract lo."
      ],
      "metadata": {
        "id": "f1-QfUnWKfFu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('course_scraped.csv')\n",
        "\n",
        "# Get stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Add additional single-word stopwords that might appear in learning objectives\n",
        "additional_stopwords = {'who', 'what', 'when', 'where', 'why', 'how', 'this', 'that', 'these', 'those',\n",
        "                       'here', 'there', 'with', 'from', 'into', 'onto', 'upon', 'about'}\n",
        "stop_words.update(additional_stopwords)\n",
        "\n",
        "# Function to clean text by removing leading hyphens and extra whitespace\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove leading hyphens and whitespace from the text\n",
        "    cleaned = re.sub(r'^\\s*-\\s*', '', text.strip())\n",
        "    # Remove extra whitespace\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
        "    return cleaned.strip()\n",
        "\n",
        "# Function to filter out single words and stopwords\n",
        "def filter_meaningless_objectives(objectives):\n",
        "    filtered_objectives = []\n",
        "\n",
        "    for obj in objectives:\n",
        "        # Skip if it's a single word and in stopwords\n",
        "        words = obj.split()\n",
        "        if len(words) <= 1 and obj.lower() in stop_words:\n",
        "            continue\n",
        "        # Skip if it's entirely made up of stopwords\n",
        "        if all(word.lower() in stop_words for word in words):\n",
        "            continue\n",
        "        # Otherwise keep it\n",
        "        filtered_objectives.append(obj)\n",
        "\n",
        "    return filtered_objectives\n",
        "\n",
        "# Function to extract learning objectives from text\n",
        "def extract_learning_objectives(row):\n",
        "    learning_objectives = []\n",
        "\n",
        "    # Check if lo_description or lo_skills have content\n",
        "    lo_description_exists = pd.notna(row['lo_description']) and row['lo_description'].strip() != ''\n",
        "    lo_skills_exists = pd.notna(row['lo_skills']) and row['lo_skills'].strip() != ''\n",
        "\n",
        "    # Priority 1: Extract from lo_description and lo_skills if available\n",
        "    if lo_description_exists:\n",
        "        # Split by semicolons\n",
        "        objectives = row['lo_description'].split(';')\n",
        "        # Clean each objective\n",
        "        objectives = [clean_text(obj) for obj in objectives]\n",
        "        # Filter out empty strings\n",
        "        objectives = [obj for obj in objectives if obj]\n",
        "        learning_objectives.extend(objectives)\n",
        "\n",
        "    if lo_skills_exists:\n",
        "        # Split by semicolons\n",
        "        skills = row['lo_skills'].split(';')\n",
        "        # Clean each skill\n",
        "        skills = [clean_text(skill) for skill in skills]\n",
        "        # Filter out empty strings\n",
        "        skills = [skill for skill in skills if skill]\n",
        "        learning_objectives.extend(skills)\n",
        "\n",
        "    # Priority 2: If both lo_description and lo_skills are null, extract from course_title and course_description\n",
        "    if not lo_description_exists and not lo_skills_exists:\n",
        "        # Add course_title as a learning objective\n",
        "        if pd.notna(row['course_title']) and row['course_title'].strip() != '':\n",
        "            learning_objectives.append(clean_text(row['course_title']))\n",
        "\n",
        "        # Extract sentences from course_description\n",
        "        if pd.notna(row['course_description']) and row['course_description'].strip() != '':\n",
        "            sentences = sent_tokenize(row['course_description'])\n",
        "            sentences = [clean_text(s) for s in sentences]\n",
        "            sentences = [s for s in sentences if s]\n",
        "            learning_objectives.extend(sentences)\n",
        "\n",
        "    # Filter out single words and stopwords\n",
        "    learning_objectives = filter_meaningless_objectives(learning_objectives)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_objectives = []\n",
        "    seen = set()\n",
        "    for obj in learning_objectives:\n",
        "        if obj.lower() not in seen:\n",
        "            unique_objectives.append(obj)\n",
        "            seen.add(obj.lower())\n",
        "\n",
        "    # Join all objectives with semicolons for the new column\n",
        "    joined_objectives = '; '.join(unique_objectives)\n",
        "\n",
        "    # Final check to remove any remaining hyphens after semicolons\n",
        "    cleaned_joined = re.sub(r';\\s*-\\s*', '; ', joined_objectives)\n",
        "\n",
        "    return cleaned_joined\n",
        "\n",
        "# Apply the function to each row\n",
        "df['lo'] = df.apply(extract_learning_objectives, axis=1)\n",
        "\n",
        "# Final cleanup pass for the entire column to catch any edge cases\n",
        "df['lo'] = df['lo'].str.replace(r';\\s*-\\s*', '; ', regex=True)\n",
        "df['lo'] = df['lo'].str.replace(r'^\\s*-\\s*', '', regex=True)\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "df.to_csv('course_scraped_with_lo.csv', index=False)\n",
        "\n",
        "print(f\"Processing complete. {len(df)} rows processed.\")\n",
        "print(\"Sample of the first 3 rows:\")\n",
        "print(df[['course_title', 'lo']].head(3))\n",
        "\n",
        "# Optional: Check for any remaining leading hyphens in the extracted LOs\n",
        "has_hyphens = df['lo'].str.contains(r';\\s*-').any() or df['lo'].str.startswith('-').any()\n",
        "print(f\"\\nLearning objectives with remaining leading hyphens: {'Yes' if has_hyphens else 'No'}\")\n",
        "\n",
        "# Optional: Check how many rows had learning objectives extracted from different sources\n",
        "lo_from_dedicated = df.apply(lambda row: pd.notna(row['lo_description']) or pd.notna(row['lo_skills']), axis=1).sum()\n",
        "lo_from_fallback = len(df) - lo_from_dedicated\n",
        "\n",
        "# Count rows with no learning objectives after filtering\n",
        "rows_with_no_lo = (df['lo'] == '').sum()\n",
        "\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"Rows with learning objectives from lo_description or lo_skills: {lo_from_dedicated}\")\n",
        "print(f\"Rows with learning objectives from course_title and course_description: {lo_from_fallback}\")\n",
        "print(f\"Rows with no learning objectives after filtering: {rows_with_no_lo}\")\n",
        "\n",
        "# Optional: Check for single-word learning objectives that might have been missed\n",
        "def count_words(text):\n",
        "    if pd.isna(text) or text == '':\n",
        "        return 0\n",
        "    objectives = text.split(';')\n",
        "    single_word_count = 0\n",
        "    for obj in objectives:\n",
        "        obj = obj.strip()\n",
        "        if len(obj.split()) <= 1 and obj != '':\n",
        "            single_word_count += 1\n",
        "    return single_word_count\n",
        "\n",
        "single_word_lo_count = df['lo'].apply(count_words).sum()\n",
        "print(f\"Remaining single-word learning objectives: {single_word_lo_count}\")"
      ],
      "metadata": {
        "id": "lIxELIdeUh5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f19a403-bc43-4272-e5b9-246fd6bd2056"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete. 2186 rows processed.\n",
            "Sample of the first 3 rows:\n",
            "                       course_title  \\\n",
            "0  Systems and Application Security   \n",
            "1   Security Concepts and Practices   \n",
            "2    Incident Response and Recovery   \n",
            "\n",
            "                                                  lo  \n",
            "0  Systems and Application Security; Course 7 - S...  \n",
            "1  Security Concepts and Practices; Course 1 - Se...  \n",
            "2  Incident Response and Recovery; Course 4 - Inc...  \n",
            "\n",
            "Learning objectives with remaining leading hyphens: No\n",
            "\n",
            "Statistics:\n",
            "Rows with learning objectives from lo_description or lo_skills: 1902\n",
            "Rows with learning objectives from course_title and course_description: 284\n",
            "Rows with no learning objectives after filtering: 0\n",
            "Remaining single-word learning objectives: 1806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import module_scraped and extract relevant info"
      ],
      "metadata": {
        "id": "e3Ibe9e0VEc8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('module_scraped.csv')\n",
        "\n",
        "# Function to extract module content\n",
        "def extract_module_content(row):\n",
        "    module_contents = []\n",
        "\n",
        "    # Check if videos column has content\n",
        "    videos_exists = pd.notna(row['videos']) and row['videos'].strip() != ''\n",
        "\n",
        "    # Priority 1: Extract from videos if available\n",
        "    if videos_exists:\n",
        "        # Split by semicolons\n",
        "        video_contents = [content.strip() for content in row['videos'].split(';') if content.strip()]\n",
        "        module_contents.extend(video_contents)\n",
        "\n",
        "    # Priority 2: If videos is null, extract from mod_description and mod_title\n",
        "    else:\n",
        "        # Extract sentences from mod_description if available\n",
        "        if pd.notna(row['mod_description']) and row['mod_description'].strip() != '':\n",
        "            sentences = sent_tokenize(row['mod_description'])\n",
        "            module_contents.extend([s.strip() for s in sentences if s.strip()])\n",
        "\n",
        "        # Always add mod_title even if mod_description is not empty\n",
        "        if pd.notna(row['mod_title']) and row['mod_title'].strip() != '':\n",
        "            module_contents.append(row['mod_title'].strip())\n",
        "\n",
        "    # Join all contents with semicolons for the new column\n",
        "    return '; '.join(module_contents)\n",
        "\n",
        "# Apply the function to each row\n",
        "df['module_content'] = df.apply(extract_module_content, axis=1)\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "df.to_csv('module_scraped_with_content.csv', index=False)\n",
        "\n",
        "print(f\"Processing complete. {len(df)} rows processed.\")\n",
        "print(\"Sample of the first 3 rows:\")\n",
        "print(df[['mod_title', 'module_content']].head(3))\n",
        "\n",
        "# Optional: Check how many rows had module content extracted from different sources\n",
        "content_from_videos = df.apply(lambda row: pd.notna(row['videos']) and row['videos'].strip() != '', axis=1).sum()\n",
        "content_from_description_or_title = len(df) - content_from_videos\n",
        "\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"Modules with content extracted from videos: {content_from_videos}\")\n",
        "print(f\"Modules with content extracted from mod_description or mod_title: {content_from_description_or_title}\")\n"
      ],
      "metadata": {
        "id": "VlVorKE4VLqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c466e6-566c-4262-89fc-77eb20a5b425"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete. 10272 rows processed.\n",
            "Sample of the first 3 rows:\n",
            "  mod_title                                     module_content\n",
            "0  Overview  Malware Attackers; Endpoints; Security Strateg...\n",
            "1  Overview  Professional Ethics; Confidentiality; Integrit...\n",
            "2  Overview  The Incident Response Team (IRT); Incident Ana...\n",
            "\n",
            "Statistics:\n",
            "Modules with content extracted from videos: 8920\n",
            "Modules with content extracted from mod_description or mod_title: 1352\n"
          ]
        }
      ]
    }
  ]
}