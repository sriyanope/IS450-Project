<DOC>
bi foundations with sql etl and data warehousing
the job market for business intelligence bi analysts is expected to grow by percent from to us bureau of labor statistics this ibm gives you soughtafter employers look for when recruiting for a bi analyst
bi analysts gather clean and analyze key business data to find patterns and insights that aid business decisionmaking
during this the basics of sql focusing on how to query relational databases using this popular and powerful language use essential linux commands to create basic shell scripts plus how to build and automate etl elt and data pipelines using bash scripts apache airflow and apache kafka
discover why companies use data lakes and data marts and with adata warehouse plus dive into the rewarding aspect of bi creating interactive reports and dashboards so you can derive insights from data in your warehouse
additionally gain valuable handson practice employing realworld tools used by data professionals each has numerous handson labs and includes a project which gives you plenty to talk about in interviews
when you complete the full have a shareableportfolio of projects and a for your resume and linkedin profile
get started on an indemand business intelligence role enroll today

using handson labs and projects hosted in cloudbased environments gain jobready with the following realworld tools and tasks
writing sql queries and creating joins using postgresql mysql and db databases
running linux commands and pipes creating shell scripts
scheduling jobs using cron building etl and data pipelines creating monitoring airflow dags working with streaming data using kafka
designing data warehouses using star and snowflake schemas
verifying data quality loading staging and production warehouses
developing cubes rollups and materialized viewstables
creating interactive reports and dashboards
analyzing warehouse data using bi tools such as cognos analytics
</DOC>

<DOC>
etl and data pipelines with shell airflow and kafka
delve into the two different approaches to converting raw data into analyticsready data one approach is the extract transform load etl process the other contrasting approach is the extract load and transform elt process etl processes apply to data warehouses and data marts elt processes apply to data lakes where the data is transformed on demand by the requestingcalling application
about the different tools and techniques that are used with etl and data pipelines both etl and elt extract data from source systems move the data through the data pipeline and store the data in destination systems during this how elt and etl processing differ and identify use cases for both identify methods and tools used for extracting the data merging extracted data either logically or physically and for loading data into data repositories also define transformations to apply to source data to make the data credible contextual and accessible to data users be able to outline some of the multiple methods for loading data into the destination system verifying data quality monitoring load failures and the use of recovery mechanisms in case of failure by the end of this also know how to use apache airflow to build data pipelines as be knowledgeable about the advantages of using this approach also how to use apache kafka to build streaming pipelines as as the core components of kafka which include brokers topics partitions replications producers and consumers finally complete a shareable final project that enables you to demonstrate the you acquired in each

data processing techniques
etl or extract transform and load processes are used for cases where flexibility speed and scalability of data are important explore some key differences between similar processes etl and elt which include the place of transformation flexibility big data support and timetoinsight that there is an increasing demand for access to raw data that drives the evolution from etl to elt data extraction involves advanced technologies including database querying web scraping and apis also that data transformation is about formatting data to suit the application and that data is loaded in batches or streamed continuously
intro etl fundamentals elt basics comparing etl and elt data extraction techniques introduction to data transformation techniques data loading techniques

etl data pipelines tools and techniques
extract transform and load etl pipelines are created with bash scripts that can be run on a schedule using cron data pipelines move data from one place or form to another data pipeline processes include scheduling or triggering monitoring maintenance and optimization furthermore batch pipelines extract and operate on batches of data whereas streaming data pipelines ingest data packets onebyone in rapid succession that streaming pipelines apply when the most current data is needed explore that parallelization and io buffers help mitigate bottlenecks also how to describe data pipeline performance in terms of latency and throughput
etl using shell scripting introduction to data pipelines key data pipeline processes batch versus streaming data pipeline use cases data pipeline tools and technologies

building data pipelines using airflow
the key advantage of apache airflows approach to representing data pipelines as dags is that they are expressed as code which makes your data pipelines more maintainable testable and collaborative tasks the nodes in a dag are created by implementing airflows builtin operators about apache airflow having a rich ui that simplifies working with data pipelines explore how to visualize your dag in graph or tree mode also about the key components of a dag definition file and that airflow logs are saved into local file systems and then sent to cloud storage search engines and log analyzers
apache airflow advantages of representing data pipelines as dags in apache airflow apache airflow ui build a dag using airflow airflow logging and monitoring

building streaming pipelines using kafka
apache kafka is a very popular open source event streaming pipeline an event is a type of data that describes the entitys observable state updates over time popular kafka service providers include confluent cloud ibm event stream and amazon msk additionally kafka streams api is a client library supporting you with data processing in event streaming pipelines that the core components of kafka are brokers topics partitions replications producers and consumers explore two special types of processors in the kafka stream api streamprocessing topology the source processor and the sink processor also about building event streaming pipelines using kafka
distributed event streaming platform components apache kafka building event streaming pipelines using kafka kafka streaming process

final
final apply your newly gained knowledge to explore two very exciting handson labs creating etl data pipelines using apache airflow and creating streaming data pipelines using kafka explore building these etl pipelines using realworld scenarios extract transform and load data into a csv file also create a topic named toll in apache kafka download and customize a streaming data consumer as as verifying that streaming data has been collected in the database table
</DOC>

<DOC>
bi dashboards with ibm cognos analytics and google looker
business intelligence bi analyst is one of the top fastest growing roles according to statista in its which jobs have a future update ibm cognos analytics and google looker studio are powerful bi tools used for data visualization analytics and reporting this short helps you to build ibm cognos analytics and google looker studio that can open up in business analytics data science and bi across industries
the introduces you to the features and capabilities of ibm cognos analytics and google looker studio the basics of visualizing data without writing code plus how use both to create interactive dashboards also gain practical through handson labs and complete a final project in which create data visualizations and an interactive dashboard that you can share with prospective employers to highlight your if youre looking to get started as a data analyst bi analyst or data warehouse specialist this provides the ideal introduction to two high profile tools used in these roles enroll selfpaced today and develop valuable bi dashboard you can talk about in interviews

ibm cognos analytics for data analysis and visualization
fasttrack your data analytics learning and gain handson data analytics using ibm cognos analytics after registering with ibm cognos analytics explore the platforms capabilities by creating visualizations building a simple dashboard and trying out its advanced features
introduction introduction to analytics and business intelligence bi tools cognos analytics introduction and how to sign up navigating in cognos analytics creating a simple dashboard in cognos analytics advanced capabilities in cognos analytics dashboards accessing your warehouse with cognos analytics

data visualizations and dashboards with google looker studio
explore google looker studio to create visualizations and build dashboards
getting started with google looker studio connecting to data sources visualizing reports and configuring themes and layouts working with pages and blended data in reports in google looker studio adding controls filters and visualizations in reports in google looker studio

final project final exam and wrapup
complete the final that will be graded by your peers final create some visualizations and add them to a dashboard using ibm cognos analytics or google looker studio
</DOC>

<DOC>
handson introduction to linux commands and shell scripting
this provides a practical understanding of common linux unix shell commands beginner friendly about the linux basics shell commands and bash shell scripting
begin this with an introduction to linux and explore the linux architecture interact with the linux terminal execute commands navigate directories edit files as as install and update software next become familiar with commonly used linux commands with general purpose commands like id date uname ps top echo man
directory management commands such as pwd cd mkdir rmdir find df
file management commands like cat wget more head tail cp mv touch tar zip unzip
access control command chmod
text processing commands wc grep tr
as as networking commands hostname ping ifconfig and curl then move on to learning the basics of shell scripting to automate a variety of tasks create simple to more advanced shell scripts that involve metacharacters quoting variables command substitution io redirection pipes filters and command line arguments also schedule cron jobs using crontab the includes both videobased lectures as as handson labs to practice and apply what you have nocharge access to a virtual linux server that you can access through your web browser so you dont need to download and install anything to complete the labs end this with a final project as as a final exam in the final project demonstrate your knowledge of concepts by performing your own extract transform and load etl process and create a scheduled backup script this is ideal for data engineers data scientists software developers and cloud practitioners who want to get familiar with frequently used commands on linux macos and other unixlike operating systems as as get started with creating shell scripts

introduction to linux
about the basics of linux be able to summarize the origins of the linux operating system and list its key features and use cases what a linux distribution is the names of popular distributions and their key characteristics also be able to explain the linux architecture interact with a linux system using the terminal and navigate directories using paths and shortcuts this will also teach you how to create and edit text files using text editors such as nano and vim lastly how to use a software package manager to install and updates on a linux system
introduction introducing linux and unix linux distributions of linux architecture linux terminal creating and editing text files installing software and updates

introduction to linux commands
how to use common linux commands what a shell and shell commands are and how to use commands to do various tasks in linux this will teach you how to use informational commands to find relevant information about your system navigation commands to navigate files and directories and management commands to create delete copy and move files and directories also to sort and view files in useful ways and extract specific lines and fields from your files be able to use networking commands to examine your network configuration and evaluate identify and retrieve data from urls finally this will cover file archiving and compression commands
of common linux shell commands informational commands file and directory navigation commands file and directory management commands viewing file content useful commands for wrangling text files networking commands file archiving and compression commands

introduction to shell scripting
the basics of shell scripting what a script is and when to use scripts be able to describe the shebang interpreter directive and create and run a simple shell script additionally this will teach you how to use pipes and filters and set shell and environment variables by the end of this also be able to list features of bash shell scripting and use crontab to schedule cron jobs understand the cron syntax and view and remove cron jobs
shell scripting basics filters pipes and variables useful features of the bash shell scheduling jobs using cron

final project and final exam
complete a practice project in which you create an automated extract transform load etl process to extract daily weather forecasts and observed weather data schedule this process to run automatically at a set time daily and how to create a script to measure forecast accuracy in your peergraded final project create a scheduled backup script finally demonstrate the knowledge youve gained by taking a final graded exam
</DOC>

<DOC>
data warehouse fundamentals
whether youre an aspiring data engineer data architect business analyst or data scientist strong data warehousing are a must with the handson and competencies you gain on this your resume will catch the eye of employers and power up your
a data warehouse centralizes and organizes data from disparate sources into a single repository making it easier for data professionals to access clean and analyze integrated data efficiently this teaches you how to design deploy load manage and query data warehouses data marts and data lakes dive into designing modeling and implementing data warehouses and explore data warehousing architectures like star and snowflake schemas master techniques for populating data warehouses through etl and elt processes and hone your in verifying and querying data and utilizing concepts like cubes rollups and materialized viewstables additionally gain valuable practical working on handson labs where apply your knowledge to real data warehousing tasks with repositories like postgresql and ibm db and complete a project that you can refer to in interviews

data warehouses data marts and data lakes
welcome to your first this provides an introduction to data warehouse systems data lakes and data marts when you complete this be able to identify and compare data warehouse systems data marts and data lakes based on their architecture and understand how organizations can benefit from each of these three data storage entities then about three types of data warehouse systems and popular data warehouse system vendors to help your organization assess new data warehouse system offerings when you know the five essential critical criteria including the total cost of ownership to evaluate before changing to a new data warehouse system
introduction data warehouse popular data warehouse systems selecting a data warehouse system data marts ibm db warehouse data lakes data lakehouses explained

designing modeling and implementing data warehouses
knowledgepacked explore general and reference enterprise data warehousing architecture discover how data cubes relate to star schemas then how to slice dice drill up or down roll up and pivot relative to data cubes next examine the capabilities of materialized views their benefits and how to apply them how a data organization using facts and dimensions and their related tables organizes information then explore how to use normalization to create a snowflake schema as an extension of the star schema also about populating a data warehouse incremental data updates verifying data querying data and interpreting an entityrelationship diagram for a star schema finally the will delve into the creation of a materialized view the application of cube and rollup options and examine the advantages organizations gain from implementing staging
of data warehouse architectures cubes rollups and materialized views and tables facts and dimensional modeling data modeling using star and snowflake schemas staging areas for data warehouses verify data quality populating a data warehouse querying the data

final and final quiz
complete your practice project and final project which bring together concepts and practices you previously learned in the first two in the final project design and load data into a data warehouse using facts and dimension tables then write aggregation queries using cube and rollup functions and create a materialized view in the optional lesson explore the workings of ibm db data warehouse system architecture view use cases and understand the key capabilities and integrations available with ibm db warehouse the handson labs lesson will enable you to gain practical knowledge on how to create a db service instance how to populate a data warehouse using ibm db how to query the data warehouse using ibm db
</DOC>

<DOC>
databases and sql for data science with python
working knowledge of sql or structured query language is a must for data professionals like data scientists data analysts and data engineers much of the worlds data resides in databases sql is a powerful language used for communicating with and extracting data from databases
sql inside out from the very basics of select statements to advanced concepts like joins write foundational sql statements like select insert update and delete filter result sets use where count distinct and limit clauses differentiate between dml ddl create alter drop and load tables use string patterns and ranges
order and group result sets and builtin database functions build subqueries and query data from multiple tables access databases as a data scientist using jupyter notebooks with sql and python with advanced concepts like stored procedures views acid transactions inner outer joins through handson labs and projects practice building sql queries with real databases on the cloud and use real data science tools in the final project analyze multiple realworld datasets to demonstrate your

getting started with sql
be introduced to databases how to use basic sql statements like select insert update and delete also get an understanding of how to refine your query results with the where clause as as using count limit and distinct
introduction introduction to databases select statement count distinct limit insert statement update and delete statements

introduction to relational databases and tables
more about relational database concepts and their importance this helps you to understand the process of creating a table in your database on mysql using the graphical interface and sql scripts further also how to alter the entries or delete the entries for any table in the database or even delete the table itself
relational database concepts types of sql statements ddl vs dml create table statement alter drop and truncate tables how to create a database instance on cloud

intermediate sql
this helps you how to use string patterns and ranges to search data and how to sort and group data in result sets also practice composing nested queries and execute select statements to access data from multiple tables
using string patterns and ranges sorting result sets grouping result sets builtin database functions date and time builtin functions subqueries and nested selects working with multiple tables

accessing databases using python
the basic concepts of using python to connect to databases in a jupyter notebook create tables load data query data using sql magic and sqlite python library also how to analyze data using python
how to access databases using python writing code using dbapi accessing databases with sql magic analyzing data with python connecting to a database using ibmdb api creating tables loading data and querying data


be working with multiple realworld datasets for the city of chicago be asked questions that will help you understand the data just as you would in the real world be assessed on the correctness of your sql queries and results
working with real world datasets getting table and column details

bonus advanced sql for data engineer honors
this covers some advanced sql techniques that will be useful for data engineers how to build more powerful queries with advanced sql techniques like views transactions stored procedures and joins if you are following the data engineering track you must complete this completion of this is not required for those completing the data science or data analyst tracks
views stored procedures acid transactions join inner join outer joins
</DOC>

