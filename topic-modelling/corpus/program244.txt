<DOC>
machine
this from leading researchers at the university of washington introduces you to the exciting highdemand field of machine through a series of practical case studies gain applied in major areas of machine including prediction classification clustering and information retrieval to analyze large and complex datasets create systems that adapt and improve over time and build intelligent applications that can make predictions from dataapplied projectlearners will implement and apply predictive classification clustering and information retrieval machine algorithms to real datasets throughout each in the they will walk away with applied machine and python programming
</DOC>

<DOC>
machine foundations a case study approach
do you have data and wonder what it can tell you do you need a deeper understanding of the core ways in which machine can improve your business do you want to be able to converse with specialists about anything from regression and classification to deep and recommender systemsin this get handson with machine from a series of practical casestudies at the end of the first have studied how to predict house prices based on houselevel features analyze sentiment from user reviews retrieve documents of interest recommend products and search for images through handson practice with these use cases be able to apply machine methods in a wide range of domains this first treats the machine method as a black box using this abstraction focus on understanding tasks of interest matching these tasks to machine tools and assessing the quality of the output in subsequent courses delve into the components of this black box by examining models and algorithms together these pieces form the machine pipeline which use in developing intelligent applications by the end of this be able to identify potential applications of machine in practice describe the core differences in analyses enabled by regression classification and clustering select the appropriate machine task for a potential application apply regression classification clustering retrieval recommender systems and deep represent your data as features to serve as input to machine models assess the model quality in terms of relevant error metrics for each task utilize a dataset to fit a model to analyze new data build an endtoend application that uses machine at its core implement these techniques in python

welcome
machine is everywhere but is often operating behind the scenes pthis introduction to the provides you with insights into the power of machine and the multitude of intelligent applications you personally will be able to develop and deploy upon completionpwe also discuss who we are how we got here and our view of the future of intelligent applications
welcome to this and who we are machine is changing the world why a case study approach overview how we got into ml who is this for what be able to do the capstone and an example intelligent application the future of intelligent applications starting a jupyter notebook creating variables in python conditional statements and loops in python creating functions and lambdas in python starting turi create loading an sframe canvas for data visualization interacting with columns of an sframe using apply for data transformation

regression predicting house prices
build your first intelligent application that makes predictions from datapwe will explore this idea within the context of our first case study predicting house prices where create models that predict a continuous value price from input features square footage number of bedrooms and bathrooms pthis is just one of the many places where regression can be appliedother applications range from predicting health in medicine stock prices in finance and power usage in highperformance computing to analyzing which regulators are important for gene expressionpyou will also examine how to analyze the performance of your predictive model and implement regression in practice using a jupyter notebook
predicting house prices a case study in regression what is the goal and how might you naively address it linear regression a modelbased approach adding higher order effects evaluating overfitting via trainingtest split trainingtest curves adding other features other regression examples regression ml block diagram loading exploring house sale data splitting the data into training and test sets a simple regression model to predict house prices from house size evaluating error rmse of the simple model visualizing predictions of simple model with matplotlib inspecting the model coefficients learned exploring other features of the data a model to predict house prices from more features applying learned models to predict price of an average house applying learned models to predict price of two fancy houses

classification analyzing sentiment
how do you guess whether a person felt positively or negatively about an just from a short review they wrotepin our second case study analyzing sentiment create models that predict a class positivenegative sentiment from input features text of the reviews user profile informationthis task is an example of classification one of the most widely used areas of machine with a broad array of applications including ad targeting spam detection medical diagnosis and image classificationpyou will analyze the accuracy of your classifier implement an actual classifier in a jupyter notebook and take a first stab at a core piece of the intelligent application build and deploy in your capstone
analyzing the sentiment of reviews a case study in classification what is an intelligent restaurant review system examples of classification tasks linear classifiers decision boundaries training and evaluating a classifier whats a good accuracy false positives false negatives and confusion matrices curves class probabilities classification ml block diagram loading exploring product review data creating the word count vector exploring the most popular product defining which reviews have positive or negative sentiment training a sentiment classifier evaluating a classifier the roc curve applying model to find most positive negative reviews for a product exploring the most positive negative aspects of a product

clustering and similarity retrieving documents
a reader is interested in a specific news article and you want to find a similar articles to recommend what is the right notion of similarity how do i automatically search over documents to find the one that is most similar how do i quantitatively represent the documents in the first placepin this third case study retrieving documents examine various document representations and an algorithm to retrieve the most similar subset also consider structured representations of the documents that automatically group articles by similarity eg document topicpyou will actually build an intelligent document retrieval system for wikipedia entries in an jupyter notebook
document retrieval a case study in clustering and measuring similarity what is the document retrieval task word count representation for measuring similarity prioritizing important words with tfidf calculating tfidf vectors retrieving similar documents using nearest neighbor search clustering documents task overview clustering documents an unsupervised task kmeans a clustering algorithm other examples of clustering clustering and similarity ml block diagram loading exploring wikipedia data exploring word counts computing exploring tfidfs computing distances between wikipedia articles building exploring a nearest neighbors model for wikipedia articles examples of document retrieval in action

recommending products
ever wonder how amazon forms its personalized product recommendations how netflix suggests movies to watch how pandora selects the next song to stream how facebook or linkedin finds people you might connect with underlying all of these technologies for personalized content is something called collaborative filtering pyou will how to build such a recommender system using a variety of techniques and explore their tradeoffsp one method we examine is matrix factorization which learns features of users and products to form recommendations in a jupyter notebook use these techniques to build a real song recommender system
recommender systems overview where we see recommender systems in action building a recommender system via classification collaborative filtering people who bought this also bought effect of popular items normalizing cooccurrence matrices and leveraging purchase histories the matrix completion task recommendations from known useritem features predictions in matrix form discovering hidden structure by matrix factorization bringing it all together featurized matrix factorization a performance metric for recommender systems optimal recommenders precisionrecall curves recommender systems ml block diagram loading and exploring song data creating evaluating a popularitybased song recommender creating evaluating a personalized song recommender using precisionrecall to compare recommender models

deep searching for images
youve probably heard that deep is making news across the world as one of the most promising techniques in machine every industry is dedicating resources to unlock the deep potential including for tasks such as image tagging object recognition speech recognition and text analysispin our final case study searching for images how layers of neural networks provide very descriptive nonlinear features that provide impressive performance in image classification and retrieval tasks then construct deep features a transfer technique that allows you to use deep very easily even when you have little data to train the modelpusing iphython notebooks build an image classifier and an intelligent image retrieval system with deep
searching for images a case study in deep what is a visual product recommender very nonlinear features with neural networks application of deep to computer vision deep performance demo of deep model on imagenet data other examples of deep in computer vision challenges of deep deep features deep ml block diagram loading image data training evaluating a classifier using raw image pixels training evaluating a classifier using deep features loading image data creating a nearest neighbors model for image retrieval querying the nearest neighbors model to retrieve images querying for the most similar images for car image displaying other example image retrievals with a python lambda

closing remarks
in the conclusion of the we will describe the final stage in turning our machine tools into a service deploymentpwe will also discuss some open challenges that the field of machine still faces and where we think machine is heading we conclude with an overview of whats in store for you in the rest of the and the amazing intelligent applications that are ahead for us as we evolve machine
youve made it deploying an ml service what happens after deployment open challenges in ml where is ml going whats ahead in the thank you
</DOC>

<DOC>
machine regression
case study predicting housing pricesin our first case study predicting house prices create models that predict a continuous value price from input features square footage number of bedrooms and bathrooms this is just one of the many places where regression can be applied other applications range from predicting health in medicine stock prices in finance and power usage in highperformance computing to analyzing which regulators are important for gene expression explore regularized linear regression models for the task of prediction and feature selection be able to handle very large sets of features and select between models of various complexity also analyze the impact of aspects of your data such as outliers on your selected models and predictions to fit these models implement optimization algorithms that scale to large datasets by the end of this be able to describe the input and output of a regression model compare and contrast bias and variance when modeling data estimate model parameters using optimization algorithms tune parameters with cross validation analyze the performance of the model describe the notion of sparsity and how lasso leads to sparse solutions deploy methods to select between models exploit the model to form predictions build a regression model to predict prices using a housing dataset implement these techniques in python

welcome
regression is one of the most important and broadly used machine and statistics tools out there it allows you to make predictions from data by the relationship between features of your data and some observed continuousvalued response regression is used in a massive number of applications ranging from predicting stock prices to understanding gene regulatory networkspthis introduction to the provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have
welcome what is the about outlining the first half of the outlining the second half of the assumed background

simple linear regression
our starts from the most basic regression model just fitting a line to data this simple model for forming predictions from a single univariate feature of the data is appropriately called simple linear regressionp we describe the highlevel regression task and then specialize these concepts to the simple linear regression case how to formulate a simple regression model and fit the model to data using both a closedform solution as well as an iterative optimization algorithm called gradient descent based on this fitted function interpret the estimated model parameters and form predictions also analyze the sensitivity of your fit to outlying observationsp examine all of these concepts in the context of a case study of predicting house prices from the square feet of the house
a case study in predicting house prices regression fundamentals data model regression fundamentals the task regression ml block diagram the simple linear regression model the cost of using a given line using the fitted line interpreting the fitted line defining our least squares optimization objective finding maxima or minima analytically maximizing a d function a worked example finding the max via hill climbing finding the min via hill descent choosing stepsize and convergence criteria gradients derivatives in multiple dimensions gradient descent multidimensional hill descent computing the gradient of rss approach closedform solution approach gradient descent comparing the approaches influence of high leverage points exploring the data influence of high leverage points removing center city influence of high leverage points removing highend towns asymmetric cost functions a brief recap

multiple regression
the next step in moving beyond simple linear regression is to consider multiple regression where multiple features of the data are used to form predictions p more specifically how to build models of more complex relationship between a single variable eg square feet and the observed response like house sales price this includes things like fitting a polynomial to your data or capturing seasonal changes in the response value also how to incorporate multiple input variables eg square feet bedrooms bathrooms then be able to describe how all of these models can still be cast within the linear regression framework but now using multiple features within this multiple regression framework fit models to data interpret estimated coefficients and form predictions phere also implement a gradient descent algorithm for fitting a multiple regression model
multiple regression intro polynomial regression modeling seasonality where we see seasonality regression with general features of input motivating the use of multiple inputs defining notation regression with features of multiple inputs interpreting the multiple regression fit rewriting the single observation model in vector notation rewriting the model for all observations in matrix notation computing the cost of a ddimensional curve computing the gradient of rss approach closedform solution discussing the closedform solution approach gradient descent featurebyfeature update algorithmic of gradient descent approach a brief recap

assessing performance
having learned about linear regression models and algorithms for estimating the parameters of such models you are now ready to assess how well your considered method should perform in predicting new data you are also ready to select amongst possible models to choose the best performing p this is all about these important topics of model selection and assessment examine both theoretical and practical aspects of such analyses first explore the concept of measuring the loss of your predictions and use this to define training test and generalization error for these measures of error analyze how they vary with model complexity and how they might be utilized to form a valid assessment of predictive performance this leads directly to an important conversation about the biasvariance tradeoff which is fundamental to machine finally devise a method to first select amongst models and then assess the performance of the selected model pthe concepts described are key to all machine problems wellbeyond the regression setting addressed
assessing performance intro what do we mean by loss training error assessing loss on the training set generalization error what we really want test error what we can actually compute defining overfitting trainingtest split irreducible error and bias variance and the biasvariance tradeoff error vs amount of data formally defining the sources of error formally deriving why sources of error trainingvalidationtest split for model selection fitting and assessment a brief recap

ridge regression
you have examined how the performance of a model varies with increasing model complexity and can describe the potential pitfall of complex models becoming overfit to the training data explore a very simple but extremely effective technique for automatically coping with this issue this method is called ridge regression you start out with a complex model but now fit the model in a manner that not only incorporates a measure of fit to the training data but also a term that biases the solution away from overfitted functions to this end explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective derive both a closedform and gradient descent algorithm for fitting the ridge regression objective these forms are small modifications from the original algorithms you derived for multiple regression to select the strength of the bias away from overfitting explore a generalpurpose method called cross validation pyou will implement both crossvalidation and gradient descent to fit a ridge regression model and select the regularization constant
symptoms of overfitting in polynomial regression overfitting demo overfitting for more general multiple regression models balancing fit and magnitude of coefficients the resulting ridge objective and its extreme solutions how ridge regression balances bias and variance ridge regression demo the ridge coefficient path computing the gradient of the ridge objective approach closedform solution discussing the closedform solution approach gradient descent selecting tuning parameters via cross validation kfold cross validation how to handle the intercept a brief recap

feature selection lasso
a fundamental machine task is to select amongst a set of features to include in a model explore this idea in the context of multiple regression and describe how such feature selection is important for both interpretability and efficiency of forming predictions p to start examine methods that search over an enumeration of models including different subsets of features analyze both exhaustive search and greedy algorithms then instead of an explicit enumeration we turn to lasso regression which implicitly performs feature selection in a manner akin to ridge regression a complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridge this lasso method has had impact in numerous applied domains and the ideas behind the method have fundamentally changed machine and statistics also implement a coordinate descent algorithm for fitting a lasso model pcoordinate descent is another general optimization technique which is useful in many areas of machine
the feature selection task all subsets complexity of all subsets greedy algorithms complexity of the greedy forward stepwise algorithm can we use regularization for feature selection thresholding ridge coefficients the lasso objective and its coefficient path visualizing the ridge cost visualizing the ridge solution visualizing the lasso cost and solution lasso demo what makes the lasso objective different coordinate descent normalizing features coordinate descent for least squares regression normalized features coordinate descent for lasso normalized features assessing convergence and other lasso solvers coordinate descent for lasso unnormalized features deriving the lasso coordinate descent update choosing the penalty strength and other practical issues with lasso a brief recap

nearest neighbors kernel regression
up to this point we have focused on methods that fit parametric functionslike polynomials and hyperplanesto the entire dataset we instead turn our attention to a class of nonparametric methods these methods allow the complexity of the model to increase as more data are observed and result in fits that adapt locally to the observations p we start by considering the simple and intuitive example of nonparametric methods nearest neighbor regression the prediction for a query point is based on the outputs of the most related observations in the training set this approach is extremely simple but can provide excellent predictions especially for large datasets deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors building on this idea we turn to kernel regression instead of forming predictions based on a small set of neighboring observations kernel regression uses all observations in the dataset but the impact of these observations on the predicted value is weighted by their similarity to the query point analyze the theoretical performance of these methods in the limit of infinite training data and explore the scenarios in which these methods well versus struggle also implement these techniques and observe their practical behavior
limitations of parametric regression nearest neighbor regression approach distance metrics nearest neighbor algorithm knearest neighbors regression knearest neighbors in practice weighted knearest neighbors from weighted knn to kernel regression global fits of parametric models vs local fits of kernel regression performance of nn as amount of data grows issues with highdimensions data scarcity and computational complexity knn for classification a brief recap

closing remarks
in the conclusion of the we will recap what we have covered this represents both techniques specific to regression as well as foundational machine concepts that will appear throughout the we also briefly discuss some important regression techniques we did not cover coursep we conclude with an overview of whats in store for you in the rest of the
simple and multiple regression assessing performance and ridge regression feature selection lasso and nearest neighbor regression what we covered and what we didnt cover thank you
</DOC>

<DOC>
machine clustering retrieval
case studies finding similar documentsa reader is interested in a specific news article and you want to find similar articles to recommend what is the right notion of similarity moreover what if there are millions of other documents each time you want to a retrieve a new document do you need to search through all other documents how do you group similar documents together how do you discover new emerging topics that the documents cover third case study finding similar documents examine similaritybased algorithms for retrieval also examine structured representations for describing the documents in the corpus including clustering and mixed membership models such as latent dirichlet allocation lda implement expectation maximization em to the document clusterings and see how to scale the methods using mapreduce by the end of this be able to create a document retrieval system using knearest neighbors identify various similarity metrics for text data reduce computations in knearest neighbor search by using kdtrees produce approximate nearest neighbors using locality sensitive hashing compare and contrast supervised and unsupervised tasks cluster documents by topic using kmeans describe how to parallelize kmeans using mapreduce examine probabilistic clustering approaches using mixtures models fit a mixture of gaussian model using expectation maximization em perform mixed membership modeling using latent dirichlet allocation lda describe the steps of a gibbs sampler and how to use its output to draw inferences compare and contrast initialization techniques for nonconvex optimization objectives implement these techniques in python

welcome
clustering and retrieval are some of the most highimpact machine tools out there retrieval is used in almost every applications and device we interact with like in providing a set of products related to one a shopper is currently considering or a list of people you might want to connect with on a social media platform clustering can be used to aid retrieval but is a more broadly useful tool for automatically discovering structure in data like uncovering groups of similar patientspthis introduction to the provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have
welcome and introduction to clustering and retrieval tasks overview modulebymodule topics covered assumed background

nearest neighbor search
we start the by considering a retrieval task of fetching a document similar to one someone is currently reading we cast this problem as one of nearest neighbor search which is a concept we have seen in the foundations and regression courses however here take a deep dive into two critical components of the algorithms the data representation and metric for measuring similarity between pairs of datapoints examine the computational burden of the naive nearest neighbor search algorithm and instead implement scalable alternatives using kdtrees for handling large datasets and locality sensitive hashing lsh for providing approximate nearest neighbors even in highdimensional spaces explore all of these ideas on a wikipedia dataset comparing and contrasting the impact of the various choices you can make on the nearest neighbor results produced
retrieval as knearest neighbor search nn algorithm knn algorithm document representation distance metrics euclidean and scaled euclidean writing scaled euclidean distance using weighted inner products distance metrics cosine similarity to normalize or not and other distance considerations complexity of brute force search kdtree representation nn search with kdtrees complexity of nn search with kdtrees visualizing scaling behavior of kdtrees approximate knn search using kdtrees limitations of kdtrees lsh as an alternative to kdtrees using random lines to partition points defining more bins searching neighboring bins lsh in higher dimensions optional improving efficiency through multiple tables a brief recap

clustering with kmeans
in clustering our goal is to group the datapoints in our dataset into disjoint sets motivated by our document analysis case study use clustering to discover thematic groups of articles by topic these topics are not provided unsupervised task rather the idea is to output such cluster labels that can be postfacto associated with known topics like science world news etc even without such postfacto labels examine how the clustering output can provide insights into the relationships between datapoints in the dataset the first clustering algorithm implement is kmeans which is the most widely used clustering algorithm out there to scale up kmeans about the general mapreduce framework for parallelizing and distributing computations and then how the iterates of kmeans can utilize this framework show that kmeans can provide an interpretable grouping of wikipedia articles when appropriately tuned
the goal of clustering an unsupervised task hope for unsupervised and some challenge cases the kmeans algorithm kmeans as coordinate descent smart initialization via kmeans assessing the quality and choosing the number of clusters motivating mapreduce the general mapreduce abstraction mapreduce execution overview and combiners mapreduce for kmeans other applications of clustering a brief recap

mixture models
in kmeans observations are each hardassigned to a single cluster and these are based just on the cluster centers rather than also incorporating shape information in our second on clustering perform probabilistic modelbased clustering that provides a more descriptive notion of a cluster and accounts for uncertainty in of datapoints to clusters via soft explore and implement a broadly useful algorithm called expectation maximization em for inferring these soft as well as the model parameters to gain intuition first consider a visually appealing image clustering task then cluster wikipedia articles handling the highdimensionality of the tfidf document representation considered
motiving probabilistic clustering models aggregating over unknown classes in an image dataset univariate gaussian distributions bivariate and multivariate gaussians mixture of gaussians interpreting the mixture of gaussian terms scaling mixtures of gaussians for document clustering computing soft from known cluster parameters optional responsibilities as bayes rule estimating cluster parameters from known cluster estimating cluster parameters from soft em iterates in equations and pictures convergence initialization and overfitting of em relationship to kmeans a brief recap

mixed membership modeling via latent dirichlet allocation
the clustering model inherently assumes that data divide into disjoint sets eg documents by topic but often our data objects are better described via memberships in a collection of sets eg multiple topics in our fourth explore latent dirichlet allocation lda as an example of such a mixed membership model particularly useful in document analysis interpret the output of lda and various ways the output can be utilized like as a set of learned document features the mixed membership modeling ideas you about through lda for document analysis carry over to many other interesting models and applications like social network models where people have multiple affiliationspthroughout this we introduce aspects of bayesian modeling and a bayesian inference algorithm called gibbs sampling be able to implement a gibbs sampler for lda by the end of the
mixed membership models for documents an alternative document clustering model components of latent dirichlet allocation model goal of lda inference the need for bayesian inference gibbs sampling from feet a standard gibbs sampler for lda what is collapsed gibbs sampling a worked example for lda initial setup a worked example for lda deriving the resampling distribution using the output of collapsed gibbs sampling a brief recap

hierarchical clustering closing remarks
in the conclusion of the we will recap what we have covered this represents both techniques specific to clustering and retrieval as well as foundational machine concepts that are more broadly usefulpwe provide a quick tour into an alternative clustering approach called hierarchical clustering which experiment with on the wikipedia dataset following this exploration we discuss how clusteringtype ideas can be applied in other areas like segmenting time series we then briefly outline some important clustering and retrieval ideas that we did not cover coursep we conclude with an overview of whats in store for you in the rest of the
recap recap recap recap why hierarchical clustering divisive clustering agglomerative clustering the dendrogram agglomerative clustering details hidden markov models what we didnt cover thank you
</DOC>

<DOC>
machine classification
case studies analyzing sentiment loan default predictionin our case study on analyzing sentiment create models that predict a class positivenegative sentiment from input features text of the reviews user profile information in our second case study for this loan default prediction tackle financial data and predict when a loan is likely to be risky or safe for the bank these tasks are an examples of classification one of the most widely used areas of machine with a broad array of applications including ad targeting spam detection medical diagnosis and image classification create classifiers that provide stateoftheart performance on a variety of tasks become familiar with the most successful techniques which are most widely used in practice including logistic regression decision trees and boosting in addition be able to design and implement the underlying algorithms that can these models at scale using stochastic gradient ascent implement these technique on realworld largescale machine tasks also address significant tasks face in realworld applications of ml including handling missing data and measuring precision and recall to evaluate a classifier this is handson actionpacked and full of visualizations and illustrations of how these techniques will behave on real data weve also included optional content in every covering advanced topics for those who want to go even deeper objectives by the end of this be able to describe the input and output of a classification model tackle both binary and multiclass classification problems implement a logistic regression model for largescale classification create a nonlinear model using decision trees improve the performance of any model using boosting scale your methods with stochastic gradient ascent describe the underlying decision boundaries build a classification model to predict sentiment in a product review dataset analyze financial data to predict loan defaults use techniques for handling missing data evaluate your models using precisionrecall metrics implement these techniques in python or in the language of your choice though python is highly recommended

welcome
classification is one of the most widely used techniques in machine with a broad array of applications including sentiment analysis ad targeting spam detection risk assessment medical diagnosis and image classification the core goal of classification is to predict a category or class y from some inputs x through this become familiar with the fundamental models and algorithms used in classification as well as a number of core machine concepts rather than covering all aspects of classification focus on a few core techniques which are widely used in the realworld to get stateoftheart performance by following our handson approach implement your own algorithms on multiple realworld tasks and deeply grasp the core techniques needed to be successful with these approaches in practice this introduction to the provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have
welcome to the classification a part of the machine what is this about impact of classification overview outline of first half of outline of second half of assumed background lets get started
</DOC>

