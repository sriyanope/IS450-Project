<DOC>
nosql big data and spark foundations
big data engineers and professionals with nosql are highly sought after in the data management industry this is designed for those seeking to develop fundamental for working with big data apache spark and nosql databases three informationpacked courses cover popular nosql databases like mongodb and apache cassandra the widely used apache hadoop ecosystem of big data tools as well as apache spark analytics engine for largescale data processingyou start with an overview of various categories of nosql not only sql data repositories and then handson with several of them including ibm cloudant monogodb and cassandra perform various data management tasks such as creating replicating databases inserting updating deleting querying indexing aggregating sharding data next gain fundamental knowledge of big data technologies such as hadoop mapreduce hdfs hive and hbase followed by a more in depth working knowledge of apache spark spark dataframes spark sql pyspark the spark application ui and scaling spark with kubernetes in the final to with spark structured streaming spark ml for performing extract transform and load processing etl and machine tasks this is suitable for beginners in the fields of nosql and big data whether you are or preparing to be a data engineer software developer it architect data scientist or it manager applied projectthe emphasis is on by doing as such each includes handson labs to practice apply the nosql and big data you during lectures in the first handson with several nosql databases mongodb apache cassandra and ibm cloudant to perform a variety of tasks creating the database adding documents querying data utilizing the http api performing create read update delete crud operations limiting sorting records indexing aggregation replication using cql shell keyspace operations other table operationsin the next launch a hadoop cluster using docker and run map reduce jobs explore working with spark using jupyter notebooks on a python kernel build your spark using dataframes spark sql and scale your jobs using kubernetes in the final use spark for etl processing and machine model training and deployment using ibm watson
</DOC>

<DOC>
introduction to nosql databases
get started with nosql databases with this beginnerfriendly introductory this will provide technical handson knowledge of nosql databases and databaseasaservice daas offerings with the advent of big data and agile development methodologies nosql databases have gained a lot of relevance in the database landscape their main advantage is the ability to handle scalability and flexibility issues modern applications raise start this by the history and the basics of nosql databases document keyvalue column and graph and discover their key characteristics and benefits about the four categories of nosql databases and how they differ also explore the differences between the acid and base consistency models the pros and cons of distributed systems and when to use rdbms and nosql also about vector databases an emerging class of databases popular in ai next explore the architecture and features of several implementations of nosql databases namely mongodb cassandra and ibm cloudant about the common tasks that they each perform and their key and defining characteristics then get handson using those nosql databases to perform standard database management tasks such as creating and replicating databases loading and querying data modifying database permissions indexing and aggregating data and sharding or partitioning data at the end of this complete a final where apply all your knowledge of the content to a specific scenario and with several nosql databases this suits anyone wanting to expand their data management and information technology set

introducing nosql
nosql means not only sql the term refers to a class of databases that are nonrelational in architecture nosql databases have their roots in the opensource community and have become more popular due to the demands of big data about the characteristics of nosql and the four main categories of nosql databases document keyvalue column and graph explore the differences between the acid and base consistency models the advantages and challenges of distributed systems and the cap theorem and its characteristics also how to decide when to use rdbms and when to use nosql
introduction overview of nosql characteristics of nosql databases keyvalue nosql databases documentbased nosql databases columnbased nosql databases graph nosql databases acid versus base operations distributed databases cap theorem challenges in migrating from rdbms to nosql databases

introducing mongodb an opensource nosql database
mongodb is a documentoriented nosql database mongodb databases are easy to access by indexing these databases support various data types including dates and numbers mongodb is the most popular nosql database today empowering users to query manipulate and find interesting insights from their collected data it can also be used for various purposes because of the flexibility of storing structured or unstructured data about the characteristics of mongodb and expand your handson working knowledge of mongodb performing various common tasks including create read update and delete crud operations limit and sort records indexing and aggregation explore replication and sharding which are capabilities that support the scalability and availability seen with mongodb
overview of mongodb advantages of mongodb use cases for mongodb crud operations indexes aggregation framework replication and sharding accessing mongodb from python

introducing apache cassandra an opensource nosql database
apache cassandra is an open source database that is best used by always available applications these applications require a database that is always available highly available and that scales fast in hightraffic situations without compromising performance apache cassandra is best for online services like netflix uber and spotify about the characteristics of apache cassandra also expand your handson knowledge of how to perform common cassandra tasks including using the cql shell keyspace operations table operations and crud operations
apache cassandra overview key features of apache cassandra apache cassandra data model part apache cassandra data model part introduction to cassandra query language shell cqlsh cql data types apache cassandra keyspace operations table operations crud operations part crud operations part

final working with nosql databases
this contains the final for this consolidate the you learned throughout this by applying them to a specific scenario to complete the successfully you must demonstrate that you have the to perform the tasks outlined your peers will grade this final

optional introducing ibm cloudant a nosql dbaas
databaseasaservice dbaas is a popular solution for hybrid multicloud applications ibm cloudant is a fully managed dbaas built on open source apache couchdb cloudant aims to be the data layer for all your web and mobile applications find out how simple developing modern web applications is with cloudants rich features and json document storeyou will explore the architecture of cloudant as a nosql database gain handson with cloudant capabilities and key technologies also how to use the cloudant dashboard to create and manage your database
overview of cloudant cloudant architecture and key technologies cloudant benefits and solutions deployment options for cloudant dashboards in cloudant working with databases in cloudant http api basics working with the http api
</DOC>

<DOC>
introduction to big data with spark and hadoop
this selfpaced ibm will teach you all about big data become familiar with the characteristics of big data and its application in big data analytics also gain handson with big data processing tools like apache hadoop and apache spark bernard marr defines big data as the digital trace that we are generating digital era start the by understanding what big data is and exploring how insights from big data can be harnessed for a variety of use cases also explore how big data uses technologies like parallel processing scaling and data parallelism next about hadoop an opensource framework that allows for the distributed processing of large data and its ecosystem discover important applications that go hand in hand with hadoop like distributed file system hdfs mapreduce and hbase become familiar with hive a data warehouse software that provides an sqllike interface to efficiently query and manipulate large data sets then gain insights into apache spark an opensource processing engine that provides users with new ways to store and use big data discover how to leverage spark to deliver reliable insights the provides an overview of the platform going into the components that make up apache spark about dataframes and perform basic dataframe operations and with sparksql explore how spark processes and monitors the requests your application submits and how you can track using the spark application ui this has several handson labs to help you apply and practice the concepts you complete hadoop and spark labs using various tools and technologies including docker kubernetes python and jupyter notebooks

what is big data
begin your acquisition of big data knowledge with the most uptodate definition of big data explore the impact of big data on everyday personal tasks and business transactions with big data use cases also how big data uses parallel processing scaling and data parallelism going further explore commonly used big data tools and explain the role of opensource in big data finally go beyond the hype and explore additional big data viewpoints
introduction what is big data impact of big data parallel processing scaling and data parallelism big data tools and ecosystem open source and big data beyond the hype big data use cases

introduction to the hadoop ecosystem
gain a fundamental understanding of the apache hadoop architecture ecosystem practices and commonly used applications including distributed file system hdfs mapreduce hive and hbase also gain practical in handson labs when you query the data added using hive launch a singlenode hadoop cluster using docker and run mapreduce jobs
introduction to hadoop intro to mapreduce hadoop ecosystem hdfs hive hbase

apache spark
turn your attention to the popular apache spark platform where explore the attributes and benefits of apache spark and distributed computing gain key insights about functional programming and lambda functions also explore resilient distributed datasets rdds parallel programming resilience in apache spark and relate rdds and parallel programming with apache spark then dive into additional apache spark components and how apache spark scales with big data working with big data signals the need for working with queries including structured queries using sql also about the functions parts and benefits of spark sql and dataframe queries and discover how dataframes with spark sql
why use apache spark functional programming basics parallel programming using resilient distributed datasets scale out data parallelism in apache spark dataframes and sparksql

dataframes and spark sql
about resilient distributed datasets rdds their uses in apache spark and rdd transformations and actions compare the use of datasets with sparks latest data abstraction dataframes to identify and apply basic dataframe operations explore apache spark sql optimization and how spark sql and memory optimization benefit from using catalyst and tungsten finally fortify your with guided handson lab to create a table view and apply data aggregation techniques
rdds in parallel programming and spark dataframes and datasets catalyst and tungsten etl with dataframes realworld usage of sparksql

development and runtime environment options
explore how spark processes the requests that your application submits and how you can track using the spark application ui because spark application happens on the cluster you need to be able to identify apache cluster managers their components and benefits also know how to connect with each cluster manager and how and when you might want to set up a local standalone spark instance next about apache spark application submission including the use of sparks unified interface sparksubmit and about options and dependencies also describe and apply options for submitting applications identify external application dependency management techniques and list spark shell benefits also look at recommended practices for sparks static and dynamic configuration options and perform handson labs to use apache spark on ibm cloud and run spark on kubernetes
apache spark architecture overview of apache spark cluster modes how to run an apache spark application using apache spark on ibm cloud setting apache spark configuration running spark on kubernetes

monitoring and tuning
platforms and applications require monitoring and tuning to manage issues that inevitably happen about connecting the apache spark user interface web server and using the same ui web server to manage application processes also identify common apache spark application issues and about debugging issues using the application ui and locating related log files further discover and gain realworld knowledge about how spark manages memory and processor resources using the handson lab
the apache spark user interface monitoring application progress debugging apache spark application issues understanding memory resources understanding processor resources

final and assessment
perform a practice lab where explore two critical aspects of data processing using spark working with resilient distributed datasets rdds and constructing dataframes from json data also apply various transformations and actions on both rdds and dataframes to gain insights and manipulate the data effectively further apply your knowledge in a final where create a dataframe by loading data from a csv file and applying transformations and actions using spark sql finally be assessed based on your from the
</DOC>

<DOC>
machine with apache spark
explore the exciting world of machine with this ibm start by ml fundamentals before unlocking the power of apache spark to build and deploy ml models for data engineering applications dive into supervised and unsupervised techniques and discover the revolutionary possibilities of generative ai through instructional and gain handson with spark structured streaming develop an understanding of data engineering and ml pipelines and become proficient in evaluating ml models using sparkml in practical labs utilize sparkml for regression classification and clustering enabling you to construct prediction and classification models connect to spark clusters analyze sparksql datasets perform etl activities and create ml models using spark ml and scikit finally demonstrate your acquired through a final this intermediate is suitable for aspiring and experienced data engineers as well as working professionals in data analysis and machine prior knowledge in big data hadoop spark python and etl is highly recommended for this

get started with machine
gain knowledge of machine techniques that enable computers to perform tasks without explicit programming explore the lifecycle of machine models and understand the crucial role of data engineering in machine the covers supervised and unsupervised techniques including classification regression and clustering furthermore acquire valuable insights into generative ai and its potential to revolutionize multiple industries enhance peoples lives and generate newer and previously unimaginable data and
introduction introduction to machine for everyone role of data engineering in machine machine model lifecycle supervised vs unsupervised regression classification evaluating machine models clustering generative ai overview and use cases generative ai applications

machine with apache spark
this will introduce you to spark and provide an overview of its key features and applications in the field of data engineering discover the process of connecting to a spark cluster using sn labs and delve into various topics such as regression mileage prediction classification diabetic classification clustering and clustering load data using sparkml additionally gain insights into how to construct these models using spark ml moreover this will cover graphframes on apache spark and guide you in handson labs
spark for data engineers regression using sparkml classification using sparkml clustering using sparkml graphframes on apache spark

data engineering for machine using apache spark
this begins with apache spark structured streaming and its role in processing streaming data with spark sql acquire knowledge about key terms associated with structured streaming the then covers the extracttransformload process and provides handson in transferring data from one source to another destination with varying data formats or structures additionally gain a practical understanding of feature extraction and transformation using spark extract and transform features the also delves into machine pipelines using spark demonstrating the process and benefits involved lastly grasp the concept of model persistence and its significant role in machine
spark sql etl workloads spark structured streaming feature extraction and transformation machine pipelines using spark model persistence

final
apply the data engineering and techniques you have acquired throughout the the concludes with a final and that allow you to demonstrate your proficiency in these areas step into the role of a data engineer working at a renowned aeronautics consulting company recognized for its adeptness in handling large datasets your role as a data engineer is crucial as the data scientists rely on your expertise to carry out etl extract transform load tasks and establish machine pipelines while data scientists possess expertise in machine they depend on your specialized knowledge to handle various algorithms and data formats your contribution plays a vital role in ensuring the smooth execution of their tasks
</DOC>

