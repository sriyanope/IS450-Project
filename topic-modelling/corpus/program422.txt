<DOC>
ibm data warehouse engineer
this is intended to help you develop the jobready and portfolio for an entrylevel business intelligence bi or data warehousing engineering position throughout the online courses immerse yourself in the indemand role of a data warehouse engineer and acquire the essential you need to with a range of tools and databases to design deploy operationalize and manage enterprise data warehouses edw
by the end of this be able to perform the key tasks required in a data warehouse engineering role with relational database management systems rdbms and query data using sql statements
use linuxunix shell scripts to automate repetitive tasks and build data pipelines with tools like apache airflow and kafka to extract transform and load etl data gain with managing databases and data warehouses
finally design and populate data warehouse systems and utilize business intelligence tools to analyze and extract insights using reports and dashboards
this is suitable for anyone with a passion for learning and is suitable for you whether you have a college degree or not and does not require any prior data engineering or programming

each includes numerous handson labs and a project to hone and apply the concepts and you by the end of the have designed implemented configured queried and maintained numerous databases and created data pipelines using realworld tools and data repositories to build a portfolio of jobready
start by provisioning a database instance on cloud next design databases using entityrelationship diagrams erd and create database objects like tables and keys using mysql postgresql and ibm db
then become proficient with querying databases with sql using select insert update and delete statements and to filter sort aggregate result sets next become familiar with common linuxunix shell commands and use them to build bash scripts
create data pipelines for batch and streaming etl jobs using apache airflow and kafka finally implement data warehouses create bi dashboards
</DOC>

<DOC>
etl and data pipelines with shell airflow and kafka
delve into the two different approaches to converting raw data into analyticsready data one approach is the extract transform load etl process the other contrasting approach is the extract load and transform elt process etl processes apply to data warehouses and data marts elt processes apply to data lakes where the data is transformed on demand by the requestingcalling application
about the different tools and techniques that are used with etl and data pipelines both etl and elt extract data from source systems move the data through the data pipeline and store the data in destination systems during this how elt and etl processing differ and identify use cases for both identify methods and tools used for extracting the data merging extracted data either logically or physically and for loading data into data repositories also define transformations to apply to source data to make the data credible contextual and accessible to data users be able to outline some of the multiple methods for loading data into the destination system verifying data quality monitoring load failures and the use of recovery mechanisms in case of failure by the end of this also know how to use apache airflow to build data pipelines as be knowledgeable about the advantages of using this approach also how to use apache kafka to build streaming pipelines as as the core components of kafka which include brokers topics partitions replications producers and consumers finally complete a shareable final project that enables you to demonstrate the you acquired in each
</DOC>
<DOC>data processing techniques
etl or extract transform and load processes are used for cases where flexibility speed and scalability of data are important explore some key differences between similar processes etl and elt which include the place of transformation flexibility big data support and timetoinsight that there is an increasing demand for access to raw data that drives the evolution from etl to elt data extraction involves advanced technologies including database querying web scraping and apis also that data transformation is about formatting data to suit the application and that data is loaded in batches or streamed continuously
intro etl fundamentals elt basics comparing etl and elt data extraction techniques introduction to data transformation techniques data loading techniques
</DOC>

<DOC>etl data pipelines tools and techniques
extract transform and load etl pipelines are created with bash scripts that can be run on a schedule using cron data pipelines move data from one place or form to another data pipeline processes include scheduling or triggering monitoring maintenance and optimization furthermore batch pipelines extract and operate on batches of data whereas streaming data pipelines ingest data packets onebyone in rapid succession that streaming pipelines apply when the most current data is needed explore that parallelization and io buffers help mitigate bottlenecks also how to describe data pipeline performance in terms of latency and throughput
etl using shell scripting introduction to data pipelines key data pipeline processes batch versus streaming data pipeline use cases data pipeline tools and technologies
</DOC>

<DOC>building data pipelines using airflow
the key advantage of apache airflows approach to representing data pipelines as dags is that they are expressed as code which makes your data pipelines more maintainable testable and collaborative tasks the nodes in a dag are created by implementing airflows builtin operators about apache airflow having a rich ui that simplifies working with data pipelines explore how to visualize your dag in graph or tree mode also about the key components of a dag definition file and that airflow logs are saved into local file systems and then sent to cloud storage search engines and log analyzers
apache airflow advantages of representing data pipelines as dags in apache airflow apache airflow ui build a dag using airflow airflow logging and monitoring
</DOC>

<DOC>building streaming pipelines using kafka
apache kafka is a very popular open source event streaming pipeline an event is a type of data that describes the entitys observable state updates over time popular kafka service providers include confluent cloud ibm event stream and amazon msk additionally kafka streams api is a client library supporting you with data processing in event streaming pipelines that the core components of kafka are brokers topics partitions replications producers and consumers explore two special types of processors in the kafka stream api streamprocessing topology the source processor and the sink processor also about building event streaming pipelines using kafka
distributed event streaming platform components apache kafka building event streaming pipelines using kafka kafka streaming process
</DOC>

<DOC>final
final apply your newly gained knowledge to explore two very exciting handson labs creating etl data pipelines using apache airflow and creating streaming data pipelines using kafka explore building these etl pipelines using realworld scenarios extract transform and load data into a csv file also create a topic named toll in apache kafka download and customize a streaming data consumer as as verifying that streaming data has been collected in the database table
</DOC>
<DOC>
relational database administration dba
get started with relational database administration and database management selfpaced
this begins with an introduction to database management
about things like the database management lifecycle the roles of a database administrator dba as as database storage then discover some of the activities techniques and best practices for managing a database also about database optimization including updating statistics slow queries types of indexes and index creation and usage about configuring and upgrading database server software and related products also about database security
how to implement user authentication assign roles and assign objectlevel permissions and gain an understanding of how to perform backup and restore procedures in case of system failures how to optimize databases for performance monitor databases collect diagnostic data and access error information to help you resolve issues that may occur many of these tasks are repetitive so how to schedule maintenance activities and regular diagnostic tests and send automated messages of the success or failure of a task the includes both videobased lectures as as handson labs to practice and apply what you this ends with a final project where assume the role of a database administrator and complete a number of database administration tasks across many different databases
</DOC>
<DOC>introduction to database management
welcome to your first on database administration during this be introduced to a wide variety of information including an of the types of tasks involved in database management and what a typical workday may look like see that many tasks revolve around activities ranging from designing databases to planning and troubleshooting errors at the end of the there will be several labs where gain handson learning about server objects configurations and database objects including schemas tables triggers and events
day in the life of a database administrator database management lifecycle database objects system objects and database configuration database storage
</DOC>

<DOC>managing databases
during this how to back up and restore databases these processes are essential aspects of any organizations health and its ability to respond quickly to changes in the system through handson labs why it is vital to create backups and define policies and procedures also about database security and user management including creating and resetting user passwords creating groups and more
introduction to backup and restore types of backup backup policies using database transaction logs for recovery of database security users groups and roles managing access to databases and their objects auditing database activity encrypting data
</DOC>

<DOC>monitoring and optimization
how to create and keep baselines performance metrics standards and finally monitor ram and disk usage connections and cache stats also about database optimization including updating statistics slow queries types of indexes and index creation and usage
of database monitoring monitoring usage and performance part monitoring usage and performance part optimizing databases optimizing queries using indexes
</DOC>

<DOC>troubleshooting automation
about some basic troubleshooting processes that help data engineers find frequently occurring issues such as connectivity login configuration and whether the instance is running also how to automate many database functions from managing alerts to generating and sending reports using standard linux and unix shell commands or cron jobs
troubleshooting common issues using status variables error codes and documentation using logs for troubleshooting of automating database tasks automating reports and alerts
</DOC>

<DOC>final and final exam
complete your final project which brings together concepts and practices you previously learned in the first four three part perform database administration tasks across different databases such as mysql postgresql db and sqlite start by installing and configuring a database managing users and performing a backup move on to recovery indexing optimization and automation of routine tasks finally restore a database create an index create a view and connect to a database from the command line
</DOC>
<DOC>
bi dashboards with ibm cognos analytics and google looker
business intelligence bi analyst is one of the top fastest growing roles according to statista in its which jobs have a future update ibm cognos analytics and google looker studio are powerful bi tools used for data visualization analytics and reporting this short helps you to build ibm cognos analytics and google looker studio that can open up in business analytics data science and bi across industries
the introduces you to the features and capabilities of ibm cognos analytics and google looker studio the basics of visualizing data without writing code plus how use both to create interactive dashboards also gain practical through handson labs and complete a final project in which create data visualizations and an interactive dashboard that you can share with prospective employers to highlight your if youre looking to get started as a data analyst bi analyst or data warehouse specialist this provides the ideal introduction to two high profile tools used in these roles enroll selfpaced today and develop valuable bi dashboard you can talk about in interviews
</DOC>
<DOC>ibm cognos analytics for data analysis and visualization
fasttrack your data analytics learning and gain handson data analytics using ibm cognos analytics after registering with ibm cognos analytics explore the platforms capabilities by creating visualizations building a simple dashboard and trying out its advanced features
introduction introduction to analytics and business intelligence bi tools cognos analytics introduction and how to sign up navigating in cognos analytics creating a simple dashboard in cognos analytics advanced capabilities in cognos analytics dashboards accessing your warehouse with cognos analytics
</DOC>

<DOC>data visualizations and dashboards with google looker studio
explore google looker studio to create visualizations and build dashboards
getting started with google looker studio connecting to data sources visualizing reports and configuring themes and layouts working with pages and blended data in reports in google looker studio adding controls filters and visualizations in reports in google looker studio
</DOC>

<DOC>final project final exam and wrapup
complete the final that will be graded by your peers final create some visualizations and add them to a dashboard using ibm cognos analytics or google looker studio
</DOC>
<DOC>
introduction to relational databases rdbms
are you ready to dive into the world of data engineering beginner level gain a solid understanding of how data is stored processed and accessed in relational databases rdbmses with different types of databases that are appropriate for various data processing requirements
begin this by being introduced to relational database concepts as as several industry standard relational databases including ibm db mysql and postgresql next utilize rdbms tools used by professionals such as phpmyadmin and pgadmin for creating and maintaining relational databases also use the command line and sql statements to create and manage tables this incorporates handson practical exercises to help you demonstrate your learning with real databases and explore realworld datasets create database instances and populate them with tables and data at the end of this complete a final where apply your accumulated knowledge from this and demonstrate that you have the to design a database for a specific analytics requirement normalize tables create tables and views in the database load and access data no prior knowledge of databases or programming is required anyone can audit this at nocharge if you choose to take this and earn the coursera you can also earn an ibm digital badge upon successful completion of the
</DOC>
<DOC>relational database concepts
first about the fundamental aspects of data structures and file formats along with the differences between relational and nonrelational databases explore various types of data models and discuss fundamental concepts in database management additionally explore entityrelationship diagrams erd along with their components and relationships also gain expertise in diverse database topics finally gain a clear understanding of db and postgresql
introduction review of data fundamentals information and data models erds and types of relationships mapping entities to tables data types relational model concepts database architecture distributed architecture and clustered databases database usage patterns introduction to relational database offerings db mysql postgresql
</DOC>

<DOC>using relational databases
explore the types of sql statements like data definition language ddl and data manipulation language dml how to create modify and manage tables using ddl statements and understand data movement utilities for efficient data loading and management additionally dive into key database objects such as schemas primary keys foreign keys and indexes gaining insights into their roles in data organization integrity and retrieval also understand the importance of normalization for reducing redundancy and ensuring data consistency while also understanding various constraints within the relational model to maintain data accuracy and reliability
types of sql statements ddl vs dml creating tables create table statement alter drop and truncate tables data movement utilities loading data database objects and hierarchy including schemas primary keys and foreign keys of indexes normalization relational model constraints advanced
</DOC>

<DOC>mysql and postgresql
about the fundamental aspects of mysql and postgresql and identify relational database management system rdbms tools explore the process of creating databases and tables and the definition of keys constraints and connections in mysql additionally discover important processes in postgresql using command line pgadmin and views moreover gain essential such as database loading techniques and insights into securing sensitive data and streamlining data retrieval
getting started with mysql creating databases and tables in mysql populating mysql databases and tables using keys and constraints in mysql getting started with postgresql creating databases and loading data in postgresql views
</DOC>

<DOC>final project and assessment
navigate the database design process refine your practical and understand essential steps discover the role of entity relationship diagrams erds and get an to engage in a handson database design lab where use your theoretical knowledge to create databases as you progress receive an optional final and project submission stages for those seeking an advanced challenge a final project using db is available a glossary is available for quick reference to key terms used throughout the
</DOC>
<DOC>
handson introduction to linux commands and shell scripting
this provides a practical understanding of common linux unix shell commands beginner friendly about the linux basics shell commands and bash shell scripting
begin this with an introduction to linux and explore the linux architecture interact with the linux terminal execute commands navigate directories edit files as as install and update software next become familiar with commonly used linux commands with general purpose commands like id date uname ps top echo man
directory management commands such as pwd cd mkdir rmdir find df
file management commands like cat wget more head tail cp mv touch tar zip unzip
access control command chmod
text processing commands wc grep tr
as as networking commands hostname ping ifconfig and curl then move on to learning the basics of shell scripting to automate a variety of tasks create simple to more advanced shell scripts that involve metacharacters quoting variables command substitution io redirection pipes filters and command line arguments also schedule cron jobs using crontab the includes both videobased lectures as as handson labs to practice and apply what you have nocharge access to a virtual linux server that you can access through your web browser so you dont need to download and install anything to complete the labs end this with a final project as as a final exam in the final project demonstrate your knowledge of concepts by performing your own extract transform and load etl process and create a scheduled backup script this is ideal for data engineers data scientists software developers and cloud practitioners who want to get familiar with frequently used commands on linux macos and other unixlike operating systems as as get started with creating shell scripts
</DOC>
<DOC>introduction to linux
about the basics of linux be able to summarize the origins of the linux operating system and list its key features and use cases what a linux distribution is the names of popular distributions and their key characteristics also be able to explain the linux architecture interact with a linux system using the terminal and navigate directories using paths and shortcuts this will also teach you how to create and edit text files using text editors such as nano and vim lastly how to use a software package manager to install and updates on a linux system
introduction introducing linux and unix linux distributions of linux architecture linux terminal creating and editing text files installing software and updates
</DOC>

<DOC>introduction to linux commands
how to use common linux commands what a shell and shell commands are and how to use commands to do various tasks in linux this will teach you how to use informational commands to find relevant information about your system navigation commands to navigate files and directories and management commands to create delete copy and move files and directories also to sort and view files in useful ways and extract specific lines and fields from your files be able to use networking commands to examine your network configuration and evaluate identify and retrieve data from urls finally this will cover file archiving and compression commands
of common linux shell commands informational commands file and directory navigation commands file and directory management commands viewing file content useful commands for wrangling text files networking commands file archiving and compression commands
</DOC>

<DOC>introduction to shell scripting
the basics of shell scripting what a script is and when to use scripts be able to describe the shebang interpreter directive and create and run a simple shell script additionally this will teach you how to use pipes and filters and set shell and environment variables by the end of this also be able to list features of bash shell scripting and use crontab to schedule cron jobs understand the cron syntax and view and remove cron jobs
shell scripting basics filters pipes and variables useful features of the bash shell scheduling jobs using cron
</DOC>

<DOC>final project and final exam
complete a practice project in which you create an automated extract transform load etl process to extract daily weather forecasts and observed weather data schedule this process to run automatically at a set time daily and how to create a script to measure forecast accuracy in your peergraded final project create a scheduled backup script finally demonstrate the knowledge youve gained by taking a final graded exam
</DOC>
<DOC>
sql a practical introduction for querying databases
much of the worlds data lives in databases sql or structured query language is a powerful programming language that is used for communicating with and manipulating data in databases a working knowledge of databases and sql is a must for anyone who wants to start a in data engineering data warehousing data analytics data science or business intelligence the purpose of this is to help you and apply foundational and intermediate knowledge of the sql language and become familiar with many relational database rdbms concepts along the way
start with performing basic create read update and delete crud operations using create select insert update and delete statements then to filter order sort and aggregate data with functions perform subselects and nested queries as as join data in multiple tables also with views transactions and create stored procedures the emphasis is on handson practical learning as such with real database systems use real tools and realworld datasets create a database instance in the cloud through a series of handson labs practice building and running sql queries at the end of the apply and demonstrate your with a final project the sql you will be applicable to a variety of rdbmses such as mysql postgresql ibm db oracle sql server and others no prior knowledge of databases sql or programming is required however some basic data literacy is beneficial
</DOC>
<DOC>getting started with sql
be introduced to databases create a database instance on the cloud some of the basic sql statements also write and practice basic sql handson on a live database
introduction introduction to databases select statement count distinct limit insert statement update and delete statements
</DOC>

<DOC>introduction to relational databases and tables
explore the fundamental concepts behind databases tables and the relationships between them then create an instance of a database discover sql statements that allow you to create and manipulate tables and then practice them on your own live database
relational database concepts types of sql statements ddl vs dml create table statement alter drop and truncate tables how to create a database instance on cloud
</DOC>

<DOC>intermediate sql
how to use string patterns and ranges to search data and how to sort and group data in result sets also practice composing nested queries and execute select statements to access data from multiple tables
using string patterns and ranges sorting result sets grouping result sets builtin database functions date and time builtin functions subqueries and nested selects working with multiple tables
</DOC>

<DOC>working with realworld data sets final project exam
be working with multiple real world datasets for the city of chicago be asked questions that will help you understand the data just as you would in the real wold be assessed on the correctness of your sql queries and results
working with real world datasets getting table and column details
</DOC>

<DOC>advanced sql honors
this covers some advanced sql techniques that will be useful for data engineers if you are following the data engineering track you must complete this completion of this is not required for those completing the data science or data analyst tracks how to build more powerful queries with advanced sql techniques like views transactions stored procedures and joins
views stored procedures acid transactions join inner join outer joins
</DOC>
<DOC>
data warehouse fundamentals
whether youre an aspiring data engineer data architect business analyst or data scientist strong data warehousing are a must with the handson and competencies you gain on this your resume will catch the eye of employers and power up your
a data warehouse centralizes and organizes data from disparate sources into a single repository making it easier for data professionals to access clean and analyze integrated data efficiently this teaches you how to design deploy load manage and query data warehouses data marts and data lakes dive into designing modeling and implementing data warehouses and explore data warehousing architectures like star and snowflake schemas master techniques for populating data warehouses through etl and elt processes and hone your in verifying and querying data and utilizing concepts like cubes rollups and materialized viewstables additionally gain valuable practical working on handson labs where apply your knowledge to real data warehousing tasks with repositories like postgresql and ibm db and complete a project that you can refer to in interviews
</DOC>
<DOC>data warehouses data marts and data lakes
welcome to your first this provides an introduction to data warehouse systems data lakes and data marts when you complete this be able to identify and compare data warehouse systems data marts and data lakes based on their architecture and understand how organizations can benefit from each of these three data storage entities then about three types of data warehouse systems and popular data warehouse system vendors to help your organization assess new data warehouse system offerings when you know the five essential critical criteria including the total cost of ownership to evaluate before changing to a new data warehouse system
introduction data warehouse popular data warehouse systems selecting a data warehouse system data marts ibm db warehouse data lakes data lakehouses explained
</DOC>

<DOC>designing modeling and implementing data warehouses
knowledgepacked explore general and reference enterprise data warehousing architecture discover how data cubes relate to star schemas then how to slice dice drill up or down roll up and pivot relative to data cubes next examine the capabilities of materialized views their benefits and how to apply them how a data organization using facts and dimensions and their related tables organizes information then explore how to use normalization to create a snowflake schema as an extension of the star schema also about populating a data warehouse incremental data updates verifying data querying data and interpreting an entityrelationship diagram for a star schema finally the will delve into the creation of a materialized view the application of cube and rollup options and examine the advantages organizations gain from implementing staging
of data warehouse architectures cubes rollups and materialized views and tables facts and dimensional modeling data modeling using star and snowflake schemas staging areas for data warehouses verify data quality populating a data warehouse querying the data
</DOC>

<DOC>final and final quiz
complete your practice project and final project which bring together concepts and practices you previously learned in the first two in the final project design and load data into a data warehouse using facts and dimension tables then write aggregation queries using cube and rollup functions and create a materialized view in the optional lesson explore the workings of ibm db data warehouse system architecture view use cases and understand the key capabilities and integrations available with ibm db warehouse the handson labs lesson will enable you to gain practical knowledge on how to create a db service instance how to populate a data warehouse using ibm db how to query the data warehouse using ibm db
</DOC>
<DOC>
introduction to data engineering
start your journey in one of the fastest growing professions today with this beginnerfriendly data engineering be introduced to the core concepts processes and tools you need to know in order to get a foundational knowledge of data engineering as as the roles that data engineers data scientists and data analysts play in the ecosystem
begin this by understanding what is data engineering as as the roles that data engineers data scientists and data analysts play exciting field next about the data engineering ecosystem the different types of data structures file formats sources of data and the languages data professionals use in their daytoday tasks become familiar with the components of a data platform and gain an understanding of several different types of data repositories such as relational rdbms and nosql databases data warehouses data marts data lakes and data lakehouses then about big data processing tools like apache hadoop and spark also become familiar with etl elt data pipelines and data integration this provides you with an understanding of a typical data engineering lifecycle which includes architecting data platforms designing data stores and gathering importing wrangling querying and analyzing data also about security governance and compliance about in the field of data engineering and the different paths that you can take for getting skilled as a data engineer hear from several experienced data engineers sharing their insights and advice by the end of this also have completed several handson labs and worked with a relational database loaded data into the database and performed some basic querying operations
</DOC>
<DOC>what is data engineering
about the different entities that come together to form a modern data ecosystem and the role data engineers data scientists data analysts business analysts and business intelligence analysts play ecosystem what data engineering is and the key tasks in a data engineering lifecycle also gain an understanding of the responsibilities of a data engineer the skillsets they need in order to be successful and what a typical day in the life of a data engineer looks like
welcome to introduction to data engineering modern data ecosystem key players in the data ecosystem specializations in data engineering what is data engineering viewpoints defining data engineering viewpoints evolution of data engineering responsibilities and skillsets of a data engineer viewpoints and qualities to be a data engineer a day in the life of a data engineer
</DOC>

<DOC>the data engineering ecosystem
about the data engineering ecosystem the different types of data structures file formats sources of data and the languages data professionals use in their daytoday tasks gain an understanding of several different types of data repositories such as relational and nonrelational databases data warehouses data marts and data lakes about etl and elt processes data pipelines and data integration platforms also gain an understanding of what big data is and the tools used for processing and storing big data at the end of this be guided to create an ibm cloud account and provision an instance of ibm db
of the data engineering ecosystem types of data understanding different types of file formats sources of data languages for data professionals viewpoints working with varied data sources and types of data repositories rdbms nosql data warehouses data marts and data lakes optional data lakehouses explained viewpoints considerations for choice of data repository etl elt and data pipelines data integration platforms viewpoints tools databases and data repositories of choice foundations of big data big data processing tools hadoop hdfs hive and spark viewpoints impact of big data on data engineering
</DOC>

<DOC>data engineering lifecycle
walk you through the data engineering lifecycle about the architecture of a data platform factors for selecting and designing data stores and the different facets of security as it applies to data platforms and data lifecycle management also about the process steps and tools used for gathering importing wrangling and querying data gain an understanding of performance monitoring and the steps you can take to troubleshoot performance issues also talk about governance regulations why we need them and how technology enables compliance to regulations during the of this be guided to load data from a csv file into the ibm db instance you created in the previous also be guided to explore your dataset using some basic sql queries that will be provided to you
architecting the data platform factors for selecting and designing data stores security viewpoints importance of data security how to gather and import data data wrangling tools for data wrangling querying and analyzing data performance tuning and troubleshooting governance and compliance
</DOC>

<DOC>and data engineering in action
about in the field of data engineering and the different paths that you can take for getting skilled as a data engineer at the end of the be presented with the final graded which is divided into two parts the first part of the final includes a couple of quiz questions and the second part includes openended questions that will be reviewed and graded by a peer
in data engineering viewpoints get into data engineering data engineering learning path viewpoints what do employers look for in a data engineer viewpoints the many paths to data engineering viewpoints advice to aspiring data engineers
</DOC>
<DOC>
data warehousing capstone project
apply a variety of data warehouse engineering and techniques you have learned as part of the previous courses in the ibm data warehouse engineer assume the role of a junior data engineer who has recently joined the organization and be presented with a realworld use case that requires a data warehouse engineering solution
</DOC>
<DOC>data platform architecture and oltp database
design a data platform that uses mysql as an oltp database be using mysql to store the oltp data
introduction to capstone project oltp database
</DOC>

<DOC>build a data warehouse
design and implement a data warehouse and then generate reports from the data in the data warehouse
data warehouse design setup data warehouse reporting
</DOC>

<DOC>data analytics
assume the role of a data engineer at an ecommerce company your company has finished setting up a data warehouse now you are assigned the responsibility to design a reporting dashboard that reflects the key metrics of the business
</DOC>

<DOC>etl data pipelines
set up an etl process using a shell script to extract new transactional data for each day from the mysql database and load it into the staging data warehouse in postgresql later perform the transformation on the table in the staging warehouse to load the data in a dimension table and a fact table then export these tables as csv files to the production warehouse set up a cron job to schedule these tasks
</DOC>

<DOC>final submission and peer review
final complete your submission of screenshots from the handson labs for your peers to review once you have completed your submission then review the submission of one of your peers and grade their submission
</DOC>
