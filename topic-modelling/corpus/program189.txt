<DOC>
ibm ai enterprise workflow
this six is designed to prepare you to take the certification examination for ibm ai enterprise workflow v data science specialist ibm ai enterprise workflow is a comprehensive endtoend process that enables data scientists to build ai solutions starting with business priorities and working through to taking ai into production the learning aims to elevate the of practicing data scientists by explicitly connecting business priorities to technical implementations connecting machine learning to specialized ai use cases such as visual recognition and nlp and connecting python to ibm cloud technologies the and case studies in these courses are designed to guide you through your as a data scientist at a hypothetical streaming media company
throughout this the focus will be on the practice of data science in large modern enterprises be guided through the use of enterpriseclass tools on the ibm cloud tools that use to create deploy and test machine learning models your favorite open source tools such a jupyter notebooks and python libraries will be used extensively for data preparation and building models models will be deployed on the ibm cloud using ibm watson tooling that works seamlessly with open source tools after successfully completing this be ready to take the official ibm certification examination for the ibm ai enterprise workflowopens in a new tab
</DOC>

<DOC>
ai workflow feature engineering and bias detection
this is the third in the ibm ai enterprise workflow certification you are strongly encouraged to complete these courses in order as they are not individual independent courses but part of a workflow where each builds on the previous ones
introduces you to the next stage of the workflow for our hypothetical media company stage of best practices for feature engineering handling class imbalances and detecting bias in the data class imbalances can seriously affect the validity of your machine learning models and the mitigation of bias in data is essential to reducing the risk associated with biased models these topics will be followed by sections on best practices for dimension reduction outlier detection and unsupervised learning techniques for finding patterns in your data the case studies will focus on topic modeling and data visualization by the end of this be able to employ the tools that help address class and class imbalance issues explain the ethical considerations regarding bias in data employ ai fairness open source libraries to detect bias in models employ dimension reduction techniques for both eda and transformations stages describe topic modeling techniques in natural language processing use topic modeling and visualization to explore text data employ outlier handling best practices in high dimension data employ outlier detection algorithms as a quality assurance tool and a modeling tool employ unsupervised learning techniques using pipelines as part of the ai workflow employ basic clustering algorithms who should take this this targets existing data science practitioners that have expertise building machine learning models who want to deepen their on building and deploying ai in large enterprises if you are an aspiring data scientist this is not for you as you need real world expertise to benefit from the content of these courses what should you have it is assumed that you have completed courses and of the ibm ai enterprise workflow and you have a solid understanding of the following topics prior to starting this fundamental understanding of linear algebra
understand sampling probability theory and probability distributions
knowledge of descriptive and inferential statistical concepts
general understanding of machine learning techniques and best practices
practiced understanding of python and the packages commonly used in data science numpy pandas matplotlib scikitlearn
familiarity with ibm watson studio
familiarity with the design thinking process
</DOC>
<DOC>data transforms and feature engineering
this will introduce you to required for effective feature engineering in todays business enterprises the are presented as a series of best practices representing years of practical
data transformations introduction to class imbalance class imbalance deep dive introduction to dimensionality reduction dimension reduction case study intro feature engineering
</DOC>

<DOC>pattern recognition and data mining best practices
this will continue the discussion of related to feature engineering for practicing data scientists with a focus on outliers and the use of unsupervised learning techniques for finding patterns
exploring ibms ai fairness toolkit introduction to outliers outlier detection introduction to unsupervised learning unsupervised learning
</DOC>
<DOC>
ai workflow ai in production
this is the sixth in the ibm ai enterprise workflow certification you are strongly encouraged to complete these courses in order as they are not individual independent courses but part of a workflow where each builds on the previous ones
this focuses on models in production at a hypothetical streaming media company there is an introduction to ibm watson machine learning build your own api in a docker container and how to manage containers with kubernetes the also introduces several other tools in the ibm ecosystem designed to help deploy or maintain models in production the ai workflow is not a linear process so there is some time dedicated to the most important feedback loops in order to promote efficient iteration on the overall workflow by the end of this be able to use docker to deploy a flask application deploy a simple ui to integrate the ml model watson nlu and watson visual recognition discuss basic kubernetes terminology deploy a scalable web application on kubernetes discuss the different feedback loops in ai workflow discuss the use of unit testing in the context of model production use ibm watson openscale to assess bias and performance of production machine learning models who should take this this targets existing data science practitioners that have expertise building machine learning models who want to deepen their on building and deploying ai in large enterprises if you are an aspiring data scientist this is not for you as you need real world expertise to benefit from the content of these courses what should you have it is assumed that you have completed courses through of the ibm ai enterprise workflow and you have a solid understanding of the following topics prior to starting this fundamental understanding of linear algebra
understand sampling probability theory and probability distributions
knowledge of descriptive and inferential statistical concepts
general understanding of machine learning techniques and best practices
practiced understanding of python and the packages commonly used in data science numpy pandas matplotlib scikitlearn
familiarity with ibm watson studio
familiarity with the design thinking process
</DOC>
<DOC>feedback loops and monitoring
this focuses on feedback loops and monitoring feedback loops represent all the possible ways you can return to an earlier stage in the ai enterprise workflow we initially discussed feedback loops in the first of this however here our focus is on unit testing we are also looking at business value a very important consideration that often gets overlooked is the model having as significant effect on business metrics as intended it is important to be able to use log files that have been standardized across the team to answer questions about business value as as performance monitoring have an to complete a case study on performance monitoring where write unit tests for a logger and a logging api endpoint test them and write a suite of unit tests to validate if the logging is working correctly
feedback loops and unit testing feedback loops and unit tests performance monitoring and business metrics performance drift performance monitoring case study
</DOC>

<DOC>hands on with openscale and kubernetes
this will wrap up the formal learning by completing hands on tutorials of watson openscale and kubernetes ibm watson opensscale is a suite of services that allows you to track the performance of production ai and its impact on business goals with actionable metrics in a single console kubernetes is a container orchestration platform for managing scheduling and automating the deployment of docker containers the containers we have developed as part of this are essentially microservices meant to be deployed as cloud native applications
operationalize trusted ai with ibm watson openscale kubernetes explained kubernetes vs docker its not an eitheror question
</DOC>

<DOC>capstone pulling it all together part
you start part one data investigation of a threepart capstone project designed to pull everything you have learned together we have provided a brief review of what you should have learned thus far however you may want to review the first five courses prior to starting the project a major goal of this capstone is to emulate a realworld scenario so we wont be providing notebooks to guide you as we have done with the previous case studies
</DOC>

<DOC>capstone pulling it all together part
complete your capstone project and submit it for peer review part of the capstone project involves building models and selecting the best model to deploy use timeseries algorithms to predict future values based on previously observed values over time in part of the capstone project your focus will be creating a postproduction analysis script that investigates the relationship between model performance and the business metrics aligned with the deployed model after completing and submitting your capstone project have access to the solution files for further review
</DOC>
<DOC>
ai workflow business priorities and data ingestion
this is the first of a six part you are strongly encouraged to complete these courses in order as they are not individual independent courses but part of a workflow where each builds on the previous ones
this first in the ibm ai enterprise workflow certification introduces you to the scope of the and prerequisites specifically the courses are meant for practicing data scientists who are knowledgeable about probability statistics linear algebra and python tooling for data science and machine learning a hypothetical streaming media company will be introduced as your new client be introduced to the concept of design thinking ibms framework for organizing large enterprise ai projects also be introduced to the basics of scientific thinking because the quality that distinguishes a seasoned data scientist from a beginner is creative scientific thinking finally start your for the hypothetical media company by understanding the data they have and by building a data ingestion pipeline using python and jupyter notebooks by the end of this you should be able to know the advantages of carrying out data science using a structured process describe how the stages of design thinking correspond to the ai enterprise workflow discuss several strategies used to prioritize business explain where data science and data engineering have the most overlap in the ai workflow explain the purpose of testing in data ingestion describe the use case for sparse matrices as a target destination for data ingestion know the initial steps that can be taken towards automation of data ingestion pipelines who should take this this targets existing data science practitioners that have expertise building machine learning models who want to deepen their on building and deploying ai in large enterprises if you are an aspiring data scientist this is not for you as you need real world expertise to benefit from the content of these courses what should you have it is assumed you have a solid understanding of the following topics prior to starting this fundamental understanding of linear algebra
understand sampling probability theory and probability distributions
knowledge of descriptive and inferential statistical concepts
general understanding of machine learning techniques and best practices
practiced understanding of python and the packages commonly used in data science numpy pandas matplotlib scikitlearn
familiarity with ibm watson studio
familiarity with the design thinking process
</DOC>
<DOC>ibm ai enterprise workflow introduction
the goal of this first is to introduce you to the overall requirements evaluate your understanding of some key prerequisite knowledge and familiarize you with several process models commonly used today use the process of design thinking but it is the consistent application of a process in practice that is important not the exact process itself there are a number of reasons for choosing the design thinking process but the most important is that it is being applied in a crossdisciplinary waythat is outside of data science
introduction ibm watson studio create a project workflow
</DOC>

<DOC>data collection
throughout this or reinforce what you already know about identifying and articulating business the importance of applying a scientific thought process to the task of understanding the business use case this process has many similarities to that of being an investigator also generate a healthy respect for the need to pause step back and think scientifically about the main processes stage
data collection introduction to business introduction to scientific thinking for business introduction to gathering data ai workflow gathering data
</DOC>

<DOC>data ingestion
cleaning parsing assembling and gutchecking data is among the most timeconsuming tasks that a data scientist has to perform the time spent on data cleaning can start at and increase depending on data quality and the project requirements this looks at the process of ingesting data and presents a case study working a real world scenario
introduction to data ingestion ai workflow data ingestion ai workflow sparse matrices for data pipeline development using watson studio to complete the case study case study
</DOC>
<DOC>
ai workflow machine learning visual recognition and nlp
this is the fourth in the ibm ai enterprise workflow certification you are strongly encouraged to complete these courses in order as they are not individual independent courses but part of a workflow where each builds on the previous ones
covers the next stage of the workflow setting up models and their associated data pipelines for a hypothetical streaming media company the first topic covers the complex topic of evaluation metrics where best practices for a number of different metrics including regression metrics classification metrics and multiclass metrics which use to select the best model for your business challenge the next topics cover best practices for different types of models including linear models treebased models and neural networks outofthebox watson models for natural language understanding and visual recognition will be used there will be case studies focusing on natural language processing and on image analysis to provide realistic context for the model pipelines by the end of this be able to discuss common regression classification and multilabel classification metrics explain the use of linear and logistic regression in supervised learning applications describe common strategies for grid searching and crossvalidation employ evaluation metrics to select models for production use explain the use of treebased algorithms in supervised learning applications explain the use of neural networks in supervised learning applications discuss the major variants of neural networks and recent advances create a neural net model in tensorflow create and test an instance of watson visual recognition create and test an instance of watson nlu who should take this this targets existing data science practitioners that have expertise building machine learning models who want to deepen their on building and deploying ai in large enterprises if you are an aspiring data scientist this is not for you as you need real world expertise to benefit from the content of these courses what should you have it is assumed that you have completed courses through of the ibm ai enterprise workflow and you have a solid understanding of the following topics prior to starting this fundamental understanding of linear algebra
understand sampling probability theory and probability distributions
knowledge of descriptive and inferential statistical concepts
general understanding of machine learning techniques and best practices
practiced understanding of python and the packages commonly used in data science numpy pandas matplotlib scikitlearn
familiarity with ibm watson studio
familiarity with the design thinking process
</DOC>
<DOC>model evaluation and performance metrics
covers model selection evaluation and performance metrics the focus is on evaluating models iteratively for improvements survey the landscape of evaluation metrics and linear models in order to ensure you are comfortable using implementing baseline models the materials build up to the case study where use natural language processing in a classification setting when you are done iterating on your model connect its model performance to business metrics as an approach to better understand model utility
objectives evaluation metrics introduction to predictive linear and logistic regression linear models watson natural language understanding service case study introduction
</DOC>

<DOC>building machine learning and deep learning models
is primarily focused on building supervised learning models survey available methods in two popular and effective areas of machine learning tree based algorithms and deep learning algorithms cover the use of tree based methods like random forests and boosting along with other ensemble approaches many of these approaches serve as an important middle layer between interpretable linear models and difficult to interpret deeplearning models for deep learning use a prebuilt visual recognition model and use tensorflow to demonstrate how to build tune and iterate on neural networks also make sure that you understand popular neural network architectures in the case study implement a convolutional neural network and ready it for deployment
tree based methods introduction to tree based methods neural networks introduction to neural networks ibm watson visual recognition
</DOC>
<DOC>
ai workflow data analysis and hypothesis testing
this is the second in the ibm ai enterprise workflow certification you are strongly encouraged to complete these courses in order as they are not individual independent courses but part of a workflow where each builds on the previous ones
begin your for a hypothetical streaming media company by doing exploratory data analysis eda best practices for data visualization handling missing data and hypothesis testing will be introduced to you as part of your techniques of estimation with probability distributions and extending these estimates to apply null hypothesis significance tests apply what you through two hands on case studies data visualization and multiple testing using a simple pipeline by the end of this you should be able to list several best practices concerning eda and data visualization create a simple dashboard in watson studio describe strategies for dealing with missing data explain the difference between imputation and multiple imputation employ common distributions to answer questions about event probabilities explain the investigative role of hypothesis testing in eda apply several methods for dealing with multiple testing who should take this this targets existing data science practitioners that have expertise building machine learning models who want to deepen their on building and deploying ai in large enterprises if you are an aspiring data scientist this is not for you as you need real world expertise to benefit from the content of these courses what should you have it is assumed that you have completed of the ibm ai enterprise workflow and have a solid understanding of the following topics prior to starting this fundamental understanding of linear algebra
understand sampling probability theory and probability distributions
knowledge of descriptive and inferential statistical concepts
general understanding of machine learning techniques and best practices
practiced understanding of python and the packages commonly used in data science numpy pandas matplotlib scikitlearn
familiarity with ibm watson studio
familiarity with the design thinking process
</DOC>
<DOC>data analysis
exploratory data analysis is mostly about gaining insight through visualization and hypothesis testing this unit looks at eda data visualization and missing values one missing value strategy may be better for some models but for others another strategy may show better predictive performance
eda introduction to data visualizations data visualizations introduction to missing values missing values case study introduction
</DOC>

<DOC>data investigation
data scientists employ a broad range of statistical tools to analyze data and reach conclusions from data this unit focuses on the foundational techniques of estimation with probability distributions and extending these estimates to apply null hypothesis significance tests
introduction to hypothesis testing hypothesis testing case study introduction
</DOC>
<DOC>
ai workflow enterprise model deployment
this is the fifth in the ibm ai enterprise workflow certification you are strongly encouraged to complete these courses in order as they are not individual independent courses but part of a workflow where each builds on the previous ones
this introduces you to an area that few data scientists are able to deploying models for use in large enterprises apache spark is a very commonly used framework for running machine learning models best practices for using spark will be covered best practices for data manipulation model training and model tuning will also be covered the use case will call for the creation and deployment of a recommender system the wraps up with an introduction to model deployment technologies by the end of this be able to use apache sparks rdds dataframes and a pipeline employ sparksubmit scripts to interface with spark environments explain how collaborative filtering and contentbased filtering build a data ingestion pipeline using apache spark and apache spark streaming analyze hyperparameters in machine learning models on apache spark deploy machine learning algorithms using the apache spark machine learning interface deploy a machine learning model from watson studio to watson machine learning who should take this this targets existing data science practitioners that have expertise building machine learning models who want to deepen their on building and deploying ai in large enterprises if you are an aspiring data scientist this is not for you as you need real world expertise to benefit from the content of these courses what should you have it is assumed that you have completed courses through of the ibm ai enterprise workflow and you have a solid understanding of the following topics prior to starting this fundamental understanding of linear algebra
understand sampling probability theory and probability distributions
knowledge of descriptive and inferential statistical concepts
general understanding of machine learning techniques and best practices
practiced understanding of python and the packages commonly used in data science numpy pandas matplotlib scikitlearn
familiarity with ibm watson studio
familiarity with the design thinking process
</DOC>
<DOC>deploying models
today data scientists have more tooling than ever before to create modeldriven or algorithmic solutions and it is important to know when to take the time to make code optimizations we spend a lot of time performing hands on activities we start by interacting with apache spark then progressing to a tutorial with docker wrap up the working through a tutorial on watson machine learning
introduction to data at scale introduction to spark model management and deployment in watson studio
</DOC>

<DOC>deploying models using spark
is primarily focused on deploying models using spark the rationale to move to spark almost always has to do with scale either at the level of model training or at the level of prediction although the resources available to build spark applications are fewer than those for scikitlearn spark gives us the ability to build in an entirely scaleable environment also look at recommendation systems most recommender systems today are able to leverage both explicit eg numerical ratings and implicit eg likes purchases skipped bookmarked patterns in a ratings matrix the majority of modern recommender systems embrace either a collaborative filtering or a contentbased approach a number of other approaches and hybrids exist making some implemented systems difficult to categorize we wrap the up with our handson case study on model deployment
introduction to spark machine learning spark recommendations recommenders introduction to model deployment case study
</DOC>
