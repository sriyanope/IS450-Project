<DOC>
ibm introduction to machine
machine are becoming more and more essential in the modern job market in machine engineer was ranked as the job in the united states based on the incredible growth of job openings in the field between to and the roles average base salary of indeedopens in a new tab this fourcourse will help you gain the introductory to succeed in an indemand in machine and data science after completing this be able to realize the potential of machine algorithms and artificial intelligence in different business scenarios be able to identify when to use machine to explain certain behaviors and when to use it to predict future also how to evaluate your machine models and to incorporate best practices by the end of this have developed concrete machine to apply in your workplace or search as well as a portfolio of demonstrating your proficiency in addition to receiving a from coursera also earn an ibm badgeopens in a new tab to help you share your accomplishments with your network and potential employeryou can also leverage the from the to complete the remaining two courses of the sixcourse ibm machine certificateopens in a new tab and power a new in the field of machine applied projectin this complete handson designed to develop your analytical and machine also produce a of your insights from each using data analysis in a similar way as you would in a setting including producing a final presentation to communicate insights to fellow machine practitioners stakeholders csuite executives and chief data officersyou are highly encouraged to compile your completed into an online portfolio that showcases the learned
</DOC>

<DOC>
supervised machine regression
this introduces you to one of the main types of modelling families of supervised machine regression how to train regression models to predict continuous and how to use error metrics to compare across different models this also walks you through best practices including train and test splits and regularization techniquesby the end of this you should be able to differentiate uses and applications of classification and regression in the context of supervised machine describe and use linear regression models use a variety of error metrics to compare and select a linear regression model that best suits your data articulate why regularization may help prevent overfitting use regularization regressions ridge lasso and elastic net who should take this this targets aspiring data scientists interested in acquiring handson with supervised machine regression techniques in a business setting what should you have to make the most out of this you should have familiarity with programming on a python development environment as well as fundamental understanding of data cleaning exploratory data analysis calculus linear algebra probability and statistics

introduction to supervised machine and linear regression
this introduces a brief overview of supervised machine and its main applications classification and regression after introducing the concept of regression its best practices as well as how to measure error and select the regression model that best suits your data
welcomeintroduction introduction to supervised machine types of machine part introduction to supervised machine types of machine part supervised machine part supervised machine part regression and classification examples introduction to linear regression part introduction to linear regression part optional linear regression demo part optional linear regression demo part optional linear regression demo part

data splits and polynomial regression
there are a few best practices to avoid overfitting of your regression models one of these best practices is splitting your data into training and test sets another alternative is to use cross validation and a third alternative is to introduce polynomial features this walks you through the theoretical framework and a few handson examples of these best practices
training and test splits part training and test splits part optional training and test splits lab part optional training and test splits lab part optional training and test splits lab part optional training and test splits lab part polynomial regression

cross validation
there is a tradeoff between the size of your training set and your testing set if you use most of your data for training have fewer samples to validate your model conversely if you use more samples for testing have fewer samples to train your model cross validation will allow you to reuse your data to use more samples for training and testing
cross validation part cross validation demo part cross validation demo part cross validation demo part cross validation demo part cross validation demo part

bias variance trade off and regularization techniques ridge lasso and elastic net
this walks you through the theory and a few handson examples of regularization regressions including ridge lasso and elastic net realize the main pros and cons of these techniques as well as their differences and similarities
bias variance trade off part bias variance trade off part regularization and model selection ridge regression lasso regression part lasso regression part elastic net polynomial features and regularization demo part polynomial features and regularization demo part polynomial features and regularization demo part

regularization details
section understand the relationship between the loss function and the different regularization types
further details of regularization part further details of regularization part optional details of regularization part optional details of regularization part optional details of regularization part

final
section test everything you learned
</DOC>

<DOC>
exploratory data analysis for machine
this first in the ibm machine introduces you to machine and the content of the realize the importance of good quality data common techniques to retrieve your data clean it apply feature engineering and have it ready for preliminary analysis and hypothesis testingby the end of this you should be able to retrieve data from multiple data sources sql nosql databases apis cloud describe and use common feature selection and feature engineering techniques handle categorical and ordinal features as well as missing values use a variety of techniques for detecting and dealing with outliers articulate why feature scaling is important and use a variety of scaling techniques who should take this this targets aspiring data scientists interested in acquiring handson with machine and artificial intelligence in a business setting what should you have to make the most out of this you should have familiarity with programming on a python development environment as well as fundamental understanding of calculus linear algebra probability and statistics

a brief history of modern ai and its applications
artificial intelligence is not new but it is new in a sense that it is easier than ever to get started using machine in business settings we will go over a quick introduction to ai and machine and we will visit a brief history of the modern ai we will also explore some of the current applications of ai and machine for you to think about how you want to leverage them in your day to day business practice or personal
introduction introduction to artificial intelligence and machine machine and deep machine and deep part machine and deep part history of ai history of machine and deep modern ai applications machine workflow

retrieving and cleaning data
good data is the fuel that powers machine and artificial intelligence how to retrieve data from different sources how to clean it to ensure its quality
retrieving data from csv and json files retrieving data from databases apis and the cloud optional lab solution reading data jupyter notebook part a optionallab solution reading in database files part b data cleaning handling missing values and outliers handling missing values and outliers using residuals

exploratory data analysis and feature engineering
how to conduct exploratory analysis to visually confirm it is ready for machine modeling by feature engineering and transformations
introduction to exploratory data analysis eda eda with visualization grouping data for eda optionalsolution eda notebook part optionalsolution eda notebook part optionalsolution eda notebook part optionalsolution eda notebook part feature engineering and variable transformation background variable transformation feature encoding feature scaling common variable transformations in python optional solution feature engineering lab part optional solution feature engineering lab part optional solution feature engineering lab part

inferential statistics and hypothesis testing
inferential statistics and hypothesis testing are two types of data analysis often overlooked at early stages of analyzing your data they can give you quick insights about the quality of your data they also help you confirm business intuition and help you prescribe what to analyze next using machine this looks at useful definitions and simple examples that will help you get started creating hypothesis around your business problem and how to test them
estimation and inference introduction estimation and inference example estimation and inference parametric vs nonparametric estimation and inference commonly used distributions frequentist vs bayesian statistics introduction to hypothesis hypothesis testing example bayesian interpretation of hypothesis testing example type vs type error type vs type error examples hypothesis testing terminology significance level and pvalues significance level and pvalues and the f statistic optional hypothesis testing demo part optional hypothesis testing demo part correlation vs causation

optional honors
optional honors apply your and knowledge learned throughout the you can select a dataset from the ones used or any other dataset of interest and apply all of the demonstrated techniques including data cleaning feature engineering exploratory data visualization and hypothesis testing
</DOC>

<DOC>
unsupervised machine
this introduces you to one of the main types of machine unsupervised how to find insights from data sets that do not have a target or labeled variable several clustering and dimension reduction algorithms for unsupervised as well as how to select the algorithm that best suits your data the handson section of this focuses on using best practices for unsupervised learningby the end of this you should be able to explain the kinds of problems suitable for unsupervised approaches explain the curse of dimensionality and how it makes clustering difficult with many features describe and use common clustering and dimensionalityreduction algorithms try clustering points where appropriate compare the performance of percluster models understand metrics relevant for characterizing clusters who should take this this targets aspiring data scientists interested in acquiring handson with unsupervised machine techniques in a business setting what should you have to make the most out of this you should have familiarity with programming on a python development environment as well as fundamental understanding of data cleaning exploratory data analysis calculus linear algebra probability and statistics

introduction to unsupervised and k means
this introduces unsupervised and its applications one of the most common uses of unsupervised is clustering observations using kmeans you become familiar with the theory behind this algorithm and put it in practice in a demonstration
introduction introduction to unsupervised overview introduction to unsupervised use cases of clustering introduction to clustering kmeans kmeans initialization selecting the right number of clusters in kmeans elbow method and applying kmeans optional k means notebook part k means notebook part optional k means notebook part

distance metrics computational hurdles
reading app items
distance metrics euclidean and manhattan distance distance metrics cosine and jaccard distance curse of dimensionality notebook part curse of dimensionality notebook part curse of dimensionality notebook part curse of dimensionality notebook part

selecting a clustering algorithm
you become familiar with some of the computational hurdles around clustering algorithms and how different clustering implementations try to overcome them after a brief recapitulation of common clustering algorithms how to compare them and select the clustering technique that best suits your data
hierarchical agglomerative clustering hierarchical agglomerative clustering hierarchical linkage types applying hierarchical agglomerative clustering dbscan visualizing dbscan mean shift comparing algorithms clustering notebook part clustering notebook part optional clustering notebook part clustering notebook part

dimensionality reduction
this introduces dimensionality reduction and principal component analysis which are powerful techniques for big data imaging and preprocessing data
dimensionality reduction overview dimensionality reduction principal component analysis optional dimensionality reduction notebook part dimensionality reduction notebook part dimensionality reduction imaging example

nonlinear and distancebased dimensionality reduction
this introduces dimensionality reduction techniques like kernal principal component analysis and multidimensional scaling these methods are more powerful than principal component analysis in many applications
kernel principal component analysis and multidimensional scaling dimensionality reduction notebook part

matrix factorization
this introduces matrix factorization which is a powerful technique for big data text mining and preprocessing data
non negative matrix factorization non negative matrix factorization notebook part non negative matrix factorization notebook part

final
now you have all the tools in your toolkit to highlight your unsupervised abilities in your final
</DOC>

<DOC>
supervised machine classification
this introduces you to one of the main types of modeling families of supervised machine classification how to train predictive models to classify categorical and how to use error metrics to compare across different models the handson section of this focuses on using best practices for classification including train and test splits and handling data sets with unbalanced classesby the end of this you should be able to differentiate uses and applications of classification and classification ensembles describe and use logistic regression models describe and use decision tree and treeensemble models describe and use other ensemble methods for classification use a variety of error metrics to compare and select the classification model that best suits your data use oversampling and undersampling as techniques to handle unbalanced classes in a data set who should take this this targets aspiring data scientists interested in acquiring handson with supervised machine classification techniques in a business setting what should you have to make the most out of this you should have familiarity with programming on a python development environment as well as fundamental understanding of data cleaning exploratory data analysis calculus linear algebra probability and statistics

logistic regression
logistic regression is one of the most studied and widely used classification algorithms probably due to its popularity in regulated industries and financial settings although more modern classifiers might likely output models with higher accuracy logistic regressions are great baseline models due to their high interpretability and parametric nature this will walk you through extending a linear regression example into a logistic regression as well as the most common error metrics that you might want to use to compare several classifiers and select that best suits your business problem
welcome introduction what is classification introduction to logistic regression classification with logistic regression logistic regression with multiclasses implementing logistic regression models confusion matrix accuracy specificity precision and recall classification error metrics roc and precisionrecall curves implementing the calculation of roc and precisionrecall curves optional logistic regression lab part optional logistic regression lab part optional logistic regression lab part

k nearest neighbors
k nearest neighbors is a popular classification method because they are easy computation and easy to interpret this walks you through the theory behind k nearest neighbors as well as a demo for you to practice building k nearest neighbors models with sklearn
k nearest neighbors for classification k nearest neighbors decision boundary k nearest neighbors distance measurement k nearest neighbors pros and cons k nearest neighbors with feature scaling optional k nearest neighbors notebook part optional k nearest neighbors notebook part optional k nearest neighbors notebook part

support vector machines
this will walk you through the main idea of how support vector machines construct hyperplanes to map your data into regions that concentrate a majority of data points of a certain class although support vector machines are widely used for regression outlier detection and classification this will focus on the latter
introduction to support vector machines classification with support vector machines the support vector machines cost function regularization in support vector machines introduction to support vector machines gaussian kernels support vector machines gaussian kernels part support vector machines gaussian kernels part support vector machines workflow implementing support vector machines kernal models optional support vector machines notebook part optional support vector machines notebook part optional support vector machines notebook part

decision trees
decision tree methods are a common baseline model for classification tasks due to their visual appeal and high interpretability this walks you through the theory behind decision trees and a few handson examples of building decision tree models for classification realize the main pros and cons of these techniques this background will be useful when you are presented with decision tree ensembles in the next
overview of classifiers introduction to decision trees building a decision tree entropybased splitting other decision tree splitting criteria pros and cons of decision trees optional decision trees notebook part optional decision trees notebook part optional decision trees notebook part

ensemble models
ensemble models are a very popular technique as they can assist your models be more resistant to outliers and have better chances at generalizing with future data they also gained popularity after several ensembles helped people win prediction competitions recently stochastic gradient boosting became a goto candidate model for many data scientists this model walks you through the theory behind ensemble models and popular treebased ensembles
ensemble based methods and bagging part ensemble based methods and bagging part ensemble based methods and bagging part random forest optional bagging notebook part optional bagging notebook part optional bagging notebook part review of bagging overview of boosting adaboost and gradient boosting overview adaboost and gradient boosting syntax stacking optional boosting notebook part optional boosting notebook part optional boosting notebook part

modeling unbalanced classes
some classification models are better suited than others to outliers low occurrence of a class or rare events the most common methods to add robustness to a classifier are related to stratified sampling to rebalance the training data this will walk you through both stratified sampling methods and more novel approaches to model data sets with unbalanced classes
model interpretability examples of selfinterpretable and nonselfinterpretable models modelagnostic explanations surrogate models introduction to unbalanced classes upsampling and downsampling modeling approaches weighting and stratified sampling modeling approaches random and synthetic oversampling modeling approaches nearing neighbor methods modeling approaches blagging
</DOC>

