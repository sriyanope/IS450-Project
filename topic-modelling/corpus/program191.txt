<DOC>
ibm introduction to machine learning
machine learning are becoming more and more essential in the modern job market in machine learning engineer was ranked as the job in the united states based on the incredible growth of job openings in the field between to and the roles average base salary of indeedopens in a new tab
this fourcourse will help you gain the introductory to succeed in an indemand in machine learning and data science after completing this be able to realize the potential of machine learning algorithms and artificial intelligence in different business scenarios be able to identify when to use machine learning to explain certain behaviors and when to use it to predict future also how to evaluate your machine learning models and to incorporate best practices
by the end of this have developed concrete machine learning to apply in your workplace or search as as a portfolio of projects demonstrating your proficiency in addition to receiving a from coursera also earn an ibm badgeopens in a new tab to help you share your accomplishments with your network and potential employer
you can also leverage the learning from the to complete the remaining two courses of the sixcourse ibm machine learning certificateopens in a new tab and power a new in the field of machine learning

complete handson projects designed to develop your analytical and machine learning also produce a of your insights from each project using data analysis in a similar way as you would in a setting including producing a final presentation to communicate insights to fellow machine learning practitioners stakeholders csuite executives and chief data officers
you are highly encouraged to compile your completed projects into an online portfolio that showcases the learned
</DOC>

<DOC>
supervised machine learning regression
this introduces you to one of the main types of modelling families of supervised machine learning regression how to train regression models to predict continuous and how to use error metrics to compare across different models this also walks you through best practices including train and test splits and regularization techniques
by the end of this you should be able to differentiate uses and applications of classification and regression in the context of supervised machine learning describe and use linear regression models use a variety of error metrics to compare and select a linear regression model that best suits your data articulate why regularization may help prevent overfitting use regularization regressions ridge lasso and elastic net who should take this this targets aspiring data scientists interested in acquiring handson with supervised machine learning regression techniques in a business setting what should you have to make the most out of this you should have familiarity with programming on a python development environment as as fundamental understanding of data cleaning exploratory data analysis calculus linear algebra probability and statistics
</DOC>
<DOC>introduction to supervised machine learning and linear regression
this introduces a brief of supervised machine learning and its main applications classification and regression after introducing the concept of regression its best practices as as how to measure error and select the regression model that best suits your data
welcomeintroduction introduction to supervised machine learning types of machine learning part introduction to supervised machine learning types of machine learning part supervised machine learning part supervised machine learning part regression and classification examples introduction to linear regression part introduction to linear regression part optional linear regression demo part optional linear regression demo part optional linear regression demo part
</DOC>

<DOC>data splits and polynomial regression
there are a few best practices to avoid overfitting of your regression models one of these best practices is splitting your data into training and test sets another alternative is to use cross validation and a third alternative is to introduce polynomial features this walks you through the theoretical framework and a few handson examples of these best practices
training and test splits part training and test splits part optional training and test splits lab part optional training and test splits lab part optional training and test splits lab part optional training and test splits lab part polynomial regression
</DOC>

<DOC>cross validation
there is a tradeoff between the size of your training set and your testing set if you use most of your data for training have fewer samples to validate your model conversely if you use more samples for testing have fewer samples to train your model cross validation will allow you to reuse your data to use more samples for training and testing
cross validation part cross validation demo part cross validation demo part cross validation demo part cross validation demo part cross validation demo part
</DOC>

<DOC>bias variance trade off and regularization techniques ridge lasso and elastic net
this walks you through the theory and a few handson examples of regularization regressions including ridge lasso and elastic net realize the main pros and cons of these techniques as as their differences and similarities
bias variance trade off part bias variance trade off part regularization and model selection ridge regression lasso regression part lasso regression part elastic net polynomial features and regularization demo part polynomial features and regularization demo part polynomial features and regularization demo part
</DOC>

<DOC>regularization details
section understand the relationship between the loss function and the different regularization types
further details of regularization part further details of regularization part optional details of regularization part optional details of regularization part optional details of regularization part
</DOC>

<DOC>final project
section test everything you learned
</DOC>
<DOC>
exploratory data analysis for machine learning
this first in the ibm machine learning introduces you to machine learning and the content of the realize the importance of good quality data common techniques to retrieve your data clean it apply feature engineering and have it ready for preliminary analysis and hypothesis testing
by the end of this you should be able to retrieve data from multiple data sources sql nosql databases apis cloud describe and use common feature selection and feature engineering techniques handle categorical and ordinal features as as missing values use a variety of techniques for detecting and dealing with outliers articulate why feature scaling is important and use a variety of scaling techniques who should take this this targets aspiring data scientists interested in acquiring handson with machine learning and artificial intelligence in a business setting what should you have to make the most out of this you should have familiarity with programming on a python development environment as as fundamental understanding of calculus linear algebra probability and statistics
</DOC>
<DOC>a brief history of modern ai and its applications
artificial intelligence is not new but it is new in a sense that it is easier than ever to get started using machine learning in business settings go over a quick introduction to ai and machine learning and visit a brief history of the modern ai also explore some of the current applications of ai and machine learning for you to think about how you want to leverage them in your day to day business practice or personal projects
introduction introduction to artificial intelligence and machine learning machine learning and deep learning machine learning and deep learning part machine learning and deep learning part history of ai history of machine learning and deep learning modern ai applications machine learning workflow
</DOC>

<DOC>retrieving and cleaning data
good data is the fuel that powers machine learning and artificial intelligence how to retrieve data from different sources how to clean it to ensure its quality
retrieving data from csv and json files retrieving data from databases apis and the cloud optional lab solution reading data jupyter notebook part a optionallab solution reading in database files part b data cleaning handling missing values and outliers handling missing values and outliers using residuals
</DOC>

<DOC>exploratory data analysis and feature engineering
how to conduct exploratory analysis to visually confirm it is ready for machine learning modeling by feature engineering and transformations
introduction to exploratory data analysis eda eda with visualization grouping data for eda optionalsolution eda notebook part optionalsolution eda notebook part optionalsolution eda notebook part optionalsolution eda notebook part feature engineering and variable transformation background variable transformation feature encoding feature scaling common variable transformations in python optional solution feature engineering lab part optional solution feature engineering lab part optional solution feature engineering lab part
</DOC>

<DOC>inferential statistics and hypothesis testing
inferential statistics and hypothesis testing are two types of data analysis often overlooked at early stages of analyzing your data they can give you quick insights about the quality of your data they also help you confirm business intuition and help you prescribe what to analyze next using machine learning this looks at useful definitions and simple examples that will help you get started creating hypothesis around your business problem and how to test them
estimation and inference introduction estimation and inference example estimation and inference parametric vs nonparametric estimation and inference commonly used distributions frequentist vs bayesian statistics introduction to hypothesis hypothesis testing example bayesian interpretation of hypothesis testing example type vs type error type vs type error examples hypothesis testing terminology significance level and pvalues significance level and pvalues and the f statistic optional hypothesis testing demo part optional hypothesis testing demo part correlation vs causation
</DOC>

<DOC>optional honors project
optional honors project apply your and knowledge learned throughout the you can select a dataset from the ones used or any other dataset of interest and apply all of the demonstrated techniques including data cleaning feature engineering exploratory data visualization and hypothesis testing
</DOC>
<DOC>
unsupervised machine learning
this introduces you to one of the main types of machine learning unsupervised learning how to find insights from data sets that do not have a target or labeled variable several clustering and dimension reduction algorithms for unsupervised learning as as how to select the algorithm that best suits your data the handson section of this focuses on using best practices for unsupervised learning
by the end of this you should be able to explain the kinds of problems suitable for unsupervised learning approaches explain the curse of dimensionality and how it makes clustering difficult with many features describe and use common clustering and dimensionalityreduction algorithms try clustering points where appropriate compare the performance of percluster models understand metrics relevant for characterizing clusters who should take this this targets aspiring data scientists interested in acquiring handson with unsupervised machine learning techniques in a business setting what should you have to make the most out of this you should have familiarity with programming on a python development environment as as fundamental understanding of data cleaning exploratory data analysis calculus linear algebra probability and statistics
</DOC>
<DOC>introduction to unsupervised learning and k means
this introduces unsupervised learning and its applications one of the most common uses of unsupervised learning is clustering observations using kmeans you become familiar with the theory behind this algorithm and put it in practice in a demonstration
introduction introduction to unsupervised learning introduction to unsupervised learning use cases of clustering introduction to clustering kmeans kmeans initialization selecting the right number of clusters in kmeans elbow method and applying kmeans optional k means notebook part k means notebook part optional k means notebook part
</DOC>

<DOC>distance metrics computational hurdles
reading app items
distance metrics euclidean and manhattan distance distance metrics cosine and jaccard distance curse of dimensionality notebook part curse of dimensionality notebook part curse of dimensionality notebook part curse of dimensionality notebook part
</DOC>

<DOC>selecting a clustering algorithm
you become familiar with some of the computational hurdles around clustering algorithms and how different clustering implementations try to overcome them after a brief recapitulation of common clustering algorithms how to compare them and select the clustering technique that best suits your data
hierarchical agglomerative clustering hierarchical agglomerative clustering hierarchical linkage types applying hierarchical agglomerative clustering dbscan visualizing dbscan mean shift comparing algorithms clustering notebook part clustering notebook part optional clustering notebook part clustering notebook part
</DOC>

<DOC>dimensionality reduction
this introduces dimensionality reduction and principal component analysis which are powerful techniques for big data imaging and preprocessing data
dimensionality reduction dimensionality reduction principal component analysis optional dimensionality reduction notebook part dimensionality reduction notebook part dimensionality reduction imaging example
</DOC>

<DOC>nonlinear and distancebased dimensionality reduction
this introduces dimensionality reduction techniques like kernal principal component analysis and multidimensional scaling these methods are more powerful than principal component analysis in many applications
kernel principal component analysis and multidimensional scaling dimensionality reduction notebook part
</DOC>

<DOC>matrix factorization
this introduces matrix factorization which is a powerful technique for big data text mining and preprocessing data
non negative matrix factorization non negative matrix factorization notebook part non negative matrix factorization notebook part
</DOC>

<DOC>final project
now you have all the tools in your toolkit to highlight your unsupervised learning abilities in your final project
</DOC>
<DOC>
supervised machine learning classification
this introduces you to one of the main types of modeling families of supervised machine learning classification how to train predictive models to classify categorical and how to use error metrics to compare across different models the handson section of this focuses on using best practices for classification including train and test splits and handling data sets with unbalanced classes
by the end of this you should be able to differentiate uses and applications of classification and classification ensembles describe and use logistic regression models describe and use decision tree and treeensemble models describe and use other ensemble methods for classification use a variety of error metrics to compare and select the classification model that best suits your data use oversampling and undersampling as techniques to handle unbalanced classes in a data set who should take this this targets aspiring data scientists interested in acquiring handson with supervised machine learning classification techniques in a business setting what should you have to make the most out of this you should have familiarity with programming on a python development environment as as fundamental understanding of data cleaning exploratory data analysis calculus linear algebra probability and statistics
</DOC>
<DOC>logistic regression
logistic regression is one of the most studied and widely used classification algorithms probably due to its popularity in regulated industries and financial settings although more modern classifiers might likely output models with higher accuracy logistic regressions are great baseline models due to their high interpretability and parametric nature this will walk you through extending a linear regression example into a logistic regression as as the most common error metrics that you might want to use to compare several classifiers and select that best suits your business problem
welcome introduction what is classification introduction to logistic regression classification with logistic regression logistic regression with multiclasses implementing logistic regression models confusion matrix accuracy specificity precision and recall classification error metrics roc and precisionrecall curves implementing the calculation of roc and precisionrecall curves optional logistic regression lab part optional logistic regression lab part optional logistic regression lab part
</DOC>

<DOC>k nearest neighbors
k nearest neighbors is a popular classification method because they are easy computation and easy to interpret this walks you through the theory behind k nearest neighbors as as a demo for you to practice building k nearest neighbors models with sklearn
k nearest neighbors for classification k nearest neighbors decision boundary k nearest neighbors distance measurement k nearest neighbors pros and cons k nearest neighbors with feature scaling optional k nearest neighbors notebook part optional k nearest neighbors notebook part optional k nearest neighbors notebook part
</DOC>

<DOC>support vector machines
this will walk you through the main idea of how support vector machines construct hyperplanes to map your data into regions that concentrate a majority of data points of a certain class although support vector machines are widely used for regression outlier detection and classification this will focus on the latter
introduction to support vector machines classification with support vector machines the support vector machines cost function regularization in support vector machines introduction to support vector machines gaussian kernels support vector machines gaussian kernels part support vector machines gaussian kernels part support vector machines workflow implementing support vector machines kernal models optional support vector machines notebook part optional support vector machines notebook part optional support vector machines notebook part
</DOC>

<DOC>decision trees
decision tree methods are a common baseline model for classification tasks due to their visual appeal and high interpretability this walks you through the theory behind decision trees and a few handson examples of building decision tree models for classification realize the main pros and cons of these techniques this background will be useful when you are presented with decision tree ensembles in the next
of classifiers introduction to decision trees building a decision tree entropybased splitting other decision tree splitting criteria pros and cons of decision trees optional decision trees notebook part optional decision trees notebook part optional decision trees notebook part
</DOC>

<DOC>ensemble models
ensemble models are a very popular technique as they can assist your models be more resistant to outliers and have better chances at generalizing with future data they also gained popularity after several ensembles helped people win prediction competitions recently stochastic gradient boosting became a goto candidate model for many data scientists this model walks you through the theory behind ensemble models and popular treebased ensembles
ensemble based methods and bagging part ensemble based methods and bagging part ensemble based methods and bagging part random forest optional bagging notebook part optional bagging notebook part optional bagging notebook part review of bagging of boosting adaboost and gradient boosting adaboost and gradient boosting syntax stacking optional boosting notebook part optional boosting notebook part optional boosting notebook part
</DOC>

<DOC>modeling unbalanced classes
some classification models are better suited than others to outliers low occurrence of a class or rare events the most common methods to add robustness to a classifier are related to stratified sampling to rebalance the training data this will walk you through both stratified sampling methods and more novel approaches to model data sets with unbalanced classes
model interpretability examples of selfinterpretable and nonselfinterpretable models modelagnostic explanations surrogate models introduction to unbalanced classes upsampling and downsampling modeling approaches weighting and stratified sampling modeling approaches random and synthetic oversampling modeling approaches nearing neighbor methods modeling approaches blagging
</DOC>
