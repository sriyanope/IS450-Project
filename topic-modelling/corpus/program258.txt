<DOC>
mathematics for machine
for a lot of higher level courses in machine and data science you find you need to freshen up on the basics in mathematics stuff you may have studied before in school or university but which was taught in another context or not very intuitively such that you struggle to relate it to how its used in computer science this aims to bridge that gap getting you up to speed in the underlying mathematics building an intuitive understanding and relating it to machine and data sciencein the first on linear algebra we look at what linear algebra is and how it relates to data then we look through what vectors and matrices are and how to with themthe second multivariate calculus builds on this to look at how to optimize fitting functions to get good fits to data it starts from introductory calculus and then uses the matrices and vectors from the first to look at data fittingthe third dimensionality reduction with principal component analysis uses the mathematics from the first two courses to compress highdimensional data this is of intermediate difficulty and will require python and numpy knowledgeat the end of this have gained the prerequisite mathematical knowledge to continue your journey and take more advanced courses in machine learningapplied projectthrough the of this use the you have learned to produce miniprojects with python on interactive notebooks an easy to tool which will help you apply the knowledge to real world problems for example using linear algebra in order to calculate the page rank of a small simulated internet applying multivariate calculus in order to train your own neural network performing a nonlinear least squares regression to fit a model to a data set and using principal component analysis to determine the features of the mnist digits data set
</DOC>

<DOC>
mathematics for machine linear algebra
on linear algebra we look at what linear algebra is and how it relates to vectors and matrices then we look through what vectors and matrices are and how to with them including the knotty problem of eigenvalues and eigenvectors and how to use these to solve problems finally we look at how to use these to do fun things with datasets like how to rotate images of faces and how to extract eigenvectors to look at how the pagerank algorithm workssince were aiming at datadriven applications well be implementing some of these ideas in code not just on pencil and paper towards the end of the write code blocks and encounter jupyter notebooks in python but dont worry these will be quite short focussed on the concepts and will guide you through if youve not coded before at the end of this have an intuitive understanding of vectors and matrices that will help you bridge the gap into linear algebra problems and how to apply these concepts to machine

introduction to linear algebra and to mathematics for machine
first we look at how linear algebra is relevant to machine and data science then well wind up the with an initial introduction to vectors throughout were focussing on developing your mathematical intuition not of crunching through algebra or doing long penandpaper examples for many of these operations there are callable functions in python that can do the adding up the point is to appreciate what they do and how they so that when things go wrong or there are special cases you can understand why and what to do
introduction solving data science challenges with mathematics motivations for linear algebra getting a handle on vectors operations with vectors

vectors are objects that move around space
we look at operations we can do with vectors finding the modulus size angle between vectors dot or inner product and projections of one vector onto another we can then examine how the entries describing a vector will depend on what vectors we use to define the axes the basis that will then let us determine whether a proposed set of basis vectors are whats called linearly independent this will complete our examination of vectors allowing us to move on to matrices in and then start to solve linear algebra problems
introduction to vectors modulus inner product cosine dot product projection changing basis basis vector space and linear independence applications of changing basis

matrices in linear algebra objects that operate on vectors
now that weve looked at vectors we can turn to matrices first we look at how to use matrices as tools to solve linear algebra problems and as objects that transform vectors then we look at how to solve systems of linear equations using matrices which will then take us on to look at inverse matrices and determinants and to think about what the determinant really is intuitively speaking finally well look at cases of special matrices that mean that the determinant is zero or where the matrix isnt invertible cases where algorithms that need to invert a matrix will fail
matrices vectors and solving simultaneous equation problems how matrices transform space types of matrix transformation composition or combination of matrix transformations solving the apples and bananas problem gaussian elimination going from gaussian elimination to finding the inverse matrix determinants and inverses

matrices make linear mappings
in we continue our discussion of matrices first we think about how to code up matrix multiplication and matrix operations using the einstein summation convention which is a widely used notation in more advanced linear algebra courses then we look at how matrices can transform a of a vector from one basis set of axes to another this will allow us to for example figure out how to apply a reflection to an image and manipulate images well also look at how to construct a convenient basis vector set in order to do such transformations then well write some code to do these transformations and apply this computationally
introduction einstein summation convention and the symmetry of the dot product matrices changing basis doing a transformation in a changed basis orthogonal matrices the gramschmidt process example reflecting in a plane

eigenvalues and eigenvectors application to data problems
eigenvectors are particular vectors that are unrotated by a transformation matrix and eigenvalues are the amount by which the eigenvectors are stretched these special eigenthings are very useful in linear algebra and will let us examine googles famous pagerank algorithm for presenting web search results then well apply this in code which will wrap up the
welcome to what are eigenvalues and eigenvectors special eigencases calculating eigenvectors changing to the eigenbasis eigenbasis example introduction to pagerank wrap up of this linear algebra
</DOC>

<DOC>
mathematics for machine multivariate calculus
this offers a brief introduction to the multivariate calculus required to build many common machine techniques we start at the very beginning with a refresher on the rise over run formulation of a slope before converting this to the formal definition of the gradient of a function we then start to build up a set of tools for making calculus easier and faster next we how to calculate vectors that point up hill on multidimensional surfaces and even put this into action using an interactive game we take a look at how we can use calculus to build approximations to functions as well as helping us to quantify how accurate we should expect those approximations to be we also spend some time talking about where calculus comes up in the training of neural networks before finally showing you how it is applied in linear regression models this is intended to offer an intuitive understanding of calculus as well as the language necessary to look concepts up yourselves when you get stuck hopefully without going into too much detail still come away with the confidence to dive into some more focused machine courses in future

what is calculus
understanding calculus is central to understanding machine you can think of calculus as simply a set of tools for analysing the relationship between functions and their inputs often in machine we are trying to find the inputs which enable a function to best match the data we start this from the basics by recalling what a function is and where we might encounter one following this we talk about the how when sketching a function on a graph the slope describes the rate of change of the output with respect to an input using this visual intuition we next derive a robust mathematical definition of a derivative which we then use to differentiate some interesting functions finally by studying a few examples we develop four handy time saving rules that enable us to speed up differentiation for many common scenarios
welcome to multivariate calculus welcome to functions rise over run definition of a derivative differentiation examples special cases product rule chain rule taming a beast see you next

multivariate calculus
building on the foundations of the previous we now generalise our calculus tools to handle multivariable systems this means we can take a function with multiple inputs and determine the influence of each of them separately it would not be unusual for a machine method to require the analysis of a function with thousands of inputs so we will also introduce the linear algebra structures necessary for storing the results of our multivariate calculus analysis in an orderly fashion
welcome to variables constants context differentiate with respect to anything the jacobian jacobian applied the sandpit the hessian reality is hard see you next

multivariate chain rule and its applications
having seen that multivariate calculus is really no more complicated than the univariate case we now focus on applications of the chain rule neural networks are one of the most popular and successful conceptual structures in machine they are build up from a connected web of neurons and inspired by the structure of biological brains the behaviour of each neuron is influenced by a set of control parameters each of which needs to be optimised to best fit the data the multivariate chain rule can be used to calculate the influence of each parameter of the networks allow them to be updated during training
welcome to multivariate chain rule more multivariate chain rule simple neural networks more simple neural networks see you next

taylor series and linearisation
the taylor series is a method for reexpressing functions as polynomial series this approach is the rational behind the use of simple linear approximations to complicated functions we will derive the formal expression for the univariate taylor series and discuss some important consequences of this result relevant to machine finally we will discuss the multivariate case and see how the jacobian and the hessian come in to play
welcome to building approximate functions power series power series derivation power series details examples linearisation multivariate taylor see you next

intro to optimisation
if we want to find the minimum and maximum points of a function then we can use multivariate calculus to do this say to optimise the parameters the space of a function to fit some data first well do this in one dimension and use the gradient to give us estimates of where the zero points of that function are and then iterate in the newtonraphson method then well extend the idea to multiple dimensions by finding the gradient vector grad which is the vector of the jacobian this will then let us find our way to the minima and maxima in what is called the gradient descent method well then take a moment to use grad to find the minima and maxima along a constraint in the space which is the lagrange multipliers method
welcome to gradient descent constrained optimisation see you next

regression
in order to optimise the fitting parameters of a fitting function to the best fit for some data we need a way to define how good our fit is this goodness of fit is called chisquared which well first apply to fitting a straight line linear regression then well look at how to optimise our fitting function using chisquared in the general case using the gradient descent method finally well look at how to do this easily in python in just a few lines of code which will wrap up the
simple linear regression general non linear least squares doing least squares regression analysis in practice wrap up of this
</DOC>

<DOC>
mathematics for machine pca
this intermediatelevel introduces the mathematical foundations to derive principal component analysis pca a fundamental dimensionality reduction technique well cover some basic statistics of data sets such as mean values and variances well compute distances and angles between vectors using inner products and derive orthogonal projections of data onto lowerdimensional subspaces using all these tools well then derive pca as a method that minimizes the average squared reconstruction error between data points and their reconstructionat the end of this be familiar with important mathematical concepts and you can implement pca all by yourself if youre struggling find a set of jupyter notebooks that will allow you to explore properties of the techniques and walk you through what you need to do to get on track if you are already an expert this may refresh some of your knowledge the lectures examples and exercises require some ability of abstract thinking good background in linear algebra eg matrix and vector algebra linear independence basis basic background in multivariate calculus eg partial derivatives basic optimization basic knowledge in python programming and numpy disclaimer this is substantially more abstract and requires more programming than the other two courses of the however this type of abstract thinking algebraic manipulation and programming is necessary if you want to understand and develop machine algorithms

statistics of datasets
principal component analysis pca is one of the most important dimensionality reduction algorithms in machine we lay the mathematical foundations to derive and understand pca from a geometric point of view we how to summarize datasets eg images using basic statistics such as the mean and the variance we also look at properties of the mean and the variance when we shift or scale the original data set we will provide mathematical intuition as well as the to derive the results we will also implement our results in code jupyter notebooks which will allow us to practice our mathematical understand to compute averages of image data sets therefore some pythonnumpy background will be necessary to get through this note if you have taken the other two courses of this this one will be harder mostly because of the programming however if you make it through the first of this make it through the full with high probability
introduction to the welcome to mean of a dataset variance of onedimensional datasets variance of higherdimensional datasets effect on the mean effect on the covariance see you next

inner products
data can be interpreted as vectors vectors allow us to talk about geometric concepts such as lengths distances and angles to characterize similarity between vectors this will become important later in the when we discuss pca we will introduce and practice the concept of an inner product inner products allow us to talk about geometric concepts in vector spaces more specifically we will start with the dot product which we may still know from school as a special case of an inner product and then move toward a more general concept of an inner product which play an integral part in some areas of machine such as kernel machines this includes support vector machines and gaussian processes we have a lot of exercises to practice and understand the concept of inner products
welcome to dot product inner product definition inner product length of vectors inner product distances between vectors inner product angles and orthogonality inner products of functions and random variables optional heading for the next

orthogonal projections
we will look at orthogonal projections of vectors which live in a highdimensional vector space onto lowerdimensional subspaces this will play an important role in the next when we derive pca we will start off with a geometric motivation of what an orthogonal projection is and our way through the corresponding derivation we will end up with a single equation that allows us to any vector onto a lowerdimensional subspace however we will also understand how this equation came about as in the other we will have both penandpaper practice and a small programming example with a jupyter notebook
welcome to projection onto d subspaces example projection onto d subspaces projections onto higherdimensional subspaces example projection onto a d subspace this was

principal component analysis
we can think of dimensionality reduction as a way of compressing data with some loss similar to jpg or mp principal component analysis pca is one of the most fundamental dimensionality reduction techniques that are used in machine we use the results from the first three of this and derive pca from a geometric point of view within this this is the most challenging one and we will go through an explicit derivation of pca plus some coding exercises that will make us a proficient user of pca
welcome to problem setting and pca objective finding the coordinates of the projected data reformulation of the objective finding the basis vectors that span the principal subspace steps of pca pca in high dimensions other interpretations of pca optional of this this was the on pca
</DOC>

